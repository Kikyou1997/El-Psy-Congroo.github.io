<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>BitTorrent 协议规范</title>
    <link href="/2020/04/22/BitTorrent%E5%8D%8F%E8%AE%AE/BitTorrent/"/>
    <url>/2020/04/22/BitTorrent%E5%8D%8F%E8%AE%AE/BitTorrent/</url>
    
    <content type="html"><![CDATA[<h1 id="BitTorrent-规范"><a href="#BitTorrent-规范" class="headerlink" title="BitTorrent 规范"></a>BitTorrent 规范</h1><p>翻译自<a href="http://bittorrent.org/beps/bep_0003.html" target="_blank" rel="noopener">http://bittorrent.org/</a></p><p>由于本人英语/技术水平有限，可能存在诸多谬误。</p><h2 id="基本组成"><a href="#基本组成" class="headerlink" title="基本组成"></a>基本组成</h2><ul><li>一个普通的web服务器</li><li>一个静态’metainfo’文件</li><li>一个BitTorrent tracker</li><li>一个下载器</li><li>终端用户的web浏览器</li><li>终端用户的的下载器</li></ul><h2 id="为了开始服务，一台主机需要经历如下步骤"><a href="#为了开始服务，一台主机需要经历如下步骤" class="headerlink" title="为了开始服务，一台主机需要经历如下步骤"></a>为了开始服务，一台主机需要经历如下步骤</h2><ol><li>运行一个tracker</li><li>运行一个普通的web服务器，如apache等</li><li>Associate the extension .torrent with mimetype application/x-bittorrent on their web server (or have done so already).、</li><li>生成一个metainfo(.torrent)文件，使该文件可通过url被访问到</li><li>将metainfo文件置于web服务器</li><li>在某个网页上得到该metainf(.torrent)文件的链接</li><li>获得完整的原始文件后开始下载</li></ol><h2 id="为了开始下载，一个用户需要做以下的事"><a href="#为了开始下载，一个用户需要做以下的事" class="headerlink" title="为了开始下载，一个用户需要做以下的事"></a>为了开始下载，一个用户需要做以下的事</h2><ol><li>安装BitTorrent</li><li>开始网上冲浪！</li><li>点击 指向.torrent 文件的链接</li><li>决定保存文件到哪里以及是否恢复尚未完成的下载</li><li>等待下载完成</li><li>关闭下载器并退出(否则其默认会保持上传)</li></ol><h2 id="编码（bencode）"><a href="#编码（bencode）" class="headerlink" title="编码（bencode）"></a>编码（bencode）</h2><ul><li>字符串被标以基于10进制的长度前缀，该长度前缀和字符串间以冒号间隔, 例如与<code>4:spam</code> 相应的<code>spam</code></li><li>整数以 字母i + 十进制数 + 字母e的形式表示， 除了0之外的任何数以0开头都是非法的</li><li>列表被编码以 字母l + 它的元素们 + 字母e，例如与<code>l4:spam4:eggse</code> 相应的是 <code>[&#39;spam&#39;, &#39;eggs&#39;]</code>.</li><li>字典被编码以 字母d + 一个键值对的列表 + 字母e，例如与<code>d3:cow3:moo4:spam4:eggse</code> 对应的是<code>{&#39;cow&#39;: &#39;moo&#39;, &#39;spam&#39;: &#39;eggs&#39;}</code> <code>d4:spaml1:a1:bee</code> 对应的是 <code>{&#39;spam&#39;: [&#39;a&#39;, &#39;b&#39;]}</code>键必须是</li></ul><h2 id="metainfo-文件-torrent文件"><a href="#metainfo-文件-torrent文件" class="headerlink" title="metainfo 文件(.torrent文件)"></a>metainfo 文件(.torrent文件)</h2><p>被编码成一个有如下键的字典。另外，文件中所有的字符串都以utf-8编码</p><ul><li><p>announce</p><p>  Tracker的URL</p></li><li><p>info</p><p>  映射到另一个info字典</p><ul><li><p>info dictionary</p><ul><li>name key  映射到一个建议的目录（下载多个文件）或文件名（单个文件），用以保存文件</li><li>piece length 映射到由每个文件分割而成的小块中的每个小块的字节数。为了进行传输，文件会被分割成固定大小的块，除了最后一个块。通常块大小是2的幂次方。最常见的是most commonly 2的18次方 = 256 K（而BitTorrent3.2版本使用2的20次方=1M作为默认大小）</li><li>pieces 被映射到一个长度是20的整倍数字符串。它被进一步细分为可根据索引访问的每个长度为20的每个piece(块)的sha1hash</li><li>key length或key files 二者仅存在一个。如果是length 那么意味着当前只是下载单个文件，否则代表一个文件集。在单个文件的情况下，长度映射到文件的长度（以字节为单位） 而对于多个文件的情况，通过将各个文件彼此连接 从而作为一个文件对待，而文件列表就是files键所映射的，而该列表又包含如下键: length 文件长度(字节) path(一个utf-8编码的子目录名列表，每个列表的最后一个代表真正的文件名?)</li></ul></li></ul></li></ul><h2 id="Trackers"><a href="#Trackers" class="headerlink" title="Trackers"></a>Trackers</h2><p>对于Tracker的GET请求有如下的键:(Tracker GET requests have the following keys:)</p><ul><li><p>info_hash</p><p>  bencoded的20字节的基于metainfo文件的info值的sha1 hash 该值必须被转义</p></li><li><p>peer_id</p><p>  长为20的的字符串，下载器以此为其id，每个下载者随机生成其id在新的下载开始的时候，该值必须被转义</p></li><li><p>ip</p><p>  一个可选的参数，给出本端的ip或dns域名</p></li><li><p>port</p><p>  该peer正在监听的端口，通常一个BitTorrent Downloader监听在6881，如果该端口已被占用则逐个向后尝试，6882,6883…</p></li><li><p>uploaded</p><p>  十进制 ascii编码的数，表示目前总的上传量</p></li><li><p>downloaded</p><p>  十进制 ascii编码的数，表示目前总的下载量</p></li><li><p>left</p><p>  十进制 ascii编码的数，表示目前待下载的量。注意该值不能根据已下载的量或者文件长度进行计算，因为如果当前正在进行断点重传，那么如果一些已经下载的数据的完整性校验失败，那么这部分将被重传</p></li><li><p>event</p><p>  可选的key，映射为started, completed, 或stopped 三个状态</p></li></ul><p>Tracker响应以bencoded字典，如果tracker的响应有一个失败的键 那么其将被映射到一个人类可读的字符串用以解释为什么查询失败了，并且不再需要的键。否则响应必须有两个键:</p><ul><li><p>interval</p><p>  映射到一个数字，表示下载器到再次发送请求需要等待的时间</p></li><li><p>peers</p><p>  映射到一个由字典构成的列表，表示其他端点，每个字典包含如下的键 peer id, ip和 port 每个键映射到各自实际的String类型的 ID, IP address or dns name as a string和 端口号</p></li></ul><p>关于Tracker的响应的更多信息可以参见<a href="http://bittorrent.org/beps/bep_0023.html" target="_blank" rel="noopener">bep23</a></p><p>下载器可能会再次发起请求，如果他们需要更多的peers的信息</p><h2 id="Peer-Protocol"><a href="#Peer-Protocol" class="headerlink" title="Peer Protocol"></a>Peer Protocol</h2><p>BitTorrent对端传输协基于TCP或者<a href="http://bittorrent.org/beps/bep_0029.html" target="_blank" rel="noopener">uTP</a>(utorrent Transport Protocol)</p><p>对端连接是双全工的</p><p>协议按照metainfo文件中描述的索引引用文件块，当一端完成了一块(piece)的下载并计算hash是否配，那么他就通知其他peers它已经拥有了这一文件块。</p><p>连接包括2bit用以表示两端的状态，是否阻塞（choked） 是否有兴趣(interested) 前者用以通知在unchoking发生前将不会继续发送数据。</p><p>在有一方感兴趣而另一方没有阻塞(unchoking)的情况，将会进行数据传输。Interest state必须时刻保持- whenever a downloader doesn’t have something they currently would ask a peer for in unchoked, they must express lack of interest, despite being choked.(?不知道想表达啥) Implementing this properly is tricky, but makes it possible for downloaders to know which peers will start downloading immediately if unchoked.</p><p>当数据传输的过程中，下载方应该维护一个队列用以存储对一些piece的请求以得到更好的TCP性能(pipelining)，另一方面，对于不能被立即写入TCP buffer中的请求也应该缓存在队列中，而不是令他们一直保存在应用层网络缓冲中，从而使我们可以在发生阻塞(拥塞)时立刻放弃这些请求。</p><p>后面这部分没太看懂</p><p>The peer wire protocol consists of a handshake followed by a never-ending stream of length-prefixed messages. The handshake starts with character ninteen (decimal) followed by the string ‘BitTorrent protocol’. The leading character is a length prefix, put there in the hope that other new protocols may do the same and thus be trivially distinguishable from each other.</p><p>All later integers sent in the protocol are encoded as four bytes big-endian.</p><p>After the fixed headers come eight reserved bytes, which are all zero in all current implementations. If you wish to extend the protocol using these bytes, please coordinate with Bram Cohen to make sure all extensions are done compatibly.</p><p>Next comes the 20 byte sha1 hash of the bencoded form of the info value from the metainfo file. (This is the same value which is announced as info_hash to the tracker, only here it’s raw instead of quoted here). If both sides don’t send the same value, they sever the connection. The one possible exception is if a downloader wants to do multiple downloads over a single port, they may wait for incoming connections to give a download hash first, and respond with the same one if it’s in their list.</p><p>After the download hash comes the 20-byte peer id which is reported in tracker requests and contained in peer lists in tracker responses. If the receiving side’s peer id doesn’t match the one the initiating side expects, it severs the connection.</p><p>That’s it for handshaking, next comes an alternating stream of length prefixes and messages. Messages of length zero are keepalives, and ignored. Keepalives are generally sent once every two minutes, but note that timeouts can be done much more quickly when data is expected.</p><h2 id="Peer-Messages"><a href="#Peer-Messages" class="headerlink" title="Peer Messages"></a>Peer Messages</h2><p>所有的除了keep-alive包(默认两分钟发送一次)之外所有的消息都有一个字节开始来表示他们的类型</p><p>可能的值包括:</p><ul><li><p>0 - choke</p></li><li><p>1 - unchoke</p></li><li><p>2 - interested</p></li><li><p>3 - not interested</p></li><li><p>4 - have</p><pre><code>  have 消息的payload只有一个简单的数字用来表示当前的下载方刚刚完成下载并hash校验过的块</code></pre></li><li><p>5 - bitfield</p><pre><code>  该类型的消息只会作为第一条消息发送，他的payload是一个位域按如下规则表示：当前downloader已经拥有的块索引，置为1，尚未拥有的则置为0.如果当前的downloader尚未保有任何块，则可以跳过发送这条消息。其payload的第一个字节的位域从高为到低位分别代表块0-7，如果还有下一次字节那么就是8-15 以此类推，至于末尾多余的位则被置0</code></pre></li><li><p>6 - request</p><pre><code>  requset消息包括一个索引，起始偏移量和长度(长度通常是2的幂次方 除非它被EOF截断)</code></pre></li><li><p>7 - piece</p><pre><code>  该类型的消息由索引，起始位置和块组成，注意他们和request消息类型存在隐含的关系。</code></pre></li><li><p>8 - cancel</p><pre><code>  与request消息拥有一样的payload，他们通常在下载即将完成的时候发送（ They are generally only sent towards the end of a download, during what&#39;s called &#39;endgame mode&#39;.） 当一次下载已经几乎完成的时候，当前下在方会向所有其他节点发送最后的尚未下载的块，那么将导致，其他所有节点向下载方同时传输同样的块，但是实际上该块的下载可能已经即将完成，这就导致带宽的利用效率低下，为了避免这种情况，他会在所请求的piece已经到达时向其他节点发送cancel消息。(When a download is almost complete, there&#39;s a tendency for the last few pieces to all be downloaded off a single hosed modem line, taking a very long time. To make sure the last few pieces come in quickly, once requests for all pieces a given downloader doesn&#39;t have yet are currently pending, it sends requests for everything to everyone it&#39;s downloading from. To keep this from becoming horribly inefficient, it sends cancels to everyone else every time a piece arrives.)</code></pre></li></ul><p>下载方通常会以随机顺序下载块，这样可以很好的防止其拥有他们节点所拥有的块的严格子集或超集(Downloaders generally download pieces in random order, which does a reasonably good job of keeping them from having a strict subset or superset of the pieces of any of their peers. 个人的理解是，对于某个(批)文件,可能不同的节点保存有不同部分的块，通过从多个节点随机下载，使得最后下载方最后仍能够得到完成的原文件)</p><p>阻塞(Choking)发生的原因，如TCP拥塞控制在立刻同时在很多连接发送数据的情况下表现的很糟</p><p>Choking机制使得每个节点使用<code>tit-for-tat-ish</code>算法(titi-for-tat 可理解为 等价的 以牙还牙的等 ish在这里是形容词后缀)来保证他们有一个相对一致的下载速率</p><p>一个好的choking算法应该满足以下几种特质：</p><ol><li>限制同时进行上传的连接数量来保证tcp传输性能</li><li>避免过于频繁的choking和unchoking（这种现象也被成为fibrillation 颤动）</li><li>对于贡献上传量的节点应该给予一定的奖励机制</li><li>如果对于当前的下载有多条连接可用，应该间歇性检查是否存在比当前正在使用的连接更优的连接</li></ol><p>当前部署的choking算法通过每10s改变一次当前处于choked状态的节点来避免颤动；通常开放4个拥有最好的下载速度的节点来奖励上传者；当前拥有较好的上传速率而没有被interested的节点优先unchoked，一旦他们成为了下载方的interested那么最差上传速率的节点将被choked；如果一个下载方拥有一份完整的文件，那么他根据他的上传速率而不是下载速率来决定unchoke的节点</p><p>为了优化unchoking，在任何时刻每30s都会轮流有一个节点unchoked无论他的上传速率如何，以使他们得到正当的机会去上传。新的连接得到unchoke的几率是其他当前正在轮流unchoke节点的3倍。</p><p>更多内容可见<a href="http://bittorrent.org/bittorrentecon.pdf" target="_blank" rel="noopener">原论文</a></p>]]></content>
    
    
    <categories>
      
      <category>p2p</category>
      
      <category>网络协议</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>DDIA读书笔记</title>
    <link href="/2020/04/04/DDIA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/ddia/"/>
    <url>/2020/04/04/DDIA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/ddia/</url>
    
    <content type="html"><![CDATA[<h1 id="DDIA读书笔记"><a href="#DDIA读书笔记" class="headerlink" title="DDIA读书笔记"></a>DDIA读书笔记</h1><p>TO-DO:</p><ol><li>lamport timestamp 与 vector clock的对比</li><li>读关于dynamo的论文</li><li><del>读lamport关于分布式系统中的时钟与事件顺序原文</del></li></ol><h2 id="数据检索"><a href="#数据检索" class="headerlink" title="数据检索"></a>数据检索</h2><ul><li><p>索引:</p><p>  基于原始数据派生而来的额外数据结构 适当的索引可以加速读取查询 但由于每次写数据时 需要更新索引 因此任何类型的索引都会降低写的速度</p></li></ul><h3 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h3><ul><li><p>哈希索引</p></li><li><p>SortedStringTable(Log Structured Merge Tree)</p><p>  基于这种实现的存储引擎通常工作方式如下:</p><ol><li>当写入时，先将要写入的数据添加到内存中的平衡树结构中(如AVL或者红黑树)，这块内存区域也被成为内存表</li><li>当内存表大于某个阈值时 将其作为一个SSTable文件写入磁盘，由于树已经维护了按键排序的k-v对或数据行，写磁盘可以比较高效。当前一个SSTable写磁盘的同时，可以将期间的写入操作添加到一个新的内存表实例</li><li>为了处理新的读请求，首先尝试在内粗表中查找key，然后是最新的磁盘段文件，然后是次新的磁盘段文件，以此类推，直到找到目标数据或者为空(这也意味着对于不存在的数据的查找，SST(LSM)效率将非常低下)</li><li>后台进程周期性的执行合并与压缩过程，以合并多个段文件，并丢弃那些已经被覆盖或者删除的val</li></ol></li><li><p>B-Tree</p><p>  B树像SSTable一样保留了按键排序的键值对 但相比于前两者始终按顺序写入段 B树将数据库分解为<strong>固定大小</strong>的块或页 传统上大小为4KB 页是内部读/写的最小单元 这种设计更接近底层硬件 因为磁盘也是以固定大小的块排列</p><p>  每个页面都可以使用地址或者位置进行标识 这样可以让一个页面引用另一个页面 类似指针 不过<strong>指向磁盘地址 而不是内存</strong> 可以使用这些页面引用来构造一个树状页面</p><p>  B树的写操作与前两者仅追加不修改相比 其会使用新数据覆盖磁盘上的旧页 而B树的更新操作(插入删除等等)往往会需要进行一系列较为复杂的操作 如插入时可能会需要分裂页 这意味着如果数据库在完成部分页写入之后发生崩溃 最终会导致整个索引的崩坏 因此基于B树的数据库往往会引入<strong>WAL(write ahead log)</strong> 也成为重做日志 其是一个仅支持追加修改的文件 每个B树的修改必须先更新WAL然后修改树本身的页 当数据库在崩溃后需要恢复时 该日志用于将B树恢复到最近一致的状态(MYSQL中的redo log与bin log)</p><p>  B树的优化:</p><ul><li>保存缩略的信息，如MySQL中非叶子节点只存储键值 不存储数据 以节省页空间 让树具有更高的<strong>分支因子</strong> 时更多的键压入到页中 从而减少层数</li><li>添加额外的指针到树中 每个叶子页面可能会向左或者向右引用其同级的兄弟页 这样可以顺序扫面键 而不用跳回到父页</li></ul></li></ul><h4 id="B树与LSM的优缺点"><a href="#B树与LSM的优缺点" class="headerlink" title="B树与LSM的优缺点"></a>B树与LSM的优缺点</h4><ul><li><p>LSM-Tree的优点</p><p>  B树索引至少写2次数据 一次写入预写日志 一次写入树的页本身(还可能发生页分裂) 即使该页中只有几个字节的更改 也必须成熟整个页的开销 这种由一次写入请求导致多次磁盘读写的现象我们称之为<strong>写放大</strong> 对于写密集型应用 性能瓶颈很可能在于写入磁盘的次数过多 可用磁盘带宽中每秒处理的写入就更少</p><p>  而LSM-tree相对于B-tree来讲一般写放大较小 其以顺序写入放入写入紧凑的SSTable文件 而不必重写树中的多个页</p><p>  LSM-tree更好的支持压缩 因此通常磁盘上的文件比B tree小很多 因为其不是面向页的 并且定期重写SSTable以消除碎片化 所以存储开销较低(在许多SSD上 固件内部使用<strong>日志结构化算法将随机写入转化为顺序写入</strong> 所以 存储引擎的写入模式影响可能不那么明显 但是更低的写放大和碎片减少对于SSD仍是有益的)</p></li><li><p>缺点</p><p>  压缩过程有时会干扰正在进行的写操作 即使存储引擎尝试增量的进行压缩 并且不影响并发访问 但是磁盘的并发资源终究是有限的 所以当磁盘进行昂贵的压缩操作时 容易发生读写请求等待的情况 相比之下B树的响应延迟更具有确定性</p><p>  另外B树的优点是每个键都恰好唯一对应于索引中的某个位置 而日志结构的存储引擎可能在不同的段中具有相同键的多个副本 这使得如果数据库希望提供强大的事务语义时 B树会更有吸引力:<strong>在于多关系型数据库中 事务隔离是通过键范围上的锁来实现的 并且在B树索引中 这些锁可以直接定义到树中</strong></p></li></ul><h2 id="复制"><a href="#复制" class="headerlink" title="复制"></a>复制</h2><h3 id="无主节点复制"><a href="#无主节点复制" class="headerlink" title="无主节点复制"></a>无主节点复制</h3><pre><code>典型的无主节点复制系统如亚马逊的dynamo，在这种实现中 客户端直接将其写请求发送到多副本，而在一些其他的实现中，有一个协调者节点代表客户端进行写入。</code></pre><h4 id="quorum机制"><a href="#quorum机制" class="headerlink" title="quorum机制"></a>quorum机制</h4><p>如果有N个副本，写入需要w个节点确认，读取必须至少查询r个节点，则只要<code>w + r &gt; n</code> 那么读取的节点中一定会包含最新值。 </p><p>在dynamo风格的数据库中，n w r通常是可配置的，n通常设置为某奇数，<code>w = r = ⌈(n + 1) / 2⌉</code>， 例如， 对于读多写少的场景 可以将w设置的较大，r设置的较小。</p><p>仲裁条件 <code>w + r &gt; n</code> 定义了系统可容忍的失效节点数， 假定n = 5， w = 3, r = 3,则可以容忍两个不可用的节点</p><p>通常写入和读取操作总是<strong>并行</strong>的发送到所有的n个副本，参数w和参数r只是决定要等待的节点数，即，需要有多少个节点返回结果 我们才能判断出结果的正确性</p><ul><li><p>局限性</p><ol><li>两个写操作同时发生，则无法明确先后顺序</li><li>如果写和读同时发生，写可能只在一部分副本上完成，此时，读取返回旧值还是新值具有不确定性</li><li>如果具有新值的节点后来发生失效，但恢复数据来自某个旧值，那么总的新值副本将低于w，这就打破了之前的判定条件</li></ol></li></ul><h2 id="一致性问题"><a href="#一致性问题" class="headerlink" title="一致性问题"></a>一致性问题</h2><h3 id="几种模型"><a href="#几种模型" class="headerlink" title="几种模型"></a>几种模型</h3><ul><li><p>可线性化(强一致性)</p><p>  基本思想是让一个系统看起来只有一个数据副本，且所有的操作都是原子的。使应用程序不必关系系统的多个副本。在可线性化的系统中，一旦某个客户端成功提交写请求，所有客户端的读请求都能看到刚刚写入的值。由于存在并发读写的情况，因此还需要为这类系统添加这样一个约束: 一旦某个读操作返回了新值，之后所有的读都必须返回新值。</p><p>  对于可线性化的系统，存在<strong>全序操作关系</strong>。系统的行为好像只有一个数据副本，且每个操作都是原子的，这意味着任何两个操作，我们总是可以指出哪一个在前。</p></li><li><p>因果一致性</p><p>  如果系统服从于因果关系(比如发送消息先于收到消息，问题出现在答案之前等等)所规定的顺序，则称之为因果一致性。</p><p>  如果两个操作都没有发生在对方之前，那么这两个操作是并发关系。因果关系，可以被排序，并发的事件则不能。也就是说，因果关系至少可以被定义为<strong>偏序</strong></p><ul><li><p>捕获因果依赖关系</p><ol><li>序列号排序: 其保证了全需关系，每一个操作都有一个唯一的顺序号，并且总是可以通过比较来确定哪个更大。但当设计到多主节点下的跨节点捕获操作时，其并不能保证序列号与因果关系的严格一致。</li><li>lamport时间戳–</li></ol></li></ul></li><li><p>最终一致性</p><p>  不一致的现象是暂时的，所有的副本最终会收敛到相同的值。大多数多副本的数据库都至少提供了最终一致性</p></li><li><p>单调读一致性</p><p>  单调读保证如果某个用户依次进行多次读取，则其绝不会看到回滚现象。即在读取较新值之后又发生读旧值的情况。</p></li></ul><h2 id="分布式系统中的时间，时钟与事件顺序"><a href="#分布式系统中的时间，时钟与事件顺序" class="headerlink" title="分布式系统中的时间，时钟与事件顺序"></a>分布式系统中的时间，时钟与事件顺序</h2><ul><li><p>并发关系与前后关系</p><ul><li><p>并发关系</p><p>  如果两个事件之间并没有因果关系，那么我们称之为并发的。</p></li><li><p>happens-before关系</p><ol><li>如果a，b是同一个进程中的事件，且a在b之前发生，那么a -&gt; b</li><li>如果a是process-a中的发送消息事件，而b是process-b中的process-a发出的消息的确认事件，那么a -&gt; b</li><li>这种关系是可传递的，如果a -&gt; b b -&gt; c 那么 a -&gt; c</li></ol></li></ul></li><li><p>逻辑时钟的两个条件</p><ol><li>如果a b是进程process-i 中的两个事件，且a早于b发生，那么有C<sub>i</sub>(a) &lt; C<sub>i</sub>(b)</li><li>如果a是process-i的发送消息事件，b是process-j对process-i在事件a中发送消息的接受事件，那么有C<sub>i</sub>(a) &lt; C<sub>j</sub>(b)</li></ol><ul><li><p>为了满足这两个条件我们遵循如下规则:</p><ul><li>IR1: 每一个进程p-i递增C<sub>i</sub>当后继的事件来到时</li><li>IR2: 为了满足条件2，我们遵循如下规则:事件a为p-i发送了一条消息m，那么在消息m中包含一个timestamp Tm = C<sub>i</sub>(a),事件b为p-j收到了消息m，P-j将为C-j设置一个大于或等于当前C-j并且大于T-m所带的C-i(比如set C-j = max(C-j, T-m) + 1 这样?)的值</li></ul></li></ul></li></ul><h3 id="lamport-timestamp"><a href="#lamport-timestamp" class="headerlink" title="lamport timestamp"></a>lamport timestamp</h3><p>每个节点都有一个唯一的标识符，并且各自维护一个计数器来记录各自已经处理的请求总数。lamport是一个值对(计数器，节点id) 其与墙上时钟不存在直接对应关系，但其可以保证全序: 给定两个lamport时间戳，计数器大的时间戳大，如果计数器值正好相同，则节点id越大，时间戳越大。</p><p>其基本流程如下: 每个节点以及每个客户端都跟踪迄今为止见到的最大计数器值，并在每个请求中附带该最大计数器值。当节点收到某个请求时，如果发现请求内嵌的最大计数器值大于自身的计数器值，则立即把自己的计数器修改为该最大值。</p><p><img src="/img/lamport_timestamp.jpg" srcset="/img/loading.gif" alt=""></p><h3 id="version-clock"><a href="#version-clock" class="headerlink" title="version clock"></a>version clock</h3><p><img src="/img/verctor_lock.png" srcset="/img/loading.gif" alt=""></p><p><a href="https://lrita.github.io/2018/10/24/lamport-logical-clocks-vector-lock/" target="_blank" rel="noopener">REF</a></p><p><img src="/img/vector-clock-2.jpg" srcset="/img/loading.gif" alt=""></p><p>当客户端读取数据时，数据库副本返回版本矢量给客户端，客户端在随后的写入时需要将版本信息包含在请求中一起发送得到数据库。这种方式只能发现存在的写冲突，但无法无法解决。发生写冲突时，应将冲突的写操作都记录下来。在客户端再次对冲突写的数据发起读时，将这些数据都返回给客户端，由其进行合并。</p><ul><li><p>一个写冲突检测的例子<a href="[2](https://lrita.github.io/2018/10/24/lamport-logical-clocks-vector-lock/)">SOURCE</a>:</p><ol><li>“用户A在N1节点上设置x=100”   ————  节点N1生成向量&lt;(N1,1)&gt;</li><li>“用户A在N1节点上设置x=200”   ————  节点N1生成向量&lt;(N1,2)&gt;</li><li>“N1将x=200传播到N2” ———–  节点N2生成向量&lt;(N1,2)&gt;</li><li>“N1将x=200传播到N3” ———–   节点N3生成向量&lt;(N1,2)&gt;</li><li>“用户A在N2节点上设置x=300”   ————  节点N2生成向量&lt;(N1,2), (N2,1)&gt;</li><li>“用户B在N3节点上设置x=400”   ———–  节点N3生成向量&lt;(N1,2), (N3,1)&gt;</li></ol></li><li><p>此时各个节点的向量</p><ul><li>N1： &lt;(N1,2)&gt;</li><li>N2：&lt;(N1,2), (N2,1)&gt;</li><li>N3：&lt;(N1,2), (N3,1)&gt;</li></ul><ol start="7"><li><p>有个读x的事件</p><p> 客户端其拿到N1,N2,N3上的向量，通过比较可知，N1上的是旧数据，N2/N3版本存在冲突，此时需要用户自己去解决冲突</p></li></ol></li></ul><h3 id="lamport-时间戳与-vector-version-的对比"><a href="#lamport-时间戳与-vector-version-的对比" class="headerlink" title="lamport 时间戳与 vector version 的对比"></a>lamport 时间戳与 vector version 的对比</h3><p>概括性的讲，lamport时间戳保证了对于事件a &lt; 事件b 那么C(a) &lt; C(b)但它无法保证若C(a) &lt; C(b) 则 事件a &lt; 事件b，而verctor version在这基础上，进一步保证了若 TS(e1) &lt; TS(e2) 那么 e1 &lt; e2 </p><p>注:TS在这里指timestamp的简写</p><h2 id="REF"><a href="#REF" class="headerlink" title="REF"></a>REF</h2><ul><li><a href="https://blog.csdn.net/yfkiss/article/details/39966087" target="_blank" rel="noopener">site1</a></li><li><a href="https://lrita.github.io/2018/10/24/lamport-logical-clocks-vector-lock/" target="_blank" rel="noopener">site2</a></li><li><a href="https://medium.com/@balrajasubbiah/lamport-clocks-and-vector-clocks-b713db1890d7" target="_blank" rel="noopener">site3</a></li><li>《Time, Clocks, and  the Ordering of Events in a  Distributed System》</li></ul>]]></content>
    
    
    <categories>
      
      <category>操作系统</category>
      
      <category>分布式系统</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Netty源码学习2</title>
    <link href="/2020/02/27/Netty%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A02/index/"/>
    <url>/2020/02/27/Netty%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A02/index/</url>
    
    <content type="html"><![CDATA[<h1 id="Netty-源码学习2"><a href="#Netty-源码学习2" class="headerlink" title="Netty 源码学习2"></a>Netty 源码学习2</h1><p>最近在写一个基于netty实现的程序的时候碰到了一些问题 暂时想不到解决方法 故再次学习一下netty源码 </p><h2 id="Netty中的Reactor"><a href="#Netty中的Reactor" class="headerlink" title="Netty中的Reactor"></a>Netty中的Reactor</h2><p><img src="/img/Netty_NioEventloop.webp" srcset="/img/loading.gif" alt=""></p><p>Netty中的<code>NioEventLoop</code>即是对多线程Reactor模型中的Reactor线程的实现 其大致分为3个步骤</p><ol><li>轮询出IO事件</li><li>处理IO事件</li><li>处理任务队列</li></ol><p>下面主要分析<code>NioEventLoop::run</code>(4.1.45Final版本)方法 </p><pre><code class="java"> @Override    protected void run() {        int selectCnt = 0;        for (;;) {            try {                int strategy;                try {                    // hasTasks返回任务队列是否还有任务，若还有则calculateStrategy方法会调用NioEventLoop提供的supplier实现，即立刻selectNow，该方法返回结果是大于0的                    strategy = selectStrategy.calculateStrategy(selectNowSupplier, hasTasks());    ...</code></pre><p>上面的代码主要根据<code>taskQueue</code>中是否还有任务来计算当前的<code>SelectStrategy</code> 如果有则调用<code>selectNow</code>方法 取得要处理的事件 该方法 <strong>Selects a set of keys whose corresponding channels are ready for I/O operations. It’s un-blocked</strong> 如果当前没有要处理的任务则strategy将为<code>SELECT</code> 这种策略意味着将阻塞等待I/O事件，这就需要计算合适的阻塞时间 以避免导致其他的定时任务无法按时得到执行，通Redis中处理事件循环以及计算阻塞时间的方式类似，这里也会将距定时任务中最近将要执行的事件的时间的差设置为最长阻塞时间</p><p>这里的<code>taskQueue</code>定义在<code>SingleThreadEventExecutor</code>默认采用<code>LinkedBlockingQueue</code>实现</p><pre><code class="java">case SelectStrategy.SELECT:                        long curDeadlineNanos = nextScheduledTaskDeadlineNanos();                        if (curDeadlineNanos == -1L) {                            curDeadlineNanos = NONE; // nothing on the calendar                        }                        nextWakeupNanos.set(curDeadlineNanos);                        try {                            if (!hasTasks()) {                                strategy = select(curDeadlineNanos);                            }                        } finally {                            // This update is just to help block unnecessary selector wakeups                            // so use of lazySet is ok (no race condition)                            nextWakeupNanos.lazySet(AWAKE);                        }</code></pre><p>期间如果发生了<code>IOException</code>将重建Selector</p><pre><code class="java">catch (IOException e) {                    // If we receive an IOException here its because the Selector is messed up. Let&#39;s rebuild                    // the selector and retry. https://github.com/netty/netty/issues/8566                    rebuildSelector0();                    selectCnt = 0;                    handleLoopException(e);                    continue;                }</code></pre><p>下面的代码中的<code>selectCnt</code>是用于记录空循环次数 此值达到一个阈值（<code>selectorAutoRebuildThreshold</code>）且<code>SELECT</code>操作的阻塞时间小于设定值时 将会触发Selector重建，这是为了解决jdk中的Select返回空值引发死循环占用过多CPU资源的bug 而<code>ioRatio</code>则是服务于Netty提供的一种设定I/O所占执行时间的机制</p><pre><code class="java">                selectCnt++;                cancelledKeys = 0;                needsToSelectAgain = false;                final int ioRatio = this.ioRatio;                boolean ranTasks;                if (ioRatio == 100) {                    try {                        if (strategy &gt; 0) {                            processSelectedKeys();                        }                    } finally {                        // Ensure we always run tasks.                        ranTasks = runAllTasks();                    }                } else if (strategy &gt; 0) {                    final long ioStartTime = System.nanoTime();                    try {                        processSelectedKeys();// 处理I/O事件                    } finally {                        // Ensure we always run tasks.                        final long ioTime = System.nanoTime() - ioStartTime;                        ranTasks = runAllTasks(ioTime * (100 - ioRatio) / ioRatio);// ioTime * (100 - ioRatio) / ioRatio 其结果刚好满足runAllTasks执行时间所占比例不超过（100-ratio）/100                    }                } else {                    ranTasks = runAllTasks(0); // This will run the minimum number of tasks                }</code></pre><p><code>runAllTasks</code>在执行时若队列中并没有任务将直接返回false，如果ranTasks为false且<code>strategy</code>&gt;0 将调用<code>unexpectedSelectorWakeup</code>方法 该方法判断<code>selectCnt</code>是否已经超过阈值若是则重建Selctor 并将selectCnt置0</p><p>详细的 processSelectedKey(SelectionKey k, AbstractNioChannel ch) 简单的来讲做了下面两件事</p><ol><li>对于boss NioEventLoop来说，轮询到的是基本上就是连接事件，后续的事情就通过他的pipeline将连接扔给一个worker NioEventLoop处理</li><li>对于worker NioEventLoop来说，轮询到的基本上都是io读写事件，后续的事情就是通过他的pipeline将读取到的字节流传递给每个channelHandler来处理</li></ol><h2 id="Netty新连接接入的处理"><a href="#Netty新连接接入的处理" class="headerlink" title="Netty新连接接入的处理"></a>Netty新连接接入的处理</h2><p>Netty中每个channel都有一个对应的unsafe，服务端对应的channel的unsafe是 <code>NioMessageUnsafe</code></p><p>当检测到<code>OP_ACCEPT</code>事件时，将调用usafe的read方法</p><pre><code class="java">private void processSelectedKey(SelectionKey k, AbstractNioChannel ch) {    final AbstractNioChannel.NioUnsafe unsafe = ch.unsafe();    int readyOps = k.readyOps();    if ((readyOps &amp; (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) {        unsafe.read();    }}</code></pre><p>其主要做了下面几件事:</p><ol><li>拿到SeverChannel对应的config，pipeline以及RecvByteBufAllocator.Handler</li><li>循环调用<code>doReadMessage</code>，其在<code>NioServerSocketChannel</code>的实现中主要做的就是读取连接并封装为<code>SocketChannel</code>加入到readBuf中</li><li>逐个交由channelPiplne上的handler进行处理</li><li>清除readBuf，触发ReadComplete事件</li></ol><pre><code class="java">private final class NioMessageUnsafe extends AbstractNioUnsafe {        private final List&lt;Object&gt; readBuf = new ArrayList&lt;Object&gt;();        @Override        public void read() {            assert eventLoop().inEventLoop();            final ChannelConfig config = config();            final ChannelPipeline pipeline = pipeline();            final RecvByteBufAllocator.Handle allocHandle = unsafe().recvBufAllocHandle();            allocHandle.reset(config);            boolean closed = false;            Throwable exception = null;            try {                try {                    do {                        int localRead = doReadMessages(readBuf);                        if (localRead == 0) {                            break;                        }                        if (localRead &lt; 0) {                            closed = true;                            break;                        }                        allocHandle.incMessagesRead(localRead);                    } while (allocHandle.continueReading());                } catch (Throwable t) {                    exception = t;                }                int size = readBuf.size();                for (int i = 0; i &lt; size; i ++) {                    readPending = false;                    pipeline.fireChannelRead(readBuf.get(i));                }                readBuf.clear();                allocHandle.readComplete();                pipeline.fireChannelReadComplete();                if (exception != null) {                    closed = closeOnReadError(exception);                    pipeline.fireExceptionCaught(exception);                }                if (closed) {                    inputShutdown = true;                    if (isOpen()) {                        close(voidPromise());                    }                }            } finally {                // Check if there is a readPending which was not processed yet.                // This could be for two reasons:                // * The user called Channel.read() or ChannelHandlerContext.read() in channelRead(...) method                // * The user called Channel.read() or ChannelHandlerContext.read() in channelReadComplete(...) method                //                // See https://github.com/netty/netty/issues/2254                if (!readPending &amp;&amp; !config.isAutoRead()) {                    removeReadOp();                }            }        }    }</code></pre><ul><li><p>简化后的Channel继承关系:</p><p>  <img src="/img/Netty_Channel_Extension_Map.webp" srcset="/img/loading.gif" alt=""></p><p>  两大channel，NioServerSocketChannel，NioSocketChannel对应着服务端接受新连接过程和新连接读写过程</p></li><li><p>Pipeline</p><p>  Netty的每个Channel都包含一个pipeline字段，其以HeadContxt为头节点，以TailConext为尾节点，HeadContxt中调用Unsafe做具体的操作，TailConext中用于向用户抛出pipeline中未处理异常以及对未处理消息的警告</p></li></ul><p>在我们对一个ServerBootstrap进行初始化时，其会在piepline中自动添加一个<code>ServerBootstrapAcceptor</code></p><pre><code class="java">p.addLast(new ChannelInitializer&lt;Channel&gt;() {            @Override            public void initChannel(final Channel ch) {                final ChannelPipeline pipeline = ch.pipeline();                ChannelHandler handler = config.handler();                if (handler != null) {                    pipeline.addLast(handler);                }                ch.eventLoop().execute(new Runnable() {                    @Override                    public void run() {                        pipeline.addLast(new ServerBootstrapAcceptor(                                ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs));                    }                });            }        });</code></pre><p>其在<code>channelRead</code>方法中将上一个handler传来的msg转化为channel后 将之注册到childEventLoop上，之后该channel的读写都由childEventLoop负责 这里之所以可以直接对Object类型的msg参数直接进行类型转换为Channel类型是因为，OP_ACCEPT事件在NioMessageUnsafe的read处理中会将之包装为一个Channel添加到readBuf列表中，然后对列表中的每个元素依次调用fireChannelRead，也就是说这个msg就是来自那，所以可以直接类型转换</p><pre><code class="java">        @Override        @SuppressWarnings(&quot;unchecked&quot;)        public void channelRead(ChannelHandlerContext ctx, Object msg) {            final Channel child = (Channel) msg;            child.pipeline().addLast(childHandler);            setChannelOptions(child, childOptions, logger);            setAttributes(child, childAttrs);            try {                childGroup.register(child).addListener(new ChannelFutureListener() {                    @Override                    public void operationComplete(ChannelFuture future) throws Exception {                        if (!future.isSuccess()) {                            forceClose(child, future.cause());                        }                    }                });            } catch (Throwable t) {                forceClose(child, t);            }        }</code></pre><p>在注册socketChannel到childGroup时，需要决定channel绑定到哪一个eventLoop上。默认采用RR的方式，但是netty会判断childGroup的线程数量，如果其是2的幂次方，<br>则采用<code>PowerOfTowEventExecutorChooser</code> 否则采用<code>GenericEventExecutorChooser</code>，前者采用位运算，后者采用余运算</p><pre><code class="java">public final class DefaultEventExecutorChooserFactory implements EventExecutorChooserFactory {    public static final DefaultEventExecutorChooserFactory INSTANCE = new DefaultEventExecutorChooserFactory();    private DefaultEventExecutorChooserFactory() { }    @SuppressWarnings(&quot;unchecked&quot;)    @Override    public EventExecutorChooser newChooser(EventExecutor[] executors) {        if (isPowerOfTwo(executors.length)) {            return new PowerOfTowEventExecutorChooser(executors);        } else {            return new GenericEventExecutorChooser(executors);        }    }    private static boolean isPowerOfTwo(int val) {        return (val &amp; -val) == val;    }    private static final class PowerOfTowEventExecutorChooser implements EventExecutorChooser {        private final AtomicInteger idx = new AtomicInteger();        private final EventExecutor[] executors;//EventLoop继承于EventExecutor        PowerOfTowEventExecutorChooser(EventExecutor[] executors) {            this.executors = executors;        }        @Override        public EventExecutor next() {            return executors[idx.getAndIncrement() &amp; executors.length - 1];        }    }    private static final class GenericEventExecutorChooser implements EventExecutorChooser {        private final AtomicInteger idx = new AtomicInteger();        private final EventExecutor[] executors;        GenericEventExecutorChooser(EventExecutor[] executors) {            this.executors = executors;        }        @Override        public EventExecutor next() {            return executors[Math.abs(idx.getAndIncrement() % executors.length)];        }    }}</code></pre><p>总的来讲Netty对新连接的处理大致如下:</p><ol><li>boos reactor线程轮询到有新的连接进入</li><li>通过封装jdk底层的channel创建 NioSocketChannel以及一系列的netty核心组件</li><li>将该条连接通过chooser，选择一条worker reactor线程绑定上去</li><li>注册读事件，开始新连接的读写</li></ol><h2 id="ChannelPipeline"><a href="#ChannelPipeline" class="headerlink" title="ChannelPipeline"></a>ChannelPipeline</h2><p>pipeline中的每个节点是一个ChannelHandlerContext对象，每个context节点保存了它包裹的执行器 ChannelHandler 执行操作所需要的上下文，其实就是pipeline，因为pipeline包含了channel的引用，可以拿到所有的context信息</p><p>一条pipeline至少会有两个节点，head和tail</p><p><img src="/img/Netty_pipeline_structure.webp" srcset="/img/loading.gif" alt=""></p><p>pipeline中有两种不同类型的节点，一个是 ChannelInboundHandler，处理inBound事件，最典型的就是读取数据流，加工处理；还有一种类型的Handler是 ChannelOutboundHandler, 处理outBound事件，比如当调用writeAndFlush()类方法时，就会经过该种类型的handler</p><p>在用户代码添加一条handler的时候，首先会查看该handler有没有添加过，netty使用added成员变量标记handler是否已经被添加过，如果非Sharable且已被添加过 那么再次添加就会触发ChannelPipelineException</p><pre><code class="java">private static void checkMultiplicity(ChannelHandler handler) {    if (handler instanceof ChannelHandlerAdapter) {        ChannelHandlerAdapter h = (ChannelHandlerAdapter) handler;        if (!h.isSharable() &amp;&amp; h.added) {            throw new ChannelPipelineException(                    h.getClass().getName() +                    &quot; is not a @Sharable handler, so can&#39;t be added or removed multiple times.&quot;);        }        h.added = true;    }}</code></pre><p>isSharable方法的实现 但我并没有想到有什么用ThreadLocalMap的必要，每个Handler又不能动态的生成所以是不是Sharable的状态是固定的，完全可以把所有的Handler的sharable与否都放到一个不可变的以class为键以是否为sharable为值的Map里:</p><p>先不考虑这个，如果必须每次添加handler都执行这个方法的话，那这么做在并发量很大，连接极多的情况下确实能起到一个不小的优化作用，毕竟JAVA中的反射是一个耗时操作</p><p>如果打开注释中的连接，我们可以看到这个issue是在14年的<a href="https://bugs.openjdk.java.net/browse/JDK-7031759" target="_blank" rel="noopener">jdk1.7上</a>的java.lang.Class.initAnnotationsIfNecessary这个方法上发生的，这个方法现在已经被从class中删除了（java13），该方法会锁住class对象，再高并发情况下，将导致大量线程阻塞于isSharable方法</p><p>按照<a href="https://stackoverflow.com/questions/24493816/are-java-lang-class-methods-thread-safe" target="_blank" rel="noopener">StackOverFlow</a>中的回答：</p><blockquote><p>One thread calls isAnnotationPresent on an annotated class where the annotation is not yet initialised for its defining classloader. This will result in a call on AnnotationType.getInstance, locking the class object for sun.reflect.annotation.AnnotationType. getInstance will result in a Class.initAnnotationsIfNecessary for that annotation, trying to acquire a lock on the class object of that annotation.`</p></blockquote><pre><code class="java">    public boolean isSharable() {        /**         * Cache the result of {@link Sharable} annotation detection to workaround a condition. We use a         * {@link ThreadLocal} and {@link WeakHashMap} to eliminate the volatile write/reads. Using different         * {@link WeakHashMap} instances per {@link Thread} is good enough for us and the number of         * {@link Thread}s are quite limited anyway.         *         * See &lt;a href=&quot;https://github.com/netty/netty/issues/2289&quot;&gt;#2289&lt;/a&gt;.         */        Class&lt;?&gt; clazz = getClass();        Map&lt;Class&lt;?&gt;, Boolean&gt; cache = InternalThreadLocalMap.get().handlerSharableCache();        Boolean sharable = cache.get(clazz);        if (sharable == null) {            sharable = clazz.isAnnotationPresent(Sharable.class);            cache.put(clazz, sharable);        }        return sharable;    }</code></pre><p>定义在<code>Channel</code>接口中的<code>Unsafe</code>中的方法 按功能可以分为分配内存，Socket四元组信息，注册事件循环，绑定网卡端口，Socket的连接和关闭，Socket的读写，看的出来，这些操作都是和jdk底层相关</p><p>Netty中读事件的传递与处理是沿着链表的头节点向尾节点，写事件则反之</p><blockquote><p>AbstractChannelHandlerContext</p></blockquote><pre><code class="java">    private AbstractChannelHandlerContext findContextInbound(int mask) {        AbstractChannelHandlerContext ctx = this;        do {            ctx = ctx.next;        } while ((ctx.executionMask &amp; mask) == 0);        return ctx;    }    private AbstractChannelHandlerContext findContextOutbound(int mask) {        AbstractChannelHandlerContext ctx = this;        do {            ctx = ctx.prev;        } while ((ctx.executionMask &amp; mask) == 0);        return ctx;    }</code></pre><pre><code class="java">interface Unsafe {   RecvByteBufAllocator.Handle recvBufAllocHandle();   SocketAddress localAddress();   SocketAddress remoteAddress();   void register(EventLoop eventLoop, ChannelPromise promise);   void bind(SocketAddress localAddress, ChannelPromise promise);   void connect(SocketAddress remoteAddress, SocketAddress localAddress, ChannelPromise promise);   void disconnect(ChannelPromise promise);   void close(ChannelPromise promise);   void closeForcibly();   void beginRead();   void write(Object msg, ChannelPromise promise);   void flush();   ChannelPromise voidPromise();   ChannelOutboundBuffer outboundBuffer();}</code></pre><p>继承体系:</p><p><img src="/img/Netty_Unsafe_extensions.webp" srcset="/img/loading.gif" alt=""></p><h2 id="REF"><a href="#REF" class="headerlink" title="REF"></a>REF</h2><p><a href="https://netty.io/wiki/related-articles.html" target="_blank" rel="noopener">Netty官方的developer guide</a> 以及<a href="https://www.jianshu.com/p/0d0eece6d467" target="_blank" rel="noopener">简书的系列文章</a></p>]]></content>
    
    
    <categories>
      
      <category>源码</category>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Netty</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RocketMq 消息消费</title>
    <link href="/2020/01/20/RocketMQ%E6%B6%88%E6%81%AF%E6%B6%88%E8%B4%B9/index/"/>
    <url>/2020/01/20/RocketMQ%E6%B6%88%E6%81%AF%E6%B6%88%E8%B4%B9/index/</url>
    
    <content type="html"><![CDATA[<h1 id="RocketMQ消息消费"><a href="#RocketMQ消息消费" class="headerlink" title="RocketMQ消息消费"></a>RocketMQ消息消费</h1><p>主要问题:</p><ol><li>消息队列负载均衡</li><li>消息消费模式</li><li>消息拉取方式</li><li>消息进度反馈</li><li>消息过滤</li><li>顺序消息</li></ol><p>消息负载遵循这样一个思想: <strong>一个消息队列同一时间只允许被一个消费者消费 一个消费者可以消费多个消息队列</strong> RocketMQ 支持<strong>局部</strong>顺序消息消费 不支持消息全局顺序消费 如果要实现某一主题的全局顺序消息消费 可以将该主题的队列数设置为1 但这样牺牲了高可用性</p><p>基本流程:</p><ol><li>消息队列负载</li><li>消息拉取</li><li>消息消费</li><li>消息消费进度存储</li></ol><h2 id="消息费者启动流程"><a href="#消息费者启动流程" class="headerlink" title="消息费者启动流程"></a>消息费者启动流程</h2><p>参见<code>DefaultMQPushConsumerImpl::start</code>方法</p><ol><li>构建主题订阅信息<code>SubscriptionData</code>并加入到RebalanceImpl的订阅消息中</li><li>初始化<code>MQCLientInstance</code>,<code>RebalanceImpl</code>等</li><li>初始化消息进度 <strong>如果是集群消费 则消息进度保存在Broker上 如果是广播消费 则消息消费进度保存在消费端</strong></li><li>根据是否顺序消费 创建消费端消费线程服务</li><li>向MQClientInstance注册消费者</li></ol><h2 id="消息拉取"><a href="#消息拉取" class="headerlink" title="消息拉取"></a>消息拉取</h2><ul><li><p>ProcessQueue</p><p>  其是MessageQueue在消费端的重现 快照 PullMessageService从消息服务器默认每次拉取32条消息 按照消息队列偏移量顺序存放在ProcessQueue中 之后<code>PullMessageService</code>将其提交到消费者线程池中 消息消费成功后从<code>ProcessQueue</code>中移除</p></li></ul><h3 id="消息拉取基本流程"><a href="#消息拉取基本流程" class="headerlink" title="消息拉取基本流程"></a>消息拉取基本流程</h3><ol><li>消息拉取客户端消息 拉取请求封装</li><li>消息服务器查找并返回消息 从远端服务器拉去消息后将消息存入ProcessQueue消息队列中</li><li>消息拉取客户端处理返回的消息 由<code>ConsumeMessageService::submitConsumeRequest</code>方法进行消息消费 使用线程池消费消息 实现了消息拉取和消息消费的解耦</li></ol><p>基本流程可参见<code>DefaultMQPushConsumerImpl::pullMessage</code>方法 实现过于复杂 光这个方法本身就有两百多行。。。而<code>PullMessageProcessor::processRequest</code>一个方法近四百行(虽说这个方法是Broker端调用的吧)。。。看不下去 就搁一张技术内幕里的图得了。。。</p><p><img src="/img/rmq_pull_message_mechanism.png" srcset="/img/loading.gif" alt="rmq_pull_msg_mechanism"></p><h3 id="消息拉取长轮询机制分析"><a href="#消息拉取长轮询机制分析" class="headerlink" title="消息拉取长轮询机制分析"></a>消息拉取长轮询机制分析</h3><p>RocketMQ并没有真正实现推模式 其<strong>推模式是循环向消息服务端发送消息拉取请求</strong> 在不启用长轮询机制时 如果消息消费者向rocketmq发送消息拉取时 消息并未到达消费队列的话 服务端(也就是Broker)等待<code>shortPollingTimeMills</code>(众所周知就像本人的笔记一样 rocketmq项目中错误单词百出)再去判断消息是否已经到达消息队列 如果消息未到达则提示消息拉取客户端<code>PULL_NOT_FOUND</code>(消息不存在) 如果开启长轮询模式 则rocketmq会每隔5s轮询检查一次消息是否可达 同时一有新消息到达后立马通知挂起线程再次验证新消息是否是自己感兴趣的消息 如果是则从<code>commitlog</code>中提取消息返回给消息拉取客户端 否则直到挂起超时(push模式默认15s pull模式默认20s 封装在消息拉取时的请求参数中)</p><p>通过在broker的配置文件中设置<code>longPollingEnable</code>为<code>true</code>来开启长轮询</p><h2 id="消息队列负载与重分布机制"><a href="#消息队列负载与重分布机制" class="headerlink" title="消息队列负载与重分布机制"></a>消息队列负载与重分布机制</h2><p>两个问题:</p><ul><li><p><code>PullRequest</code>对象在什么时候创建并加入到<code>pullRequestQueue</code>中以便唤醒<code>PullMessageService</code>线程</p><p>  RebalanceService线程每隔20s对消费者订阅的主题进行一次队列重新分配 每一次分配都会获得主题的所有队列 从Broker服务器实时查询当前指定主题指定消费组内消费者列表 对<strong>新分配的消息队列</strong>会创建对应的PullRequest对象  <strong>在一个JVM进程中 同一个消费组同一个消费队列只会存在一个PullRequest对象</strong>(我知道了 大概就是这玩意创建完之后搁队列里 取出来之后调用<code>DefaultMQPushConsumerImpl::pullMessage</code>拉取成功之后在回调方法里会再把这玩意搁<code>PullMessageService</code>的那个队列里…真是奇妙深刻)</p></li><li><p>集群内多个消费者是如何负载消息下的多个消费队列 如果有新的消费者加入时 消息队列又会如何重新分布</p><p>  每次进行队列重新负载时 会从Broker查询出当前消费组内所有消费者 并对消息队列消费者列表进行排序 这样下新加入的消费者就会在队列重新分布时分配到消费队列从而消费消息</p></li></ul><p><code>RebalanceService</code>线程负责实现mq的消息队列重分布 每一个MQClientInstance都持有一个该Service的实现 随其启动而启动 该服务线程每20s执行一次<code>MQClientInstance::doRebalance</code>方法 instance遍历已注册的消费者对消费者执行<code>doRebalance</code>方法</p><pre><code class="java">    public void doRebalance() {        for (Map.Entry&lt;String, MQConsumerInner&gt; entry : this.consumerTable.entrySet()) {            MQConsumerInner impl = entry.getValue();            if (impl != null) {                try {                    impl.doRebalance();                } catch (Throwable e) {                    log.error(&quot;doRebalance exception&quot;, e);                }            }        }    }</code></pre><p>每个<code>DefaultPushConsumerImpl</code>都持有一个<code>rebalanceImpl</code>对象 该方法主要是遍历订阅信息对每个主题的队列进行重新负载</p><p>关于其如何针对特定topic进行消息队列重新负载见<code>RebalanceImpl:rebalanceByTopic</code>:</p><ul><li>以集群模式为例</li></ul><ol><li><p>从主题订阅信息缓存表中获取主题的队列信息 从Broker中获取当前消费组内所有消费者客户端id(因为<code>MQClientInstance</code>启动时会向所有的Broekr发送心跳包 其中包括了自己的消费者信息 所以Broker中会有关于消费组的信息 至于从哪个Broker中获取则随机选择)</p><pre><code class="java">    public String findBrokerAddrByTopic(final String topic) {     TopicRouteData topicRouteData = this.topicRouteTable.get(topic);     if (topicRouteData != null) {         List&lt;BrokerData&gt; brokers = topicRouteData.getBrokerDatas();         if (!brokers.isEmpty()) {             int index = random.nextInt(brokers.size());             BrokerData bd = brokers.get(index % brokers.size());             return bd.selectBrokerAddr();         }     }     return null; }</code></pre></li><li><p>对cid mqAll排序 确保同一个消费组内看到的视图保持一致 采用指定算法进行分配 RocketMQ默认提供6种分配算法<br><img src="/img/rmq_cmq_allocate_alg.png" srcset="/img/loading.gif" alt="alg"><br>通常情况下 使用平均轮询分配和平均分配即可</p></li></ol><p>尤其需要注意的是<strong>同一个消息队列只会分配给一个消费者 如果消费者个数大于消息队列数量 则有些消费者可能会无法消费消息</strong></p><ol start="3"><li><p>对比消息队列是否发生变化 更新当前consumer被分配到的consumequeue(<code>RebalanceImpl.processQueueTable</code>字段) 见<code>RebalanceImpl::updateProcessQueueTableInRebalance</code> 其主要思路是遍历当前负载队列集合 如果队列不在新分配集合中 需要将该队列停止消费 并保存消费进度; 遍历已分配队列 如果队列不在<code>RebalanceImpl.processQueueTable</code>中 则需要创建该对列拉去任务<code>PullRequest</code> 然后添加到<code>PullMessageService</code>线程的<code>pullRequestQueue</code>中 这样<code>PullMessageService</code>才会继续拉取任务</p></li><li><p>遍历本次负载分配到的队列集合 如果<code>processQueueTable</code>没有包含该消息队列 表明这是本次新增加的消息队列 首先从内存中剔除该消息队列的消息进度 然后从磁盘中读取该消息队列的消费进度(再强调一下对于广播型的消费 其消费进度保存在本地 而集群消费的consumerqueue消费进度保存在broker上此时从磁盘上读取消息队列的消费进度 也是从broker服务器的磁盘上读取) 创建PullRequest对象 对于消费进度的问题可以在创建消费者时通过<code>DefaultMQPushConsumer::setConsumeFromWhere</code>方法设置 有3种策略</p><ol><li><p>从队列最新偏移量开始消费</p></li><li><p>从头开始消费</p></li><li><p>从消费者的时间戳对应的消费进度开始消费</p><p>但是这些消费策略只有在从磁盘中读取消费进度返回-1时才会生效(?更加迷惑?) 这是指的是<code>RebalancePushImpl::computePullFromWhere</code>方法的实现中 无论我们设置了哪种策略 都会先尝试从broker消息服务器拉取 consumequeue的消费进度 失败再的话再按指定策略进行设置 具体可参看该方法的代码实现</p></li></ol></li><li><p>将<code>PullRequest</code>加入到<code>PullMessageService</code>中 以便唤醒该线程</p></li></ol><p><img src="/img/rmq_cluster_consume_progress_princinple.png" srcset="/img/loading.gif" alt="rmq_cluster"></p><h2 id="消息消费"><a href="#消息消费" class="headerlink" title="消息消费"></a>消息消费</h2><h3 id="消息进度设计"><a href="#消息进度设计" class="headerlink" title="消息进度设计"></a>消息进度设计</h3><p>关于并发消费消息进度的更新问题</p><p>消费者线程池每处理完一个消息消费任务时(ConsumeRequest) 会从ProcessQueue中移除本批消费的的消息 <strong>并返回ProcessQueue中的最小偏移量</strong>  消息消费的推进取决于ProcessQueue中偏移量最小的消息消费速度 参见<code>ProcessQueue::removeMessage</code>方法 其在<code>ConsumeMessageConcurrentlyService::processConsumeResult</code>中被调用 用该偏移量更新消息队列消费进度</p><pre><code class="java">    public long removeMessage(final List&lt;MessageExt&gt; msgs) {        long result = -1;        final long now = System.currentTimeMillis();        try {            this.lockTreeMap.writeLock().lockInterruptibly();            this.lastConsumeTimestamp = now;            try {                if (!msgTreeMap.isEmpty()) {                    result = this.queueOffsetMax + 1;                    int removedCnt = 0;                    for (MessageExt msg : msgs) {                        MessageExt prev = msgTreeMap.remove(msg.getQueueOffset());                        if (prev != null) {                            removedCnt--;                            msgSize.addAndGet(0 - msg.getBody().length);                        }                    }                    msgCount.addAndGet(removedCnt);                    if (!msgTreeMap.isEmpty()) {                        result = msgTreeMap.firstKey();                    }                }            } finally {                this.lockTreeMap.writeLock().unlock();            }        } catch (Throwable t) {            log.error(&quot;removeMessage exception&quot;, t);        }        return result;    }</code></pre><p>为了避免消费消息偏移量最小的消息时发生死锁 导致一致无法被消费 进而导致 消息进度无法向前推进 RocketMq 引入了一种消息拉取流控措施 <code>DefaultMQPushConsumer.consumeConcurrentlyMaxSpan</code> 一旦ProcessQueue中的最大消息偏移与最小消息偏移量超过该值 将触发流控 延迟该消息队列的消息拉取</p><p>触发消息消费进度更新的另一个是在消息负载时 如果消息消费队列被分配给其他消费者时 此时会将ProcessQueue状态设置为dropped 持久化该消息队列的消费进度 并从内存中移除</p><h2 id="顺序消息"><a href="#顺序消息" class="headerlink" title="顺序消息"></a>顺序消息</h2><p>顺序消息包含两种类型:</p><ul><li><p>分区顺序(RocketMQ所支持的)：一个Partition内所有的消息按照先进先出的顺序进行发布和消费</p></li><li><p>全局顺序：一个Topic内所有的消息按照先进先出的顺序进行发布和消费</p></li></ul><p>在MQ的模型中，顺序需要由3个阶段去保障：</p><ul><li><p>消息被发送时保持顺序</p></li><li><p>消息被存储时保持和发送的顺序一致</p></li><li><p>消息被消费时保持和存储的顺序一致</p></li></ul><p>发送时保持顺序意味着对于有顺序要求的消息，用户应该在同一个线程中采用同步的方式发送。存储保持和发送的顺序一致则要求在同一线程中被发送出来的消息A和B，存储时在空间上A一定在B之前。而消费保持和存储一致则要求消息A、B到达Consumer之后必须按照先A后B的顺序被处理。</p><p><img src="/img/rmq_order.png" srcset="/img/loading.gif" alt="rmqorder"></p><p>推荐阅读<a href="https://www.cnblogs.com/hzmark/p/orderly_message.html" target="_blank" rel="noopener">koko</a></p><p>Rocketmq首先通过<code>RebalanceService</code>线程实现消息队列的负载 集群模式下同一个消费组内的消费者共同承担其订阅主题下消息队列的消费 <strong>同一个消息消费队列在同一时刻只会被消费组内一个消费者消费 一个消费者同一个时刻可以分配多个消费队列</strong></p><p>rmq支持局部消息顺序消费 可以确保同一个消息消费队列中的消息被顺序消费 如果需要做到全局顺序消费则可以将主题配置成<strong>一个队列</strong></p><p>调用<code>RebalanceImpl::lock</code>的向Broker发送锁定消息队列的请求 -&gt; 锁定成功后 设置processqueue的locked状态为true -&gt; <code>ConsumeMessageOrderlyService</code>维护了一个<code>MessageQueueLock</code>该lock用ConcurrentHashMap 为每个MessageQueue设置了一个object对象锁 -&gt; 消费时 需要获取指定消费队列的lock 即同时只能有消费者的一个消费线程去消费指定消息队列的消息 </p><p>上述也体现了顺序消息与并发消息的最重要的区别 顺序消息消费是指消费者内的线程池内的线程只能串行的消费 以及必须成功锁定消息消费队列 在broker端会存储消息消费队列的锁占用情况</p>]]></content>
    
    
    <categories>
      
      <category>源码</category>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MessageQueue</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>内存锁定</title>
    <link href="/2020/01/16/%E5%86%85%E5%AD%98%E9%94%81%E5%AE%9A/index/"/>
    <url>/2020/01/16/%E5%86%85%E5%AD%98%E9%94%81%E5%AE%9A/index/</url>
    
    <content type="html"><![CDATA[<h1 id="4-Memory-Locking"><a href="#4-Memory-Locking" class="headerlink" title="4  Memory Locking"></a>4  Memory Locking</h1><p>本文来自<a href="(https://www3.physnet.uni-hamburg.de/physnet/Tru64-Unix/HTML/APS33DTE/DOCU_005.HTM">该大学网站上的一篇文章</a></p><p>Memory management facilities ensure that processes have effective and equitable access to memory resources. The operating system maps and controls the relationship between physical memory and the virtual address space of a process. These activities are, for the most part, transparent to the user and controlled by the operating system. However, for many realtime applications you may need to make more efficient use of system resources by explicitly controlling virtual memory usage.</p><p>This chapter includes the following sections:</p><p>Memory Management, Section 4.1</p><p>Memory-Locking and Unlocking Functions, Section 4.2</p><p>Memory locking is one way to ensure that a process stays in main memory and is exempt from paging. In a realtime environment, a system must be able to guarantee that it will lock a process in memory to reduce latency for data access, instruction fetches, buffer passing between processes, and so forth. Locking a process’s address space in memory helps ensure that the application’s response time satisfies realtime requirements. As a general rule, time-critical processes should be locked into memory.</p><h2 id="4-1-Memory-Management"><a href="#4-1-Memory-Management" class="headerlink" title="4.1    Memory Management"></a>4.1    Memory Management</h2><p>In a multiprogramming environment, it is essential for the operating system to share available memory effectively among processes. Memory management policies are directly related to the amount of memory required to execute those processes. Memory management algorithms are designed to optimize the number of runnable processes in primary memory while avoiding conflicts that adversely affect system performance. If a process is to remain in memory, the kernel must allocate adequate units of memory. If only part of a process needs to be in primary memory at any given time, then memory management can work together with the scheduler to make optimal use of resources.</p><p>Virtual address space is divided into fixed-sized units, called pages. Each process usually occupies a number of pages, which are independently moved in and out of primary memory as the process executes. Normally, a subset of a process’s pages resides in primary memory when the process is executing.</p><p>Since the amount of primary memory available is finite, paging is often done at the expense of some pages; to move pages in, others must be moved out. If the page that is going to be replaced is modified during execution, that page is written to a file area. That page is brought back into primary memory as needed and execution is delayed while the kernel retrieves the page.</p><p>Paging is generally transparent to the current process. The amount of paging can be decreased by increasing the size of physical memory or by locking the pages into memory. However, if the process is very large or if pages are frequently being paged in and out, the system overhead required for paging may decrease efficiency.</p><p>For realtime applications, having adequate memory is more important than for nonrealtime applications. Realtime applications must ensure that processes are locked into memory and that there is an adequate amount of memory available for both realtime processes and the system. Latency due to paging is often unacceptable for critical realtime tasks.</p><h2 id="4-2-Memory-Locking-and-Unlocking-Functions"><a href="#4-2-Memory-Locking-and-Unlocking-Functions" class="headerlink" title="4.2    Memory-Locking and Unlocking Functions"></a>4.2    Memory-Locking and Unlocking Functions</h2><p>Realtime application developers should consider memory locking as a required part of program initialization. Many realtime applications remain locked for the duration of execution, but some may want to lock and unlock memory as the application runs. DIGITAL UNIX memory-locking functions let you lock the entire process at the time of the function call and throughout the life of the application, or selectively lock and unlock as needed.</p><p>Memory locking applies to a process’s address space. Only the pages mapped into a process’s address space can be locked into memory. When the process exits, pages are removed from the address space and the locks are removed.</p><p>Two functions, mlock and mlockall, are used to lock memory. The mlock function allows the calling process to lock a selected region of address space. The mlockall function causes all of a process’s address space to be locked. Locked memory remains locked until either the process exits or the application calls the corresponding munlock or munlockall function.</p><p>Memory locks are not inherited across a fork and all memory locks associated with a process are unlocked on a call to the exec function or when the process terminates.</p><p>For most realtime applications the following control flow minimizes program complexity and achieves greater determinism by locking the entire address into memory.</p><p>Perform nonrealtime tasks, such as opening files or allocating memory</p><p>Lock the address space of the process calling mlockall function</p><p>Perform realtime tasks</p><p>Release resources and exit</p><p>The memory-locking functions are as follows:</p><p><img src="/img/os_memory_lock.png" srcset="/img/loading.gif" alt="func"></p><p>You must have superuser privileges to call the memory locking functions.</p><h3 id="4-2-1-Locking-and-Unlocking-a-Specified-Region"><a href="#4-2-1-Locking-and-Unlocking-a-Specified-Region" class="headerlink" title="4.2.1    Locking and Unlocking a Specified Region"></a>4.2.1    Locking and Unlocking a Specified Region</h3><p>The mlock function locks a preallocated specified region. The address and size arguments of the mlock function determine the boundaries of the preallocated region. On a successful call to mlock, the specified region becomes locked. Memory is locked by the system according to system-defined pages. If the address and size arguments specify an area smaller than a page, the kernel rounds up the amount of locked memory to the next page. The mlock function locks all pages containing any part of the requested range, which can result in locked addresses beyond the requested range.</p><p>Repeated calls to mlock could request more physical memory than is available; in such cases, subsequent processes must wait for locked memory to become available. Realtime applications often cannot tolerate the latency introduced when a process must wait for lockable space to become available. Preallocating and locking regions is recommended for realtime applications.</p><p>If the process requests more locked memory than will ever be available in the system, an error is returned.</p><p>Figure 4-1 illustrates memory allocation before and after a call to the mlock function. Prior to the call to the mlock function, buffer space in the data area is not locked and is therefore subject to paging. After the call to the mlock function the buffer space cannot be paged out of memory.</p><ul><li><strong>Figure 4-1:  Memory Allocation with mlock</strong></li></ul><p><img src="/img/memory_alloc_with_mlock.gif" srcset="/img/loading.gif" alt="mawml"></p><p>The mlock function locks all pages defined by the range addr to addr+len-1 (inclusive). The area locked is the same as if the len argument were rounded up to a multiple of the next page size before decrementing by 1. The address must be on a page boundary and all pages mapped by the specified range are locked. Therefore, you must determine how far the return address is from a page boundary and align it before making a call to the mlock function.</p><p>Use the sysconf(_SC_PAGE_SIZE) function to determine the page size. The size of a page can vary from system to system. To ensure portability, call the sysconf function as part of your application or profile when writing applications that use the memory-locking functions. The sys/mman.h header file defines the maximum amount of memory that can be locked. Use the getrlimit function to determine the amount of total memory.</p><p>Exercise caution when you lock memory; if your processes require a large amount of memory and your application locks memory as it executes, your application may take resources away from other processes. In addition, you could attempt to lock more virtual pages than can be contained in physical memory.</p><p>Locked space is automatically unlocked when the process exits, but you can also explicitly unlock space. The munlock function unlocks the specified address range regardless of the number of times the mlock function was called. In other words, you can lock address ranges over multiple calls to the mlock function, but can remove the locks with a single call to the munlock function. Space locked with a call to the mlock function must be unlocked with a corresponding call to the munlock function.</p><p>Example 4-1 shows how to lock and unlock memory segments. Each user-written function determines page size, adjusts boundaries, and then either locks or unlocks the segment.</p><ul><li><strong>Example 4-1:  Aligning and Locking a Memory Segment</strong></li></ul><pre><code class="c">#include &lt;unistd.h&gt;     /* Support all standards    */#include &lt;sys/mman.h&gt;   /* Memory locking functions */#define DATA_SIZE 2048lock_memory(char   *addr,            size_t  size){  unsigned long    page_offset, page_size;  page_size = sysconf(_SC_PAGE_SIZE);  page_offset = (unsigned long) addr % page_size;  addr -= page_offset;  /* Adjust addr to page boundary */  size += page_offset;  /* Adjust size with page_offset */  return ( mlock(addr, size) );  /* Lock the memory */}unlock_memory(char   *addr,              size_t  size){  unsigned long    page_offset, page_size;  page_size = sysconf(_SC_PAGE_SIZE);  page_offset = (unsigned long) addr % page_size;  addr -= page_offset;  /* Adjust addr to page boundary */  size += page_offset;  /* Adjust size with page_offset */  return ( munlock(addr, size) );  /* Unlock the memory */}main(){  char data[DATA_SIZE];  if ( lock_memory(data, DATA_SIZE) == -1 )    perror(&quot;lock_memory&quot;);           /* Do work here */  if ( unlock_memory(data, DATA_SIZE) == -1 )    perror(&quot;unlock_memory&quot;);}</code></pre><h3 id="4-2-2-Locking-and-Unlocking-an-Entire-Process-Space"><a href="#4-2-2-Locking-and-Unlocking-an-Entire-Process-Space" class="headerlink" title="4.2.2    Locking and Unlocking an Entire Process Space"></a>4.2.2    Locking and Unlocking an Entire Process Space</h3><p>The mlockall function locks all of the pages mapped by a process’s address space. On a successful call to mlockall, the specified process becomes locked and memory-resident. The mlockall function takes two flags, MCL_CURRENT and MCL_FUTURE, which determine whether the pages to be locked are those currently mapped, or if pages mapped in the future are to be locked. You must specify at least one flag for the mlockall function to lock pages. If you specify both flags, the address space to be locked is constructed from the logical OR of the two flags.</p><p>If you specify MCL_CURRENT only, all currently mapped pages of the process’s address space are memory-resident and locked. Subsequent growth in any area of the specified region is not locked into memory. If you specify the MCL_FUTURE flag only, all future pages are locked in memory. If you specify both MCL_CURRENT and MCL_FUTURE, then the current pages are locked and subsequent growth is automatically locked into memory.</p><p>Figure 4-2 shows memory allocation before and after a call to the mlockall function with both MCL_CURRENT and MCL_FUTURE flags. Prior to the call to the mlockall function, space is not locked and is therefore subject to paging. After a call to the mlockall function, which specifies the MCL_CURRENT and MCL_FUTURE flags, all memory used by the process, both currently and in the future, is locked into memory. The call to the malloc function increases the amount of memory locked for the process.</p><ul><li><strong>Figure 4-2:  Memory Allocation with mlockall</strong></li></ul><p><img src=".//img/memory_alloc_with_mlockall.gif" srcset="/img/loading.gif" alt="mawmla"></p><p>The munlockall function unlocks all pages mapped by a call to the mlockall function, even if the MCL_FUTURE flag was specified on the call. The call to the munlockall function cancels the MCL_FUTURE flag. If you want additional locking later, you must call the memory-locking functions again.</p><p>Example 4-2 illustrates how the mlockall function might be used to lock current and future address space.</p><ul><li><strong>Example 4-2:  Using the mlockall Function</strong></li></ul><pre><code class="c">#include &lt;unistd.h&gt;     /* Support all standards    */#include &lt;stdlib.h&gt;     /* malloc support           */#include &lt;sys/mman.h&gt;   /* Memory locking functions */#define BUFFER 2048main(){  void *p[3];  /* Array of 3 pointers to void */  p[0] = malloc(BUFFER);       /* Currently no memory is locked */  if ( mlockall(MCL_CURRENT) == -1 )    perror(&quot;mlockall:1&quot;);       /* All currently allocated memory is locked */  p[1] = malloc(BUFFER);       /* All memory but data pointed to by p[1] is locked */  if ( munlockall() == -1 )    perror(&quot;munlockall:1&quot;);       /* No memory is now locked */  if ( mlockall(MCL_FUTURE) == -1 )    perror(&quot;mlockall:2&quot;);       /* Only memory allocated in the future */       /*   will be locked */  p[2] = malloc(BUFFER);       /* Only data pointed to by data[2] is locked */  if ( mlockall(MCL_CURRENT|MCL_FUTURE) == -1 )    perror(&quot;mlockall:3&quot;);       /* All memory currently allocated and all memory that  */       /* gets allocated in the future will be locked         */}</code></pre>]]></content>
    
    
    <categories>
      
      <category>操作系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>任务</tag>
      
      <tag>syscall</tag>
      
      <tag>内存管理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Netty源码学习</title>
    <link href="/2020/01/11/Netty%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/index/"/>
    <url>/2020/01/11/Netty%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/index/</url>
    
    <content type="html"><![CDATA[<h1 id="Netty源码学习"><a href="#Netty源码学习" class="headerlink" title="Netty源码学习"></a>Netty源码学习</h1><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><img src="/img/event_drived_model.webp" srcset="/img/loading.gif" alt=""></p><p><img src="/img/netty_thread_model.webp" srcset="/img/loading.gif" alt=""><br><img src="/img/reactor_multi_thread_model.webp" srcset="/img/loading.gif" alt=""></p><p><img src="/img/Netty_channel_components.webp" srcset="/img/loading.gif" alt=""></p><p>Rector多线程模型:</p><p>特点：</p><ol><li>有专门一个reactor线程用于监听服务端ServerSocketChannel，接收客户端的TCP连接请求；</li><li>网络IO的读/写操作等由一个worker reactor线程池负责，由线程池中的NIO线程负责监听SocketChannel事件，进行消息的读取、解码、编码和发送。</li><li><strong>一个NIO线程可以同时处理N条链路，但是一个链路只注册在一个NIO线程上处理</strong>，防止发生并发操作问题。</li></ol><p>Netty采取了事件驱动模型设计 其具有如下4个基本组件</p><ul><li><strong>事件队列（event queue）</strong> ：接收事件的入口，存储待处理事件</li><li><strong>分发器（event mediator）</strong> ：将不同的事件分发到不同的业务逻辑单元</li><li><strong>事件通道（event channel）</strong>：分发器与处理器之间的联系渠道</li><li><strong>事件处理器（event processor）</strong>：实现业务逻辑，处理完成后会发出事件，触发下一步操作</li></ul><p>相对传统轮询模式，事件驱动有如下优点：</p><ul><li><strong>可扩展性好</strong>，分布式的异步架构，事件处理器之间高度解耦，可以方便扩展事件处理逻辑</li><li><strong>高性能</strong>，基于队列暂存事件，能方便并行异步处理事件</li></ul><p><img src="/img/event_drived_in_netty.webp" srcset="/img/loading.gif" alt=""></p><h2 id="ServerBootStrap"><a href="#ServerBootStrap" class="headerlink" title="ServerBootStrap"></a>ServerBootStrap</h2><p>来自<a href="https://www.jianshu.com/p/c5068caab217" target="_blank" rel="noopener">简书闪电侠系列文章</a></p><p>例子</p><pre><code class="java">    public static void main(String[] args) throws Exception {        EventLoopGroup bossGroup = new NioEventLoopGroup(1);        EventLoopGroup workerGroup = new NioEventLoopGroup();        try {            ServerBootstrap b = new ServerBootstrap();            b.group(bossGroup, workerGroup)                    .channel(NioServerSocketChannel.class)                    .handler(new SimpleServerHandler())                    .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() {                        @Override                        public void initChannel(SocketChannel ch) throws Exception {                        }                    });            ChannelFuture f = b.bind(8888).sync();            f.channel().closeFuture().sync();        } finally {            bossGroup.shutdownGracefully();            workerGroup.shutdownGracefully();        }    }</code></pre><p>EventLoopGroup 一个死循环，不停地检测IO事件，处理IO事件，执行任务</p><p>ServerBootstrap 是服务端的一个启动辅助类，通过给他设置一系列参数来绑定端口启动服务</p><p>group(bossGroup, workerGroup) 我们需要两种类型的人干活，一个是老板，一个是工人，老板负责从外面接活，接到的活分配给工人干，放到这里，bossGroup的作用就是不断地accept到新的连接，将新的连接丢给workerGroup来处理</p><p>.channel(NioServerSocketChannel.class) 表示服务端启动的是nio相关的channel，channel在netty里面是一大核心概念，可以理解为一条channel就是一个连接或者一个服务端bind动作，后面会细说</p><p>.handler(new SimpleServerHandler() 表示服务器启动过程中，需要经过哪些流程，这里SimpleServerHandler最终的顶层接口为ChannelHander，是netty的一大核心概念，表示数据流经过的处理器，可以理解为流水线上的每一道关卡</p><p>childHandler(new ChannelInitializer<SocketChannel>)…表示一条新的连接进来之后，该怎么处理，也就是上面所说的，老板如何给工人配活</p><p>ChannelFuture f = b.bind(8888).sync(); 这里就是真正的启动过程了，绑定8888端口，等待服务器启动完毕，才会进入下行代码</p><p>f.channel().closeFuture().sync(); 等待服务端关闭socket</p><p>bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); 关闭两组死循环</p><h2 id="Netty-中的Reactor线程"><a href="#Netty-中的Reactor线程" class="headerlink" title="Netty 中的Reactor线程"></a>Netty 中的Reactor线程</h2><p>来自<a href="https://www.jianshu.com/p/c5068caab217" target="_blank" rel="noopener">简书闪电侠系列文章</a></p><p>netty中最核心的东西莫过于两种类型的reactor线程，可以看作netty中两种类型的发动机，驱动着netty整个框架的运转</p><p>一种类型的reactor线程是boos线程组，专门用来接受新的连接，然后封装成channel对象扔给worker线程组；还有一种类型的reactor线程是worker线程组，专门用来处理连接的读写</p><p>不管是boos线程还是worker线程，所做的事情均分为以下三个步骤</p><ol><li>轮询注册在selector上的IO事件</li><li>处理IO事件</li><li>执行异步task</li></ol><p>对于boos线程来说，第一步轮询出来的基本都是 accept 事件，表示有新的连接，而worker线程轮询出来的基本都是read/write事件，表示网络的读写事件</p><h3 id="处理模式"><a href="#处理模式" class="headerlink" title="处理模式"></a>处理模式</h3><h3 id="NIO-和-BIO"><a href="#NIO-和-BIO" class="headerlink" title="NIO 和 BIO"></a>NIO 和 BIO</h3><p>Netty提供了NIO与BIO(OIO)两种模式处理这些逻辑，其中 <strong>NIO主要通过一个BOSS线程处理等待链接的接入，若干个WORKER线程(从worker线程池中挑选一个赋给Channel实例，因为Channel实例持有真正的java网络对象)接过BOSS线程递交过来的CHANNEL进行数据读写并且触发相应事件传递给pipeline进行数据处理</strong>, 而以SeverBootStrap启动时设置的额bossGroup 和 workerGroup为例来讲 bossGroup的作用就是不断地accept到新的连接，将新的连接丢给workerGroup来处理</p><p>而BIO(OIO)方式服务器端虽然还是通过一个BOSS线程来处理等待链接的接入，但是客户端是由主线程直接connect,另外写数据C/S两端都是直接主线程写，而数据读操作是通过一个WORKER 线程BLOCK方式读取(一直等待，直到读到数据，除非channel关闭)。</p><p>网络动作归结到最简单就是服务器端bind-&gt;accept-&gt;read-&gt;write,客户端 connect-&gt;read-&gt;write,一般bind或者connect后会有多次read、write。这种特性导致，bind,accept与read,write的线程分离，connect与read、write线程分离，这样做的好处就是无论是服务器端还是客户端吞吐量将有效增大，以便充分利用机器的处理能力，而不是卡在网络连接上，不过一旦机器处理能力充分利用后，这种方式反而可能会因为过于频繁的线程切换导致性能损失而得不偿失，并且这种处理模型复杂度比较高。</p><p>采用什么样的网络事件响应处理机制对于网络吞吐量是非常重要的，Netty采用的是标准的SEDA（Staged Event-Driven Architecture）架构[<a href="http://en.wikipedia.org/wiki/" target="_blank" rel="noopener">http://en.wikipedia.org/wiki/</a> Staged_event-driven_architecture]，其所设计的事件类型，代表了网络交互的各个阶段，并且在每个阶段发生时，触发相应事件交给初始化时生成的pipeline实例进行处理。事件处理都是通过Channels类的静态方法调用开始的，将事件、channel传递给 channel持有的Pipeline进行处理，Channels类几乎所有方法都为静态，提供一种Proxy的效果(整个工程里无论何时何地都可以调用其静态方法触发固定的事件流转,但其本身并不关注具体的处理流程)。</p><h2 id="ByteBuf"><a href="#ByteBuf" class="headerlink" title="ByteBuf"></a>ByteBuf</h2><ul><li><p>零拷贝(Zero Copy)</p><p>  OS层次上Zero-copy,就是在操作数据时,不需要将数据buffer从一个内存区域拷贝到另一个内存区域(从内核空间拷贝到用户空间),因为减少了一次内存的拷贝,因此CPU的效率得到了提升.</p><ul><li><p>默认情况下的数据I/O:</p><p>  基本操作就是循环的从磁盘读入文件内容到缓冲区，再将缓冲区的内容发送到socket。但是由于Linux的I/O操作默认是缓冲I/O。这里面主要使用的也就是read和write两个系统调用，我们并不知道操作系统在其中做了什么。实际上在以上I/O操作中，发生了多次的数据拷贝。</p><p>  当应用程序访问某块数据时，操作系统首先会检查，是不是最近访问过此文件，文件内容是否缓存在内核缓冲区，如果是，操作系统则直接根据read系统调用提供的buf地址，将内核缓冲区的内容拷贝到buf所指定的用户空间缓冲区中去。如果不是，操作系统则首先将磁盘上的数据拷贝的内核缓冲区，这一步目前主要依靠DMA来传输，然后再把内核缓冲区上的内容拷贝到用户缓冲区中。<br>  接下来，write系统调用再把用户缓冲区的内容拷贝到网络堆栈相关的内核缓冲区中，最后socket再把内核缓冲区的内容发送到网卡上。</p><p>   <img src="/img/OS_data_copy.webp" srcset="/img/loading.gif" alt=""></p><p>而零拷贝则可以通过<code>mmap</code> <code>sendfile</code> <code>splice</code>等系统调用来减少中间的I/O次数 从而提高cpu的利用效率 关这几种syscall的信息可见<a href="https://www.jianshu.com/p/fad3339e3448" target="_blank" rel="noopener">简书</a></p><p>Netty的zero-copy体现在很多方面.比如Buffer的<code>compose</code>,<code>duplicate</code>,<code>slice</code>操作时不会拷贝底层的数据.而是通过ByteBuf对象的组合来实现上述的操作</p><p>Netty提供了<code>CompositeByteBuf</code>类,可以将多个ByteBuf组合成一个逻辑上的Buffer,避免了各个buffer之间的拷贝,<code>CompositeByteBuf</code>并不拥有底层的数据,而是通过拥有两个buffer对象,从这两个buffer对象中获取数据来对外提供看似合并了的数据.比如我们将一份协议数据的头部buffer和消息体buffer合并成一个Buffer. 所有底层的数据还是存储在header和body这两个真实的buffer中.</p><p>另外Netty使用wrap操作将byte数组转化为ByteBuf对象时,将byte数组包裹到对象中(通过持有该byte数组的引用),而不是拷贝数组存放到对象中.</p><p>这种共享底层数据结构减少复制的内存与CPU开销的设计思想在很多语言中都有所体现 比如Go(Python?的可能也是)中的切片</p><p><img src="/img/netty_composite_buf.webp" srcset="/img/loading.gif" alt=""></p></li></ul></li></ul><h3 id="ByteBuf的回收机制"><a href="#ByteBuf的回收机制" class="headerlink" title="ByteBuf的回收机制"></a>ByteBuf的回收机制</h3><p>Netty在实现ByteBuf时采用了引用计数法进行ByteBuf的回收，使用引用计数法进行回收的ByteBuf都扩展了<code>AbstractReferenceCountedByteBuf</code>类，在使用<code>AbstractReferenceCountedByteBuf</code>时需要调用<br><code>AbstractReferenceCountedByteBuf.retain</code>方法递增引用计数器，在使用完毕时则需要调用<code>AbstractReferenceCountedByteBuf.release</code>方法递减引用计数器，当计数器为0时，会进行ByteBuf的回收工作：池化的ByteBuf不会进行实际的内存释放，会将占用的内存归还给内存池，非池化的ByteBuf则会直接释放内存（为了叙述简单，后面释放内存则指真正释放内存或者将内存归还给内存池）。</p><p>推荐阅读<a href="https://www.jianshu.com/p/7a3bd7b536e6" target="_blank" rel="noopener">该文章</a></p><h2 id="Nettty-NioEventLoop对JAVA-Selector-select可能造成的空循环bug的解决"><a href="#Nettty-NioEventLoop对JAVA-Selector-select可能造成的空循环bug的解决" class="headerlink" title="Nettty NioEventLoop对JAVA Selector.select可能造成的空循环bug的解决"></a>Nettty NioEventLoop对JAVA Selector.select可能造成的空循环bug的解决</h2><p>相关代码位于<code>NioEventLoop::select</code>中</p><p>netty对于发生空循环的判断条件如下:</p><ol><li>selector.select(timeoutMillis) 阻塞时间小于 timeoutMillis，且</li><li>select 执行次数 &gt; 阈值（默认 512） Netty 提供了<code>io.netty.selectorAutoRebuildThreshold</code>参数 可通过此参数设置select操作执行次数的阈值</li></ol><p>检测逻辑:</p><pre><code class="java">                long time = System.nanoTime();                if (time - TimeUnit.MILLISECONDS.toNanos(timeoutMillis) &gt;= currentTimeNanos) {                    // timeoutMillis elapsed without anything selected.                    selectCnt = 1;                } else if (SELECTOR_AUTO_REBUILD_THRESHOLD &gt; 0 &amp;&amp;                        selectCnt &gt;= SELECTOR_AUTO_REBUILD_THRESHOLD) {                    // The selector returned prematurely many times in a row.                    // Rebuild the selector to work around the problem.                    logger.warn(                            &quot;Selector.select() returned prematurely {} times in a row; rebuilding Selector {}.&quot;,                            selectCnt, selector);                    // 一旦满足上述两个条件就重建selector 并跳出循环                    rebuildSelector();                    selector = this.selector;                    // Select again to populate selectedKeys.                    selector.selectNow();                    selectCnt = 1;                    break;                }</code></pre><p>重建Selecttor实现在<code>NioEventLoop::rebuildSelector0</code>中 大概做了以下几件事</p><ol><li>新建一个 Selector；</li><li>将旧 Selector 的所有 channel 注册到新 Selector 上；</li><li>关闭旧 Selector；</li></ol><h2 id="番外-关于DirectByteBuffer的回收问题"><a href="#番外-关于DirectByteBuffer的回收问题" class="headerlink" title="[番外]关于DirectByteBuffer的回收问题"></a>[番外]关于DirectByteBuffer的回收问题</h2><p>众所周知 DirectByteBuffer使用的使用的是堆外内存 JVM进行GC时不会对由Unsafe分配的堆外内存进行回收 那么DirectByteBuffer的回收是如何处理的呢?</p><p>其构造函数:</p><pre><code class="java">    DirectByteBuffer(int cap) {                   // package-private        super(-1, 0, cap, cap);        boolean pa = VM.isDirectMemoryPageAligned();        int ps = Bits.pageSize();        long size = Math.max(1L, (long)cap + (pa ? ps : 0));        Bits.reserveMemory(size, cap);        long base = 0;        try {            base = unsafe.allocateMemory(size);        } catch (OutOfMemoryError x) {            Bits.unreserveMemory(size, cap);            throw x;        }        unsafe.setMemory(base, size, (byte) 0);        if (pa &amp;&amp; (base % ps != 0)) {            // Round up to page boundary            address = base + ps - (base &amp; (ps - 1));        } else {            address = base;        }        cleaner = Cleaner.create(this, new Deallocator(base, size, cap));        att = null;    }</code></pre><p>可以看到其通过<code>UNSAFE</code>进行直接内存的分配和初始化 而最后创建的<code>Cleaner</code>与<code>Deallocator</code>则是内存回收的关键</p><p>先说<code>Deallocator</code>其实现了Runnable接口 并在run方法中通过<code>UNSAFE</code>进行内存的释放</p><pre><code class="java">    private static class Deallocator        implements Runnable    {        private static Unsafe unsafe = Unsafe.getUnsafe();        private long address;        private long size;        private int capacity;        private Deallocator(long address, long size, int capacity) {            assert (address != 0);            this.address = address;            this.size = size;            this.capacity = capacity;        }        public void run() {            if (address == 0) {                // Paranoia                return;            }            unsafe.freeMemory(address);            address = 0;            Bits.unreserveMemory(size, capacity);        }    }</code></pre><p>其最后一行<code>Bits.unreserveMemory(size, capacity);</code> 代码的<code>Bits</code>是个包可见的类 而调用的这个方法则是对<code>Bits</code>中记录的一些基础信息进行更新 包括已分配的直接内存大小 已分配的直接内存块数等</p><p>再说这个<code>Clearner</code>其是<code>PhantomReference&lt;Object&gt;</code>(虚引用)的子类</p><p><img src="/img/Cleaner_.png" srcset="/img/loading.gif" alt=""></p><p>其内部通过一个双端链表对所有的Cleaner进行组织</p><pre><code class="java">    // Doubly-linked list of live cleaners, which prevents the cleaners    // themselves from being GC&#39;d before their referents    //    static private Cleaner first = null;    private Cleaner        next = null,        prev = null;</code></pre><p><code>Reference</code> 其是所有<code>Reference</code>的父类 本身被组织为队列形式 以其内部的静态字段<code>pending</code>作为队头 在<code>Reference</code>的静态初始化代码段中 启动了一个<code>ReferenceHandler</code>线程 该线程用一个死循环不停的调用<code>tryHandlePending</code>方法 时刻回收被挂到pending上面的虚引用。 当DirectByteBuffer对象仅被Cleaner引用时, Cleaner被放入pending队列, 之后调用Cleaner.clean()队列 </p><pre><code class="java">        ThreadGroup tg = Thread.currentThread().getThreadGroup();        for (ThreadGroup tgn = tg;             tgn != null;             tg = tgn, tgn = tg.getParent());        Thread handler = new ReferenceHandler(tg, &quot;Reference Handler&quot;);        /* If there were a special system-only priority greater than         * MAX_PRIORITY, it would be used here         */        handler.setPriority(Thread.MAX_PRIORITY);        handler.setDaemon(true);        handler.start();        // provide access in SharedSecrets        SharedSecrets.setJavaLangRefAccess(new JavaLangRefAccess() {            @Override            public boolean tryHandlePendingReference() {                return tryHandlePending(false);            }        });</code></pre><p>关于IO操作为什么不直接使用堆内内存:</p><blockquote><p>因为堆内内存会发生GC移动操作, 对象移动后, 其绝对内存地址也会发生改变, 而gc时对象移动操作很频繁, 不可能每次移动堆内数据, IO时缓存的buffer也跟着一起移动 而IO操作直接使用堆外内存则没有了这一限制。同时jvm中IO操作的Buffer必须是DirectBuffer(可查看IO.write/read函数)。–<a href="https://kkewwei.github.io/elasticsearch_learning/2018/07/27/DirectByteBuffer%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98%E8%AF%A6%E8%A7%A3/" target="_blank" rel="noopener">来自此处</a></p></blockquote><h2 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h2><p>Netty对内存使用与管理的优化项之一</p><p>由于IO操作通常直接使用直接内存 而直接内存的申请与释放操作开销较大 故通过池化内存减少内存申请与释放的操作 减少垃圾回收的次数 以提高性能</p><p>Netty中四种类型的内存: <code>PoolDirectBuf、UnpoolDirectBuf、PoolHeapBuf、UnpoolHeapBuf</code> m默认使用<code>PoolDirectBuf</code> 这些内存使用<code>PoolArena</code>进行管理</p><p><code>PooledByteBufAllocator</code>中的分配直接内存的操作</p><pre><code class="java">    @Override    protected ByteBuf newDirectBuffer(int initialCapacity, int maxCapacity) {        PoolThreadCache cache = threadCache.get();        PoolArena&lt;ByteBuffer&gt; directArena = cache.directArena;        final ByteBuf buf;          if (directArena != null) {            buf = directArena.allocate(cache, initialCapacity, maxCapacity);        } else {            buf = PlatformDependent.hasUnsafe() ?                    UnsafeByteBufUtil.newUnsafeDirectByteBuf(this, initialCapacity, maxCapacity) :                    new UnpooledDirectByteBuf(this, initialCapacity, maxCapacity);        }        return toLeakAwareBuffer(buf);    }</code></pre><p>其主要做了两件事:</p><ul><li>获得当前线程的<code>PoolThreadCache</code></li><li>调用当前线程绑定的<code>DirectArena</code>实例的<code>allocate</code>方法分配内存</li></ul><h3 id="PoolArena"><a href="#PoolArena" class="headerlink" title="PoolArena"></a>PoolArena</h3><p>Netty默认会产生<code>NioEventLoop |work|</code>个PoolArena块, 至于该线程绑定哪个PoolArena 则是根据和PoolArena绑定的线程的数量来决定的。 被越少的线程绑定到一起, 分配内存发生冲突的概率越小。 这里选择被绑定次数最低的那个PoolArea来构建PoolThreadCache。<br>至此, 线程与PoolThreadCache实现了一一绑定。 之后该线程分配内存, 都会利用PoolThreadLocalCache.get()获取PoolThreadCache, 然后利用里面的PoolArea来完成的。</p><p>线程分配内存主要从两个地方分配: <code>PoolThreadCache</code>和<code>PoolArena</code> 前者为线程独享 后者为共享</p><p><img src="/img/Netty_Thread_MEM_ALLOCATION.png" srcset="/img/loading.gif" alt=""></p><p>真正申请内存时调用<code>PoolArena::allocate()</code>方法</p><p><code>PoolArena::allocate()</code>分配内存主要考虑先尝试从缓存中, 然后再尝试从PoolArena分配。tiny和small申请过程一样, 以下都以tiny申请为例。具体过程如下:</p><ol><li>对申请的内存进行规范化, 就是说只能申请某些固定大小的内存, 比如tiny范围的16b倍数的内存, small范围内512b, 1k, 2k, 4k范围内存, normal范围内8k, 16k,…, 16m范围内内存, 始终是2幂次方的数据。申请的内存不足16b的,按照16b去申请。</li><li>判断是否是小于8K的内存申请, 若是申请Tiny/Small级别的内存:</li></ol><p>Netty根据申请内存的大小分为了3种情况:</p><pre><code class="java">//定义在PoolArena中    enum SizeClass {        Tiny,//[16~512)        Small,//[512~8K)        Normal//[8K-16MB]    }</code></pre><p><code>PoolArena</code>的属性:</p><pre><code class="java">//tiny级别的个数, 每次递增2^4b, tiny总共管理32个等级的小内存片:[16, 32, 48, ..., 496], 注意实际只有31个级别内存块static final int numTinySubpagePools = 512 &gt;&gt;&gt; 4;//全局默认唯一的分配者, 见PooledByteBufAllocator.DEFAULTfinal PooledByteBufAllocator parent;// log(16M/8K) = 11,指的是normal类型的内存等级, 分别为[8k, 16k, 32k, ..., 16M]private final int maxOrder;//默认8kfinal int pageSize;//log(8k) =  13final int pageShifts;//默认16Mfinal int chunkSize;//-8192final int subpageOverflowMask;//指的是small类型的内存等级: pageShifts - log(512) = 4,分别为[512, 1k, 2k, 4k]final int numSmallSubpagePools; //small类型分31个等级[16, 32, ..., 512], 每个等级都可以存放一个链(元素为PoolSubpage), 可存放未分配的该范围的内存块private final PoolSubpage&lt;T&gt;[] tinySubpagePools; //small类型分31个等级[512, 1k, 2k, 4k], 每个等级都可以存放一个链(元素为PoolSubpage), 可存放未分配的该范围的内存块private final PoolSubpage&lt;T&gt;[] smallSubpagePools;//存储1024-8096大小的内存 //存储chunk(16M)使用率的内存块, 不同使用率的chunk, 存放在不同的对象中private final PoolChunkList&lt;T&gt; q050;    ////存储内存利用率50-100%的chunkprivate final PoolChunkList&lt;T&gt; q025;   //存储内存利用率25-75%的chunkprivate final PoolChunkList&lt;T&gt; q000;   //存储内存利用率1-50%的chunkprivate final PoolChunkList&lt;T&gt; qInit;  //存储内存利用率0-25%的chunkprivate final PoolChunkList&lt;T&gt; q075;    //存储内存利用率75-100%的chunkprivate final PoolChunkList&lt;T&gt; q100;   //存储内存利用率100%的chunk// Number of thread caches backed by this arena. 该PoolArea被多少线程(NioEventLoop)引用。final AtomicInteger numThreadCaches = new AtomicInteger();</code></pre><p>tinySubpagePools分配[16b, 496b]之间的内存大小, 数组中每个元素以16b为一个单位增长, 比如申请分配16b的内存, 将在下标为0对应的链中分配; 申请32b的内存, 将在下标为1对应的链中分配。</p><p>smallSubpagePools分配[512b, 4k]之间的内存大小, 分配结构同tinySubpagePools一样。</p><p>q050、q025、q000、qInit、q075主要负责分配[8k, 16M]大小的内存, 其存放的元素都是大小为16M的PoolChunk, 这几个成员变量不同的是元素PoolChunk的使用率不同, 比如q025里面存放的chunk使用率为[25%, 75%]。 若需要申请[16b, 4k]的内存、而tinySubpagePools、smallSubpagePools没有合适的内存块时, 会从这些对象包含的PoolChunk中分配8k的叶子节点供重新划分结构进行分配。</p><p>他们存储的属性PoolChunk可以在不同的属性中移动, 其中:</p><ul><li>若q025中某个PoolChunk使用率大于25%之后, 该PoolChunk将别移动到q050中。</li><li>若q050中某个PoolChunk使用率小于50%之后, 该PoolChunk将别移动到q025中。</li><li>若qInit使用率为0, 也不会释放该节点。</li><li>若q000使用率为0, 会被释放掉。</li></ul><p>对于大于16MB的内存申请 将直接通过allocateHuge()从内存池外分配内存。</p><p>具体的分配操作:</p><pre><code class="java">    // Method must be called inside synchronized(this) { ... } blockprivate void allocateNormal(PooledByteBuf&lt;T&gt; buf, int reqCapacity, int normCapacity) {        if (q050.allocate(buf, reqCapacity, normCapacity) || q025.allocate(buf, reqCapacity, normCapacity) ||            q000.allocate(buf, reqCapacity, normCapacity) || qInit.allocate(buf, reqCapacity, normCapacity) ||            q075.allocate(buf, reqCapacity, normCapacity)) {            return;        }        // Add a new chunk.        PoolChunk&lt;T&gt; c = newChunk(pageSize, maxOrder, pageShifts, chunkSize);        long handle = c.allocate(normCapacity);        assert handle &gt; 0;        c.initBuf(buf, handle, reqCapacity);        qInit.add(c);    }boolean allocate(PooledByteBuf&lt;T&gt; buf, int reqCapacity, int normCapacity) {    if (head == null || normCapacity &gt; maxCapacity) { //head是可以直接寸数据的        // Either this PoolChunkList is empty or the requested capacity is larger then the capacity which can        // be handled by the PoolChunks that are contained in this PoolChunkList.        return false;    }    for (PoolChunk&lt;T&gt; cur = head;;) {        long handle = cur.allocate(normCapacity); //取得哪个坐标下的某个值        if (handle &lt; 0) { //在poolchunk中没有找到能装得下的，那么继续找下一个            cur = cur.next;            if (cur == null) {                return false;            }        } else {            cur.initBuf(buf, handle, reqCapacity);            if (cur.usage() &gt;= maxUsage) {//chunked量用超了则移动向下一个链                remove(cur);                nextList.add(cur);            }            return true;        }    }}</code></pre><p>会轮训该链所有PoolChunk, 直到找到一个符合要求的内存块, 当分配完成后, 检查该PoolChunk是否因为使用率超过阈值需要放到别的队列中。</p><p>若没有找到, 会去内存中申请一个PoolChunk的内存块, 在该PoolChunk中分配normCapacity大小的内存, 参考见Netty PoolChunk原理探究<br>对PoolChunk进行初始化, 并将该PoolChunk加入qInit的链中。</p><p>这里有一个细节需要了解下 注意<code>allocateNormal</code>的<code>if (q050.allocate(buf, reqCapacity, normCapacity) || q025.allocate(buf, reqCapacity, normCapacity) || q000.allocate(buf, reqCapacity, normCapacity) || qInit.allocate(buf, reqCapacity, normCapacity) || q075.allocate(buf, reqCapacity, normCapacity)) { return; }</code>语句, <strong>q050、q025、q000、qInit、q075按照这个顺序排序, 也就是说当在这几个对象都有可分配的内存时, 优先从 q050中分配, 最后从q075中分配。这样安排的考虑是:将PoolChunk分配维持在较高的比例上。保存一些空闲度比较大的内存, 以便大内存的分配。</strong></p><h3 id="PoolChunk"><a href="#PoolChunk" class="headerlink" title="PoolChunk"></a>PoolChunk</h3><p>对于大于8K的内存申请Netty采用<code>PoolChunk</code>处理</p><p>Netty进行内存分配分配时默认一次分配16MB的通过<code>PoolChunk</code>(<code>PoolChunk</code>本身则是双端链表的一个节点)组织的内存 <code>PoolChunk</code>通过<strong>完全二叉平衡树</strong>来对内存进行管理 当申请一块内存块时 会在树中进行查找 并将找到的第一块足够大小的内存区域 以long型的偏移量形式返回 然后将该内存块标记为<code>reserved</code> 以确保该内存块只被一个<code>ByteBuf</code>所使用 若申请的内存大小大于<code>chunkSize</code>则返回大于申请内存的最接近该值的2的幂次方的内存区域 以简化内存块的管理和内存区域的规整</p><p>该二叉树将PoolChunk分11层, 第一层为1个16M, 第二层为2个8MB,第三层为4个4MB的内存块, 直到第11层为2048个8KB的内存块, 8kb的内存块称之为page。</p><ul><li>如果我们申请16M的内存, 那么将直接在该二叉树第一层申请。</li><li>若申请32K的内存, 那么在该二叉树第8层申请。</li><li>若申请8K的内存, 那么将直接在第11层申请。</li></ul><p><img src="/img/Netty_PoolChunk_struct.png" srcset="/img/loading.gif" alt=""></p><p>该二叉平衡树被编码为数组形式的<code>meomoryMap</code> <code>memoryMap[id] = x</code>意味着根节点位于该<code>id</code>的子树 的第一个可用内存块位于深度<code>x</code> (in the subtree rooted at id, the first node that is free to be allocated is at depth x (counted from depth=0))</p><p>当我们分配和释放节点的时候就对该<code>memoryMap</code>进行更新</p><p><code>memoryMap</code>按如下初始化:<code>memoryMap[id] = depth_of_id</code></p><p>对于每个子树有如下规则</p><ul><li><code>memoryMap[id] = depth_of_id</code>  =&gt; 以该节点为根节点节点的子树上所有的块都还没有被分配</li><li><code>memoryMap[id] &gt; depth_of_id</code>  =&gt; 至少有一块被分配了 但是仍有可能存在未被分配的块</li><li><code>memoryMap[id] = maxOrder + 1</code> =&gt; 所有节点都被分配 maxOrder为整个二叉平衡数的深度</li></ul><p>另外Netty的很多初始化元信息都位于<code>PlatformDependent</code>final类中 该类包括了 当前运行系统 最大堆外内存量(默认与堆内内存大小相等) 优先使用堆内内存 还是堆外内存等</p><p><a href="https://kkewwei.github.io/elasticsearch_learning/2019/01/12/Netty%E5%86%85%E5%AD%98%E5%9B%9E%E6%94%B6%E5%8E%9F%E7%90%86%E8%A7%A3%E8%AF%BB/" target="_blank" rel="noopener">关于堆外内存的回收流程</a></p><p><img src="/img/Netty_direct_memory_recycle.png" srcset="/img/loading.gif" alt=""></p><h3 id="PoolSubPage"><a href="#PoolSubPage" class="headerlink" title="PoolSubPage"></a>PoolSubPage</h3><p>Netty中申请小于8K的内存 将使用<code>PoolSubpage</code> 以避免内存浪费 </p><p>每一个PoolSubpage都会与PoolChunk里面的一个叶子节点映射起来, 然后将PoolSubpage根据用户申请的ElementSize化成几等分, 之后只要再次申请ElementSize大小的内存, 将直接从这个PoolSubpage中分配。(遍历bitmap找到还未使用的即可)</p><pre><code class="java">    PoolSubpage(PoolSubpage&lt;T&gt; head, PoolChunk&lt;T&gt; chunk, int memoryMapIdx, int runOffset, int pageSize, int elemSize) {        this.chunk = chunk;        this.memoryMapIdx = memoryMapIdx;        this.runOffset = runOffset;        this.pageSize = pageSize;        bitmap = new long[pageSize &gt;&gt;&gt; 10]; // pageSize / 16 / 64        init(head, elemSize);    }    void init(PoolSubpage&lt;T&gt; head, int elemSize) {        doNotDestroy = true;        this.elemSize = elemSize;        if (elemSize != 0) {            maxNumElems = numAvail = pageSize / elemSize;            nextAvail = 0;            bitmapLength = maxNumElems &gt;&gt;&gt; 6;            if ((maxNumElems &amp; 63) != 0) {                bitmapLength ++;            }            for (int i = 0; i &lt; bitmapLength; i ++) {                bitmap[i] = 0;            }        }        addToPool(head);    }</code></pre><p>关于<code>bitmap = new long[pageSize &gt;&gt;&gt; 10];</code></p><p>一个long型有64个位 2的6次方 而Netty申请内存的最小单位是16Bytes=2的四次方 故最多只需要pageSize &gt;&gt;&gt; 10 个long 作为位图 就能够准确的表示所申请的page的使用情况</p><p>然后再调用init来对PoolSubpage结构进行初始化:</p><ol><li>总共可以分成pageSize/elemSize个element。 bitmap所有元素也不一定需要全部用上, 实际会用maxNumElems &gt;&gt;&gt; 6个long就可以了。</li><li>然后根据头插法将该PoolSubpage插入tinySubpagePools或者smallSubpagePools(参考Netty PoolArea原理探究)对应级别的链中。</li></ol><h3 id="PoolThreadCache"><a href="#PoolThreadCache" class="headerlink" title="PoolThreadCache"></a>PoolThreadCache</h3><p>NioEventLoop在为数据分配存放的内存时, 会首先尝试从线程本地缓存中去申请, 只有当本地缓存中申请失败, 才会考虑从全局内存中申请, 本地缓存的管理者就是PoolThreadCache对象。 Netty自己实现了类似LocalThread的类来充当线程缓存: PoolThreadLocalCache, 本节将充分围绕这两个类的源代码进行描述。</p><p><img src="/img/Netty_PoolThreadCache2.png" srcset="/img/loading.gif" alt=""></p><p><code>normalHeapCaches</code>只缓存[8k, 16k, 32k]大小的内存块, 每个元素对应的缓存queue个数不超过64个。 normal最大内存块为16m, 而缓存仅仅缓存最大32k内存的原因是这是一种巨大的开销: 试想仅仅16m对应的级别存储, 就可缓存16M*64大小的内存块放在内存, 而这些内存块等着被新分配出去而没有主动释放, 存在巨大的浪费。</p><p><code>PoolThreadCache</code>的构造函数 可以看出directArena与PoolThreadCache绑定了, 同时PoolThreadCache也与某个NioEventLoop对应的线程绑定的, 所以该NioEventLoop线程都与唯一的directArena(&amp;heapArena)绑定着, 这样相对减轻了线程间申请内存导致互斥的发生。</p><pre><code class="java">private static &lt;T&gt; MemoryRegionCache&lt;T&gt;[] createNormalCaches(           int cacheSize, int maxCachedBufferCapacity, PoolArena&lt;T&gt; area) {       if (cacheSize &gt; 0) {           int max = Math.min(area.chunkSize, maxCachedBufferCapacity); //默认32k           //normalHeapCaches 数组中的元素的大小，是以2的幂倍pageSize递增的           int arraySize = Math.max(1, log2(max / area.pageSize) + 1);           //只缓存8k，16k，32k的缓存，太大的话，内存扛不住，若最大缓存32m的话，缓存64*32M个，太大了，扛不住           @SuppressWarnings(&quot;unchecked&quot;)           MemoryRegionCache&lt;T&gt;[] cache = new MemoryRegionCache[arraySize];           for (int i = 0; i &lt; cache.length; i++) {               cache[i] = new NormalMemoryRegionCache&lt;T&gt;(cacheSize);           }           return cache;       } else {           return null;       }   }</code></pre><p>PoolThreadCache主要解决了了如何从本地缓存分配内存, 而本地缓存如何与该线程联系在一起的呢? 这就是<code>PoolThreadLocalCache起</code>的作用。</p><p><code>PoolThreadLocalCache</code>是<strong>全局唯一</strong>的, 任何线程分配内存, 都会调用同一个PoolThreadLocalCache.get()获取PoolThreadCache。 PoolThreadLocalCache继承了FastThreadLocal, PoolThreadLocalCache.get()实际调用了FastThreadLocal.get()方法:</p><pre><code class="java">    public final V get() {        return get(InternalThreadLocalMap.get());    }</code></pre><p>在InternalThreadLocalMap中定义了slowThreadLocalMap属性, 该类型是我们熟悉的ThreadLocal。</p><pre><code class="java">static final ThreadLocal&lt;InternalThreadLocalMap&gt; slowThreadLocalMap = new ThreadLocal&lt;InternalThreadLocalMap&gt;();</code></pre><p>而该方法经过一系列调用链最终会调用<code>PoolThreadLocalCache</code>的<code>initValue</code>方法</p><pre><code class="java">        @Override        protected synchronized PoolThreadCache initialValue() {            final PoolArena&lt;byte[]&gt; heapArena = leastUsedArena(heapArenas);            final PoolArena&lt;ByteBuffer&gt; directArena = leastUsedArena(directArenas);            if (useCacheForAllThreads || Thread.currentThread() instanceof FastThreadLocalThread) {                return new PoolThreadCache(                        heapArena, directArena, tinyCacheSize, smallCacheSize, normalCacheSize,                        DEFAULT_MAX_CACHED_BUFFER_CAPACITY, DEFAULT_CACHE_TRIM_INTERVAL);            }            // No caching for non FastThreadLocalThreads.            return new PoolThreadCache(heapArena, directArena, 0, 0, 0, 0, 0);        }</code></pre><h2 id="REF"><a href="#REF" class="headerlink" title="REF"></a>REF</h2><p>以上内容整理自</p><ul><li><a href="http://jm.taobao.org/2010/09/25/423/" target="_blank" rel="noopener">http://jm.taobao.org/2010/09/25/423/</a></li><li><a href="https://www.jianshu.com/c/3a70d53ebdf3" target="_blank" rel="noopener">简书Netty系列文章</a>For simplicity all sizes are no</li><li><a href="https://kkewwei.github.io/elasticsearch_learning/categories/Netty/" target="_blank" rel="noopener">kkewwei的github pages</a></li><li>Netty文档</li></ul>]]></content>
    
    
    <categories>
      
      <category>源码</category>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Netty</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RocketMq 技术内幕读书笔记</title>
    <link href="/2020/01/04/RocketMQ%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/index/"/>
    <url>/2020/01/04/RocketMQ%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/index/</url>
    
    <content type="html"><![CDATA[<h1 id="RocketMq-技术内幕读书笔记"><a href="#RocketMq-技术内幕读书笔记" class="headerlink" title="RocketMq 技术内幕读书笔记"></a>RocketMq 技术内幕读书笔记</h1><h2 id="NameServer路由"><a href="#NameServer路由" class="headerlink" title="NameServer路由"></a>NameServer路由</h2><p><img src="/img/ns_route_reg_del.png" srcset="/img/loading.gif" alt="route"></p><p>NameServer存储了路由表及相关的元数据</p><p>NameServer相关的两个比较重要的类:</p><ol><li><code>org.apache.rocketmq.namesrv.routeinfo.RouteInfoManager</code>存储/管理若干路由数据</li><li><code>org.apache.rocketmq.namesrv.NamesrvController</code></li></ol><p><code>RouteInfoManager</code>所存储的信息包括</p><pre><code class="java">private final HashMap&lt;String/* topic */, List&lt;QueueData&gt;&gt; topicQueueTable;private final HashMap&lt;String/* brokerName */, BrokerData&gt; brokerAddrTable;private final HashMap&lt;String/* clusterName */, Set&lt;String/* brokerName */&gt;&gt; clusterAddrTable;private final HashMap&lt;String/* brokerAddr */, BrokerLiveInfo&gt; brokerLiveTable;private final HashMap&lt;String/* brokerAddr */, List&lt;String&gt;/* Filter Server */&gt; filterServerTable;</code></pre><p><code>brokerLiveTable</code>用于记录Broker的状态信息 ns每次收到心跳包时会替换该信息 <code>BrokerLiveInfo</code>的<code>lastUpdateTimestamp</code>字段记录了上次收到该Broker的心跳消息的时间</p><p><code>filterServerTable</code>用于记录类模式消息过滤</p><p>RocketMQ基于<strong>订阅-发布</strong>机制 一个Topic拥有多个消息队列 一个Broker为每一主题默认创建4个读队列 4个写队列(这是很老的版本 稍微新一点的似乎是8个读队列 8个写队列) 多个Broker组成一个集群 BrokerName由相同的多台Broker组成Master-Slave架构 brokerId为0 代表Master 大于0 代表Slave</p><h3 id="路由注册"><a href="#路由注册" class="headerlink" title="路由注册"></a>路由注册</h3><p>RocketMQ路由注册是通过Broker与NS的心跳功能实现的 Broker启动时向集群中的所有的NameServer发送心跳语句 之后以30s/个的频率向所有的NS发送心跳包 NS收到心跳包后更新<code>BrokerLiveInfo</code>的<code>lastUpdateTimestamp</code>的字段 并且NS每隔10s扫描<code>brokerAddrTable</code> 如果发现超过120s没有收到心跳包的broker 就移除其路由信息并关闭Socket连接(<strong>这里就引出了另一个问题: Ns要等Broker失效120s才会将其从路由表剔除 如果在Broker故障期间 Consumer依然根据原来的路由信息 向已宕机的Broker发送消息 那么必然会失败 如何处理</strong>)</p><p>NS处理心跳包的代码实现在<code>RouteInfoManager::registerBroker</code>方法中 其定期扫描<code>brokerLiveTable</code>的方法在<code>RouteInfoManager::scanNotActiveBroker</code> 由<code>NamesrvController::initialize</code>调用</p><pre><code class="java">this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() {            @Override            public void run() {                NamesrvController.this.routeInfoManager.scanNotActiveBroker();            }}, 5, 10, TimeUnit.SECONDS);</code></pre><h3 id="路由发现"><a href="#路由发现" class="headerlink" title="路由发现"></a>路由发现</h3><p>RocketMQ的路由发现是<strong>非实时</strong>的 当Topic路由出现变化后NameServer不主动推送给客户端 而是由客户端定时拉取最新路由 拉取路由信息的命令为<code>GET_ROUTEINTO_BY_TOPIC</code>其定义在<code>RequestCode</code>中</p><p>由<code>DefaultRequestProcessor</code>处理收到的来自客户端的拉取路由的消息 其对消息进行基本的解码 然后调用<code>NamesrvController</code>控制类获得<code>RouteInfoManager</code> 最后调用 <code>RouteInfoManager::pickupTopicRouteData</code>方法对<code>RouteInfoManager</code>中所存储的Topic相关的Broker集群以及具体的Broker进行检索并返回相关数据 这个读取Topic相关Broker信息的过程也用读写锁进行了控制 如果查不到Topic的相关信息则返回<code>TOPIC_NOT_EXISTS</code>状态码</p><h2 id="Broker"><a href="#Broker" class="headerlink" title="Broker"></a>Broker</h2><p>to_be_continued</p><h2 id="消息发送"><a href="#消息发送" class="headerlink" title="消息发送"></a>消息发送</h2><p>首先需要考虑几个问题:</p><ol><li>消息队列如何进行负载</li><li>消息发送如何实现高可用</li><li>批量消息发送如何实现一致性</li></ol><p>RocketMQ支持3种消息发送方式:</p><ol><li>同步</li><li>异步</li><li>单向</li></ol><p><code>Message</code>的全属性构造函数</p><p>关于默认的消息存储的一些设置可以在<code>MessageStoreConfig</code>中看到</p><pre><code class="java">public Message(String topic, String tags, String keys, int flag, byte[] body, boolean waitStoreMsgOK)</code></pre><ul><li><p>keys</p><p>  Message 索引键 多个用空格隔开 用以根据key快速检索到消息</p></li><li><p>waitStoreMsgOK</p><p>  消息发送时是否等消息存储完成后再返回</p></li></ul><h3 id="DefaultMQProducer-启动流程"><a href="#DefaultMQProducer-启动流程" class="headerlink" title="DefaultMQProducer 启动流程"></a>DefaultMQProducer 启动流程</h3><p>当我们通过<code>DefaultMQProducer</code>启动一个<code>DefaultMQProducer</code>时 最终会调用<code>DefaultMQProducerImpl</code>的<code>public void start(final boolean startFactory)</code>方法</p><p>该方法首先会检查<code>producerGroup</code>是否符合要求 并改变生产者的<code>instanceName</code>为进程id</p><pre><code class="java">this.checkConfig();// 检查`producerGroup`是否符合要求if (!this.defaultMQProducer.getProducerGroup().equals(MixAll.CLIENT_INNER_PRODUCER_GROUP)) {                    this.defaultMQProducer.changeInstanceNameToPID();}</code></pre><p>获得<code>pid</code>的方法使用了<code>JMX</code> 下面所示的这个方法实现在common包中的<code>UtilAll</code>中</p><pre><code class="java">public static int getPid() {        RuntimeMXBean runtime = ManagementFactory.getRuntimeMXBean();        String name = runtime.getName(); // format: &quot;pid@hostname&quot;        try {            return Integer.parseInt(name.substring(0, name.indexOf(&#39;@&#39;)));        } catch (Exception e) {            return -1;        }}</code></pre><p>然后会创建库客户端实例</p><pre><code class="java">this.mQClientFactory = MQClientManager.getInstance().getAndCreateMQClientInstance(this.defaultMQProducer, rpcHook);</code></pre><p>整个JVM实例中只会存在一个<code>MQClientManager</code>实例</p><pre><code class="java">private static MQClientManager instance = new MQClientManager();</code></pre><p>该实例采取单例模式实现，负责维护MQClientInstance缓存表</p><pre><code class="java">private ConcurrentMap&lt;String/* clientId */, MQClientInstance&gt; factoryTable = new ConcurrentHashMap&lt;String, MQClientInstance&gt;();</code></pre><p>同一个clientid只创建一个instance clientid=客户端ip+instance+unitName 通过刚才所说的将instance设为默认的client rocketmq会将其instance设置为pid 从而避免在一台物理机上部署多个应用程序时 带来混乱</p><p>之后向clientinstance注册当前的生产者 将当前生产者加入到<code>MQClientInstance</code>中从而方便后续网络请求 心跳检测等 这个<code>MQClientInstance</code>是真正负责和NS等打交道的类 其记录了consunmer集群 producer集群 以及topic的路由信息等等 而<code>DefaultMQProducerImpl</code>最主要的功能则是消息的发送 至于<code>DefaultMQProducer</code>则主要是一些producer的基本配置</p><pre><code class="java">boolean registerOK = mQClientFactory.registerProducer(this.defaultMQProducer.getProducerGroup(), this);    if (!registerOK) {            this.serviceState = ServiceState.CREATE_JUST;            throw new MQClientException(&quot;The producer group[&quot; + this.defaultMQProducer.getProducerGroup()             + &quot;] has been created before, specify another name please.&quot; + FAQUrl.suggestTodo(FAQUrl.GROUP_NAME_DUPLICATE_URL),             null);}</code></pre><p>注册逻辑:</p><pre><code class="java">public boolean registerProducer(final String group, final DefaultMQProducerImpl producer) {        if (null == group || null == producer) {            return false;        }        MQProducerInner prev = this.producerTable.putIfAbsent(group, producer);        if (prev != null) {            log.warn(&quot;the producer group[{}] exist already.&quot;, group);            return false;        }        return true;}</code></pre><p>所以它这个代码逻辑应该是我们可以在一个JVM进程（在同一个MQClientInstance中）中启动多个属于不同group的producer 但是不能启动多个相同group的producer 否则就会喜提<code>MQClientException</code>异常 告知我们已经创建了该group</p><p>最后启动MQClientInstance</p><pre><code class="java">if (startFactory) {                    mQClientFactory.start();}</code></pre><p>从ns获得路由信息后 将会循环遍历路由信息的QueueData字段 如果队列没有写权限 则继续遍历下一个QueueData 对于可写的队列 再根据其BrokerName找到BrokerData信息 找不到或者没有找到Master节点 则遍历下一个queuedata 根据写队列的个数 按照topic+序号创建MessageQueue 填充<code>topicPublishInfo</code>的<code>List&lt;QueueMessage&gt;</code> 完成消息发送的路由查找 最终完成路由查找 这一段的逻辑实现在<code>MQClientInstance::topicRouteData2TopicPublishInfo</code>中</p><pre><code class="java">List&lt;QueueData&gt; qds = route.getQueueDatas();            Collections.sort(qds);// 根据brokerName字段进行排序            for (QueueData qd : qds) {                if (PermName.isWriteable(qd.getPerm())) {                    BrokerData brokerData = null;                    for (BrokerData bd : route.getBrokerDatas()) {                        if (bd.getBrokerName().equals(qd.getBrokerName())) {                            brokerData = bd;                            break;                        }                    }                    if (null == brokerData) {                        continue;                    }                    if (!brokerData.getBrokerAddrs().containsKey(MixAll.MASTER_ID)) {                        continue;                    }                    for (int i = 0; i &lt; qd.getWriteQueueNums(); i++) {                        MessageQueue mq = new MessageQueue(topic, qd.getBrokerName(), i);                        info.getMessageQueueList().add(mq);                    }                }            }</code></pre><h3 id="消息发送的基本流程"><a href="#消息发送的基本流程" class="headerlink" title="消息发送的基本流程"></a>消息发送的基本流程</h3><p>主要流程: 验证消息 -&gt; 查找路由 -&gt; 选择消息队列 -&gt; 消息发送 以<code>DefaultMQClientImpl::sendDefaultImpl</code>下手</p><h4 id="消息验证"><a href="#消息验证" class="headerlink" title="消息验证"></a>消息验证</h4><p>该方法一开始即对消息进行了验证</p><pre><code class="java">Validators.checkMessage(msg, this.defaultMQProducer)</code></pre><p><code>checkMessage</code>方法对消息所携带的TOPIC的合法性进行验证 然后检查消息是否为空 长度是否为0 是否超过了该producer允许的最大长度(默认4M)</p><h4 id="查找路由信息"><a href="#查找路由信息" class="headerlink" title="查找路由信息"></a>查找路由信息</h4><p><code>DefaultMQClientImpl::tryToFindTopicPublishInfo</code> 该方法首先会查找<code>topicPublishInfoTable</code>这个map中的缓存 如果没有当前消息的topic信息(TopicPublishInfo) 则将当前topic置入该map 然后调用<code>MQClientInstance::updateTopicRouteInfoFromNameServer</code>方法 尝试从ns获得topic的路由信息 在这期间 MQClientInstance 会使用<strong>ReentrantLock对从ns获取topic的路由信息以及对<code>topicRouteInfo</code>的字段的更新等操作进行加锁</strong> 同时整个方法是<strong>同步进行的</strong> MQClientInstance调用<code>MQClientAPIImpl::getTopicRouteInfoFromNameServer</code>方法 而该方法进一步调用<code>NettyRemotingClient::invokeSync</code>进行同步调用 当从ns也没有找到时路由信息时 尝试使用默认主题去查询 如果 brokercofig的autoCreateTopicEnable被设置为true 则ns将返回路由信息 否则将抛出无法找到topic异常</p><p><code>TopicPublishInfo</code>属性:</p><pre><code class="java">//是否是顺序消息private boolean orderTopic = false;private boolean haveTopicRouterInfo = false;//该主题队列的消息队列private List&lt;MessageQueue&gt; messageQueueList = new ArrayList&lt;MessageQueue&gt;();//每选择一次消息队列 该值会增加1 如果到达Integer.MAX_VALUE 则重置为0private volatile ThreadLocalIndex sendWhichQueue = new ThreadLocalIndex();private TopicRouteData topicRouteData;</code></pre><p><code>TopicRouteData</code>属性</p><pre><code class="java">    private String orderTopicConf;    // 队列元数据    private List&lt;QueueData&gt; queueDatas;    // tpoic分布的broker元数据    private List&lt;BrokerData&gt; brokerDatas;    private HashMap&lt;String/* brokerAddr */, List&lt;String&gt;/* Filter Server */&gt; filterServerTable;</code></pre><p><code>DefaultMQClientImpl::tryToFindTopicPublishInfo</code>:</p><pre><code class="java">    private TopicPublishInfo tryToFindTopicPublishInfo(final String topic) {        TopicPublishInfo topicPublishInfo = this.topicPublishInfoTable.get(topic);        if (null == topicPublishInfo || !topicPublishInfo.ok()) {            this.topicPublishInfoTable.putIfAbsent(topic, new TopicPublishInfo());            this.mQClientFactory.updateTopicRouteInfoFromNameServer(topic);            topicPublishInfo = this.topicPublishInfoTable.get(topic);        }        if (topicPublishInfo.isHaveTopicRouterInfo() || topicPublishInfo.ok()) {            return topicPublishInfo;        } else {            // 当从ns也没有找到时路由信息时 尝试使用默认主题曲查询 如果 brokercofig的autoCreateTopicEnable被设置为true 则ns将返回路由信息 否则将抛出无法找到topic异常            this.mQClientFactory.updateTopicRouteInfoFromNameServer(topic, true, this.defaultMQProducer);            topicPublishInfo = this.topicPublishInfoTable.get(topic);            return topicPublishInfo;        }    }</code></pre><ul><li>关于这个默认主题 与 <code>AUTO_CREATE_TOPIC_KEY_TOPIC</code></li></ul><p>涉及到为什么不推荐在生产环境下开启<code>autoCreateTopicEnable</code>选项</p><p><img src="/img/create_topic_sequence.jpg" srcset="/img/loading.gif" alt="cts"></p><p>经过上面自动创建路由机制的创建流程，我们可以比较容易的分析得出如下结论:</p><p>因为开启了自动创建路由信息，消息发送者根据Topic去NameServer无法得到路由信息，但接下来根据默认Topic从NameServer是能拿到路由信息(在每个Broker中，存在8个队列)，因为两个Broker在启动时都会向NameServer汇报路由信息。此时消息发送者缓存的路由信息是2个Broker，每个Broker默认4个队列（原因见3.2.1:Step2的分析）。消息发送者然后按照轮询机制，发送第一条消息选择(broker-a的messageQueue:0)，向Broker发送消息，Broker服务器在处理消息时，首先会查看自己的路由配置管理器(TopicConfigManager)中的路由信息，此时不存在对应的路由信息，然后尝试查询是否存在默认Topic的路由信息，如果存在，说明启用了autoCreateTopicEnable，则在TopicConfigManager中创建新Topic的路由信息，此时存在与Broker服务端的内存中，然后本次消息发送结束。此时，在NameServer中还不存在新创建的Topic的路由信息</p><p>这里有三个关键点:</p><ol><li>启用autoCreateTopicEnable创建主题时，在Broker端创建主题的时机为，消息生产者往Broker端发送消息时才会创建</li><li>然后Broker端会在一个心跳包周期内，将新创建的路由信息发送到NameServer，于此同时，Broker端还会有一个定时任务，定时将内存中的路由信息，持久化到Broker端的磁盘上</li><li>消息发送者会每隔30s向NameServer更新路由信息，如果消息发送端一段时间内未发送消息，就不会有消息发送集群内的第二台Broker，那么NameServer中新创建的Topic的路由信息只会包含Broker-a，然后消息发送者会向NameServer拉取最新的路由信息，此时就会消息发送者原本缓存了2个broker的路由信息，将会变为一个Broker的路由信息，则该Topic的消息永远不会发送到另外一个Broker，就出现了上述现象</li></ol><p>原因就分析到这里了，现在我们还可以的大胆假设，开启autoCreateTopicEnable机制，什么情况会在两个Broker上都创建队列，其实，我们只需要连续快速的发送9条消息，就有可能在2个Broker上都创建队列</p><h4 id="选择消息队列"><a href="#选择消息队列" class="headerlink" title="选择消息队列"></a>选择消息队列</h4><p>选择消息队列时会调用<code>DefaultMQProducer</code>会调用``MQFaultStrategy::selectOneMessageQueue<code>而该方法实现了对broker的故障延迟机制，然后最终调用</code>TopicPublishInfo::selectOneMessageQueue<code>(该方法代码见下)中 其</code>lastBrokerName<code>就是上一次选择的执行发送消息失败的broker 第一次调用给方法时</code>lastBrokerName<code>为</code>null<code>此时直接用</code>sendWhichQueue`自增取值 再与队列个数取模 返回该位置MQ 如果消息发送再失败的话下次进行消息队列选择时 规避上次MQ所在的Broker 否则很可能再次会失败.</p><pre><code class="java">    public MessageQueue selectOneMessageQueue(final String lastBrokerName) {        if (lastBrokerName == null) {            return selectOneMessageQueue();        } else {            int index = this.sendWhichQueue.getAndIncrement();            for (int i = 0; i &lt; this.messageQueueList.size(); i++) {                int pos = Math.abs(index++) % this.messageQueueList.size();                if (pos &lt; 0)                    pos = 0;                MessageQueue mq = this.messageQueueList.get(pos);                if (!mq.getBrokerName().equals(lastBrokerName)) {                    return mq;                }            }            return selectOneMessageQueue();        }    }    public MessageQueue selectOneMessageQueue() {        int index = this.sendWhichQueue.getAndIncrement();        int pos = Math.abs(index) % this.messageQueueList.size();        if (pos &lt; 0)            pos = 0;        return this.messageQueueList.get(pos);    }</code></pre><ul><li>broker故障延迟机制</li></ul><p>主要代码实现在<code>MQFaultStrategy::selectOneMessageQueue</code>中</p><p>该方法处理流程</p><ol><li>对于消息队列的列表进行轮询获取一个消息队列</li><li>验证该消息队列是否可用 通过<code>LatencyFaultTolerance::isAvailable(String brokerName)</code>判断是否可用 该方法通过判断参数是否在实现类中的faultItem列表中 返回判断结果</li><li>通过<code>LatencyFaultTolerance::pickOneAtLeast()</code>中选择一个mq 判断其是否可用 若可用则从<code>LatencyFaultTolerance</code>中剔除该Topic条目 表明Broker故障已经恢复</li></ol><p>在<code>DefaultMQProducerImpl::sendDefaultImpl</code>这个消息方法中 如果发送过程抛出异常 将调用<code>DefaultMQProducer::updateFaultItem</code>方法 该方法直接调用<code>MQFaultStrategy::updateFaultItem(brokerName, currentLatency, isolation)</code>方法 该方法的第2个参数传入本次消息发送延迟时间 但3个参数设置是否隔离 如果是 则使用默认时长30s来计算BNroker故障规避时长 如果为false 则使用本次消息发送延迟时间来计算Broker故障规避时长</p><pre><code class="java">private long[] latencyMax = {50L, 100L, 550L, 1000L, 2000L, 3000L, 15000L};private long[] notAvailableDuration = {0L, 0L, 30000L, 60000L, 120000L, 180000L, 600000L};public void updateFaultItem(final String brokerName, final long currentLatency, boolean isolation) {        if (this.sendLatencyFaultEnable) {            long duration = computeNotAvailableDuration(isolation ? 30000 : currentLatency);            this.latencyFaultTolerance.updateFaultItem(brokerName, currentLatency, duration);        }    }    private long computeNotAvailableDuration(final long currentLatency) {        for (int i = latencyMax.length - 1; i &gt;= 0; i--) {            if (currentLatency &gt;= latencyMax[i])                return this.notAvailableDuration[i];        }        return 0;    }</code></pre><p>如上面的代码所示<code>computeNotAvailableDuration</code>方法用来计算接下来多久的时间里Broker不会参与消息发送队列负载 具体算法: 从LatencyMax数组尾部开始寻找 找到第一个比currentLatency小的下标 然后从notAvailableDuration数组中获取需要规避的时长 计算完毕后调用<code>LatencyFaultTolerance::updateFaultItem</code>对faultItem进行更新</p><h4 id="进行发送"><a href="#进行发送" class="headerlink" title="进行发送"></a>进行发送</h4><p>消息发送的逻辑实现在<code>DefaultMQClientImpl::sendKernelImpl</code>中 其做了如下工作</p><ol><li><p>根据MQ获取broker的网络地址 如果 <code>MQClientInstance</code> 的<code>brokerAddressTable</code>未缓存该Broker信息 则从NS主动更新以下topic的路由信息 如果路由更新后 还是找不到则抛出<code>MQClientException</code></p></li><li><p>为消息分配全局唯一ID 如果消息体超过4K(这个值定义<code>DefaultMQProducer中</code>的<code>private int compressMsgBodyOverHowmuch = 1024 * 4;</code>) 则会其进行zip压缩 并设置标志位 如果是事务Prepared消息 则设置标志位</p><p> zip压缩代码 实现在<code>UtilAll</code></p><pre><code class="java"> public static byte[] compress(final byte[] src, final int level) throws IOException {     byte[] result = src;     ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(src.length);     java.util.zip.Deflater defeater = new java.util.zip.Deflater(level);     DeflaterOutputStream deflaterOutputStream = new DeflaterOutputStream(byteArrayOutputStream, defeater);     try {         deflaterOutputStream.write(src);         deflaterOutputStream.finish();         deflaterOutputStream.close();         result = byteArrayOutputStream.toByteArray();     } catch (IOException e) {         defeater.end();         throw e;     } finally {         try {             byteArrayOutputStream.close();         } catch (IOException ignored) {         }         defeater.end();     }     return result; }</code></pre></li><li><p>如果注册了消息发送钩子函数 则执行消息发送之前的增强逻辑 可通过<code>DefaultMQProducer::registerSendMessageHook</code> 进行注册</p></li><li><p>构建消息发送请求包 该包包括以下重要信息: 生产者组 主题名称 默认创建主题key 该主题在单个Broker默认的队列数 队列id 消息系统标记 消息发送时间 消息标记 消息扩展属性 消息充实次数 是否是批量消息等</p></li><li><p>根据消息发送方式 同步 异步或者单向进行网络传输</p></li><li><p>如果注册了消息发送钩子函数 执行after逻辑</p></li></ol><ul><li>批量消息发送</li></ul><p>略</p><h2 id="ROCKETMQ-消息存储"><a href="#ROCKETMQ-消息存储" class="headerlink" title="ROCKETMQ 消息存储"></a>ROCKETMQ 消息存储</h2><p>RocketMQ主要存储的文件包括CommitLog ConsumeQueue indexFile文件 RocketMq将所有主题的消息存储在同一个文件中 确保消息发送时顺序写文件 尽最大能力确保消息发送的高性能与高吞吐量</p><p><img src=".//img/rocketmq_msg_storage_design.jpg" srcset="/img/loading.gif" alt="rsd"></p><p>存储流程比较复杂 在<code>CommitLog::putMessage</code>,<code>MappedFile::appendMessagesInner</code> 以及<code>CommitLog$DefaultAppendMessageCallback::doAppend</code>方法中我们可以了解其最基本流程</p><ol><li><p>如果当前Broker停止工作或为SLAVE角色或当前ROCKETMQ不支持写入或消息主题超过256个字符 或消息属性超过65536个字符 则拒绝写入</p></li><li><p>如果消息的延迟级别大于0 将消息的原主题名称与原消息队列id存入消息属性中 用延迟消息主题<code>SCHEDULE_TOPIC</code>,消息队列ID更新原先消息的主题与队列 这是并发消息消费重试关键的一步(?这块还不太懂)</p></li><li><p>commitlog默认存储目录<code>${home}/store/commitlog</code> 每一个文件默认1G 一个文件写满后 再创建另一个 以该文件中第一个偏移量为文件名 可以将<code>MappedFileQueue</code>看作<code>${Rocket_mq}/store/commitlog</code>文件夹 <code>MappedFile</code> 则对应该文件夹下的一个个文件</p></li><li><p>在写入<code>CommitLog</code>之前 先申请putMessageLock 即对commitlog文件的写是<strong>串行的</strong>(这里捎带提一句无关的 它还有个spin lock的实现比较有意思)</p><pre><code class="java"> /**    * Spin lock Implementation to put message, suggest using this with low race conditions */ public class PutMessageSpinLock implements PutMessageLock { //true: Can lock, false : in lock. private AtomicBoolean putMessageSpinLock = new AtomicBoolean(true);     @Override     public void lock() {         boolean flag;         do {             flag = this.putMessageSpinLock.compareAndSet(true, false);         }         while (!flag);     }     @Override     public void unlock() {         this.putMessageSpinLock.compareAndSet(false, true);         } }</code></pre></li><li><p>设置消息的存储时间 如果mappedFile为空 表示尚未创建任何文件 则用偏移量0创建第一个commit文件 如果创建失败则<code>CREATE_MAPEDFILE_FAILED</code> 这很有可能是权限或磁盘空间不足导致的</p></li><li><p>将消息追加的MappedFile中 首先获取MappedFile当前写指针 MappedFile的<code>protected final AtomicInteger wrotePosition = new AtomicInteger(0);</code>字段记录了具体写到的位置 如果这个指针的值已经大于等文件大小 则说明文件已写满 抛出<code>AppendMessageStatus.UNKNOWN_ERROR</code> 否则通过<code>ByteBuf::slive()</code>方法创建一个与MappedFile的共享内存区 并设置position为当前写指针 这个和过程我们在<code>MappedFile::appendMessagesInner</code>方法中可以看到</p></li><li><p>创建全局唯一消息id 消息id有16字节 为保证其可读性 返回给应用程序的msgid为字符类型 可以通过<code>UtilAll::bytes2string</code> <code>UtilAll::string2Bytes</code>进行转化 消息id组成: <strong>|四字节ip|四字节端口|8字节消息偏移量</strong>(不是很清楚为什么端口号会占4字节 可能是为了对齐?) 创建id的实现方法在<code>MessageDecoder::createMessageId</code>中 由<code>CommitLog$DefaultAppendMessageCallback::doAppend</code>调用</p></li><li><p>获取该消息在消息队列的偏移量 commitlog保存了当前所有消息队列的当前待写入偏移量</p></li><li><p>根据消息体的长度 主题的长度 属性的长度结合消息存储格式计算消息总长度</p></li><li><p><code>if ((msgLen + END_FILE_MIN_BLANK_LENGTH) &gt; maxBlank)</code>将返回带有<code>AppendMessageStatus.END_OF_FILE</code>状态码的<code>AppendMessageResult</code> 随后Broker将新建一个CommitLog来存储该信息</p><pre><code class="java">if ((msgLen + END_FILE_MIN_BLANK_LENGTH) &gt; maxBlank) {            this.resetByteBuffer(this.msgStoreItemMemory, maxBlank);            // 1 TOTALSIZE            this.msgStoreItemMemory.putInt(maxBlank);            // 2 MAGICCODE            this.msgStoreItemMemory.putInt(CommitLog.BLANK_MAGIC_CODE);            // 3 The remaining space may be any value            // Here the length of the specially set maxBlank            final long beginTimeMills = CommitLog.this.defaultMessageStore.now();            byteBuffer.put(this.msgStoreItemMemory.array(), 0, maxBlank);            return new AppendMessageResult(AppendMessageStatus.END_OF_FILE, wroteOffset, maxBlank, msgId, msgInner.getStoreTimestamp(),                queueOffset, CommitLog.this.defaultMessageStore.now() - beginTimeMills);        }</code></pre><p>从这一部分的逻辑我们可以看出每个commitlog开头最少会空闲8字节 高4字节存储当前文件剩余空间 低4字节存储commitlog文件魔数</p></li><li><p>将消息内容存储到bytebuffer中 然后创建<code>AppendMessageResult</code> 注意<strong>这里只是将消息存储在MappedFile对应的内存映射Buffer中 并没有刷写到磁盘</strong></p></li><li><p>更新Topic对应的消息队列逻辑偏移量</p></li><li><p>处理完消息追加逻辑后释放<code>putMessageLock</code>锁</p></li><li><p>根据刷盘策略进行刷盘</p></li></ol><h3 id="存储文件组织与内存映射"><a href="#存储文件组织与内存映射" class="headerlink" title="存储文件组织与内存映射"></a>存储文件组织与内存映射</h3><p>RocketMQ通过使用<strong>内存映射文件</strong>(mmap)来提高IO性能 无论是CommitLog  ConsumeQueue还是indexFile 单个文件都被设计为固定长度 如果一个文件写满以后再创建一个新文件 文件名就为该文件第一条消息对应的全局物理偏移量</p><p>RocketMQ使用<code>MappedFile</code> <code>MappedFileQueue</code>来封装存储文件 后者是前者的管理容器 是对存储目录的封装</p><p><code>MappedFileQueue</code>主要属性:</p><pre><code class="java">    private final String storePath;    private final int mappedFileSize;    private final CopyOnWriteArrayList&lt;MappedFile&gt; mappedFiles = new CopyOnWriteArrayList&lt;MappedFile&gt;();    private final AllocateMappedFileService allocateMappedFileService;    private long flushedWhere = 0;    private long committedWhere = 0;    private volatile long storeTimestamp = 0;</code></pre><p><code>flushWhere</code>属性:当前刷盘指针 表示该指针以前的所有数据全部持久化到磁盘</p><p><code>committedWhere</code>属性:当前数据提交指针  内存中的ByteBuffer当前的写指针 该值大于等于flushedWhere</p><p>根据消息偏移量 计算mappedFile在<code>MappedFileQueue.mappedFiles</code>中的索引位置的计算公式:<code>int index = (int) ((offset / this.mappedFileSize) - (firstMappedFile.getFileFromOffset() / this.mappedFileSize));</code> <code>offset / this.mappedFileSize</code>计算出按照offset在没有删除的情况本该是第几个  <code>(firstMappedFile.getFileFromOffset() / this.mappedFileSize)</code>计算实际上的列表中第一个mappedFile在没有删除的情况下是第几个</p><p>之所以这样计算是因为 为了提高效率 RocketMQ采用了内存映射 只要存在于存储目录下的文件都需要创建对内存映射文件 如果不定时将已消费的消息 从存储文件中删除 会对造成极大的内存压力与资源浪费 所以RocketMQ采取定时删除存储文件的策略 也就是说在存储文件中 第一个文件不一定以0偏移 开头 <strong>因为该文件可能在某一时刻被删除</strong></p><h4 id="MappedFile"><a href="#MappedFile" class="headerlink" title="MappedFile"></a>MappedFile</h4><ul><li><p>MappedFile 初始化 init</p><p>  根据<code>transientStorePoolEnable</code>存在两种初始化情况 <code>transientStorePoolEnable</code>为true表示内容先存储在堆外内存 然后通过Commit线程将数据提交到内存映射MappedByteBuffer中 再通过flush线程将内存映射Buffer中的数据持久化到磁盘</p><p>  <code>MappedFile</code> 在init方法中 创建内存文件映射的代码</p><pre><code class="java">  ...      //确保文件目录已经被创建      ensureDirOK(this.file.getParent());      try {          this.fileChannel = new RandomAccessFile(this.file, &quot;rw&quot;).getChannel();          this.mappedByteBuffer = this.fileChannel.map(MapMode.READ_WRITE, 0, fileSize);          TOTAL_MAPPED_VIRTUAL_MEMORY.addAndGet(fileSize);          TOTAL_MAPPED_FILES.incrementAndGet();          ok = true;      }</code></pre></li><li><p>MappedFile 提交 Commit</p><p>  内存映射文件的提交动作由<code>MappedFile::commit</code>实现</p><pre><code class="java">  public int commit(final int commitLeastPages) {      if (writeBuffer == null) {          //no need to commit data to file channel, so just regard wrotePosition as committedPosition.          return this.wrotePosition.get();      }      if (this.isAbleToCommit(commitLeastPages)) {          if (this.hold()) {              commit0(commitLeastPages);              this.release();          } else {              log.warn(&quot;in commit, hold failed, commit offset = &quot; + this.committedPosition.get());          }      }      // All dirty data has been committed to FileChannel.      if (writeBuffer != null &amp;&amp; this.transientStorePool != null &amp;&amp; this.fileSize == this.committedPosition.get()) {          this.transientStorePool.returnBuffer(writeBuffer);          this.writeBuffer = null;      }      return this.committedPosition.get();  }</code></pre><p>  <code>commitLeastPages</code>为本次最少要提交的页数 如果要提交的数据不够 则待下次提交 如果<code>writeBuffer</code>为空 则直接返回<code>wrotePosition</code>指针(表明了提交的主体是<code>writeBuffer</code>)</p><p>  具体的提交实现</p><pre><code class="java">  protected void commit0(final int commitLeastPages) {      int writePos = this.wrotePosition.get();      int lastCommittedPosition = this.committedPosition.get();      if (writePos - this.committedPosition.get() &gt; 0) {          try {              //创建ByteBuffer共享缓冲区              ByteBuffer byteBuffer = writeBuffer.slice();              //将新创建的byteBuffer回退到上一次提交的位置              byteBuffer.position(lastCommittedPosition);              //设置limit为wrotePosition(即当前最大有效数据指针)              byteBuffer.limit(writePos);              this.fileChannel.position(lastCommittedPosition);              //把committedPostion到wrotePosition的数据复制到FileChannel中              this.fileChannel.write(byteBuffer);              //更新commitedPosition指针为wrotePosition              this.committedPosition.set(writePos);          } catch (Throwable e) {              log.error(&quot;Error occurred when commit data to FileChannel.&quot;, e);          }      }  }</code></pre><p>  首先创建ByteBuffer共享缓冲区 然后将新创建的byteBuffer回退到上一次提交的位置 设置limit为wrotePosition(即当前最大有效数据指针) 然后把committedPostion到wrotePosition的数据复制到FileChannel中 然后更新commitedPosition指针为wrotePosition <strong>commit的作用就是将<code>MappedFile.writeBuffer</code>中的数据提交到文件通道FileChannel中</strong></p><p>  ByteBuffer的使用技巧: slice()方法创建一个共享缓冲区 与原先的ByteBuffer共享内存但维护一套独立的指针(position mark limit)</p></li><li><p>MappedFile 刷盘 (flush)</p></li></ul><p>其基本逻辑实现在<code>MappedFile::flush</code>方法实现 其基本逻辑同``MappedFile::commit`相似</p><pre><code class="java">    /**     * @return The current flushed position     */    public int flush(final int flushLeastPages) {        if (this.isAbleToFlush(flushLeastPages)) {            if (this.hold()) {                int value = getReadPosition();                try {                    //We only append data to fileChannel or mappedByteBuffer, never both.                    if (writeBuffer != null || this.fileChannel.position() != 0) {                        this.fileChannel.force(false);                    } else {                        this.mappedByteBuffer.force();                    }                } catch (Throwable e) {                    log.error(&quot;Error occurred when force data to disk.&quot;, e);                }                this.flushedPosition.set(value);                this.release();            } else {                log.warn(&quot;in flush, hold failed, flush offset = &quot; + this.flushedPosition.get());                this.flushedPosition.set(getReadPosition());            }        }        return this.getFlushedPosition();    }</code></pre><p>通过调用<code>FileChannel</code>或者<code>mappedByteBuffer</code>的<code>force</code>方法 执行刷盘操作</p><p>获取最大的读指针:</p><pre><code class="java"> /**     * @return The max position which have valid data     */    public int getReadPosition() {        return this.writeBuffer == null ? this.wrotePosition.get() : this.committedPosition.get();    }</code></pre><p>如果当前的writeBuffer为空则直接返回当前写指针 否则返回上一次提交的指针 在MF的设计中 只有提交了的(<strong>写入到MappedBuffer或者FileChannel的数据</strong>)才是安全的数据</p><p>读取某条消息</p><pre><code class="java">    public SelectMappedBufferResult getMessage(final long offset, final int size) {        int mappedFileSize = this.defaultMessageStore.getMessageStoreConfig().getMappedFileSizeCommitLog();        MappedFile mappedFile = this.mappedFileQueue.findMappedFileByOffset(offset, offset == 0);        if (mappedFile != null) {            int pos = (int) (offset % mappedFileSize);            return mappedFile.selectMappedBuffer(pos, size);        }        return null;    }    public SelectMappedBufferResult selectMappedBuffer(int pos, int size) {        int readPosition = getReadPosition();        if ((pos + size) &lt;= readPosition) {            if (this.hold()) {                ByteBuffer byteBuffer = this.mappedByteBuffer.slice();                byteBuffer.position(pos);                // 整个共享缓冲区的容量为fileSize - position 因为slice方法所创建的共享缓冲区是从其position至limit                ByteBuffer byteBufferNew = byteBuffer.slice();                byteBufferNew.limit(size);                return new SelectMappedBufferResult(this.fileFromOffset + pos, byteBufferNew, size, this);            } else {                log.warn(&quot;matched, but hold failed, request pos: &quot; + pos + &quot;, fileFromOffset: &quot;                    + this.fileFromOffset);            }        } else {            log.warn(&quot;selectMappedBuffer request pos invalid, request pos: &quot; + pos + &quot;, size: &quot; + size                + &quot;, fileFromOffset: &quot; + this.fileFromOffset);        }        return null;    }</code></pre><p><code>selectMappedBuffer</code>在对pos数值进行检查后 由于在整个写入期间都没有改变<code>MappedByteBuffer</code>的指针 所以用<code>MappedByteBuffer::slice</code>返回的共享缓存区空间为整个MappedFile 然后通过设置bytebuffer的position为当前待查找的值 读取字节为当前可读字节长度 最终返回的bytebuffer的limit为<code>size</code>的值</p><ul><li>MappedFile 销毁 destroy</li></ul><p>基本逻辑在<code>MappedFile::destroy(final long intervalForcibly)</code>方法中 其参数为拒绝被销毁的最大时间</p><p>销毁过程分3步</p><ol><li><p>关闭MappedFile 设置 初次调用时其父类<code>ReferenceResource</code>的<code>available</code>字段为false 并设置初次关闭的时间戳<code>firstShutdownTimestamp</code> 然后掉哟<code>release</code>方法尝试释放资源 只有在<code>ReferenceResource</code>所记录的引用次数字段<code>refCount</code>小于1时 才会释放 对比当前时间与<code>firstShutdownTimestamp</code> 如果已经超过了其最大拒绝存活期 每执行一次就将其引用数减少1000 知道引用数小于0 通过release方法释放资源</p><pre><code class="java">     public void shutdown(final long intervalForcibly) {     if (this.available) {         this.available = false;         this.firstShutdownTimestamp = System.currentTimeMillis();         this.release();     } else if (this.getRefCount() &gt; 0) {         if ((System.currentTimeMillis() - this.firstShutdownTimestamp) &gt;= intervalForcibly) {             this.refCount.set(-1000 - this.getRefCount());             this.release();         }     } }</code></pre></li><li><p>判断清理是否完成 判断标准是 引用次数小于等于0 且cleanupover为true 会调用<code>ByteBuffer</code>的<code>clear</code>方法对<code>MappedByteBuffer</code>进行清除</p></li><li><p>关闭文件通道删除物理文件 调用 <code>File</code>的<code>delete</code>方法删除文件</p></li></ol><ul><li>TransientStorePool 短暂的存储池</li></ul><p>RocketMQ 单独创建一个<code>DirectByteBuffer</code>内存缓冲池 用来临时存储数据 数据先写入该内存映射中 然后由commit线程定时将数据从该内存复制到与目的物理文件对应的内存映射中 引入该机制的主要原因是<strong>提供一种内存锁定 将当前堆外内存一致锁定在内存中 避免被进程将内存交换到磁盘</strong></p><pre><code class="java">    public void init() {        for (int i = 0; i &lt; poolSize; i++) {            ByteBuffer byteBuffer = ByteBuffer.allocateDirect(fileSize);            final long address = ((DirectBuffer) byteBuffer).address();            Pointer pointer = new Pointer(address);            //使用com.sun.jna.Library库对该批内存进行锁定 避免被置换到交换区            LibC.INSTANCE.mlock(pointer, new NativeLong(fileSize));            availableBuffers.offer(byteBuffer);        }    }</code></pre><p>在消息写入时，如果writerBuffer不为空，说明开启了transientStorePoolEnable机制，则消息首先写入writerBuffer中，如果其为空，则写入mappedByteBuffer中。消息读取时，是从mappedByteBuffer中读(pageCache)。</p><p>这样就有了读写分离的效果，先写入writerBuffer中，读却是从mappedByteBuffer中读取。</p><p>为了对transientStorePoolEnable引入意图阐述的更加明白，这里我引入Rocketmq社区贡献者胡宗棠关于此问题的见解。</p><ul><li><p>通常有如下两种方式进行读写：</p><p>  第一种，Mmap+PageCache的方式，读写消息都走的是pageCache，这样子读写都在pagecache里面不可避免会有锁的问题，在并发的读写操作情况下，会出现缺页中断降低，内存加锁，污染页的回写。</p><p>  第二种，<code>DirectByteBuffer(堆外内存)+PageCache的</code>两层架构方式，这样子可以实现读写消息分离，<strong>写入消息时候写到的是DirectByteBuffer——堆外内存中,读消息走的是PageCache(对于,DirectByteBuffer是两步刷盘，一步是刷到PageCache，还有一步是刷到磁盘文件中)，带来的好处就是，避免了内存操作的很多容易堵的地方，降低了时延，比如说缺页中断降低，内存加锁，污染页的回写</strong>。</p></li></ul><p>简单的说开启transientStorePool相当于</p><p>启用“读写”分离，消息发送时消息先追加到DirectByteBuffer(堆外内存)中，然后在异步刷盘机制下，会将DirectByteBuffer中的内容提交到PageCache，然后刷写到磁盘。消息拉取时，直接从PageCache中拉取，实现了读写分离，减轻了PageCache的压力，能从根本上解决该问题。–<a href="https://yq.aliyun.com/articles/716568" target="_blank" rel="noopener">来自阿里云栖社区</a></p><ul><li>关于 <code>MappedByteBuffer</code>与<code>DirectByteBuffer</code> 引自<a href="https://stackoverflow.com/questions/1229037/difference-between-bytebuffer-allocatedirect-and-mappedbytebuffer-load" target="_blank" rel="noopener">StackOverFlow</a></li></ul><blockquote><p>Direct ByteBuffers (those allocated using ByteBuffer.allocateDirect) are different to MappedByteBuffers in that they represent different sections of memory and are allocated differently. Direct ByteBuffers are a way to access a block of memory allocated outside of the JVM generally allocated with a malloc call (although most implementations will probably use an efficient slab allocator). I.e. it’s just a pointer to a block of memory.</p></blockquote><blockquote><p>A MappedByteBuffer represents a section of memory allocated using mmap call, which is used to perform memory mapped I/O. Therefore MappedByteBuffers won’t register their use of memory in the same way a Direct ByteBuffer will.</p></blockquote><blockquote><p>So while both are “direct” in that they represent memory outside of the JVM their purposes are different.</p></blockquote><ul><li>关于内存锁定 以防止由于内存换页带来的性能暴跌</li></ul><p>可以参见<a href="https://www3.physnet.uni-hamburg.de/physnet/Tru64-Unix/HTML/APS33DTE/DOCU_005.HTM" target="_blank" rel="noopener">这篇文章</a></p><p>本人已将其搬运至本地<a href="../os/memory_locking.md">这里</a></p><p>除了开启<code>transientStoreEnable</code>选项之外 在创建<code>MappedFile</code>时 如果开启了<code>warmMapedFileEnable</code>选项 那么并为其分配内存时会进行所谓的内存预热</p><p>以下关于内存预热的讨论来自<a href="http://tinylcy.me/2019/the-design-of-rocketmq-message-storage-system/" target="_blank" rel="noopener">这位大佬的blog</a></p><p>RocketMQ 利用 mmap 将内核空间的一段内存区域映射至用户空间，映射关系一旦建立，应用程序对这段内存区域的修改可以直接反映到内核空间，反之亦然。相比如 read/write 系统调用，mmap 减少了内核空间和用户空间之间的数据拷贝，在存在大量数据传输的场景下可以有效提升 IO 效率。但是，<strong>通过 mmap 建立内存映射仅是将文件磁盘地址和虚拟地址通过映射对应起来，此时物理内存并没有填充磁盘文件内容。当实际发生文件读写时，产生缺页中断并陷入内核，然后才会将磁盘文件内容读取至物理内存</strong>。(关于这一点 本人特地去查了一下 按照维基百科的说法 In computing, mmap(2) is a POSIX-compliant Unix system call that maps files or devices into memory. It is a method of memory-mapped file I/O. It implements <strong>demand paging</strong>, because file contents are not read from disk directly and initially do not use physical RAM at all. <strong>The actual reads from disk are performed in a “lazy” manner, after a specific location is accessed</strong>. 简单的翻译一下:确实如此 而关于它所提的<strong>demand pafing</strong>:demand paging (as opposed to anticipatory paging) is a method of virtual memory management. In a system that uses demand paging, the operating system copies a disk page into physical memory only if an attempt is made to access it and that page is not already in memory)针对上述场景，RocketMQ 设计了 MappedFile 预热机制。</p><p>回顾 MappedFile 的创建流程，AllocateMappedFileService 线程轮询 AllocateRequest 请求队列并创建MappedFile，此时文件系统中已经存在对应的固定大小的文件。当 RocketMQ 开启 MappedFile 内存预热（warmMapedFileEnable），且 MappedFile 文件映射空间大小<strong>大于等于</strong> mapedFileSizeCommitLog（1 GB） 时(<strong>这里我有个小小的问题 为啥MappedFile 的映射空间大小 会 出现 大于<code>mapedFileSizeCommitLog</code>的情况 因为似乎二者的设置都是根据<code>MessageStoreConfig.mappedFileSizeCommitLog</code>属性设置的</strong> 对不起 我是傻逼)，调用 warmMappedFile 方法对 MappedFile 进行预热。上述逻辑核心代码精简如下。</p><pre><code class="java">private boolean mmapOperation() {    boolean isSuccess = false;    AllocateRequest req = null;    try {        req = this.requestQueue.take();        ...        if (req.getMappedFile() == null) {            MappedFile mappedFile;            if (messageStore.getMessageStoreConfig().isTransientStorePoolEnable()) {                mappedFile = ServiceLoader.load(MappedFile.class).iterator().next();                mappedFile.init(req.getFilePath(), req.getFileSize(),                                 messageStore.getTransientStorePool());            } else {                mappedFile = new MappedFile(req.getFilePath(), req.getFileSize());            }            ...            // pre write mappedFile            if (mappedFile.getFileSize() &gt;= getMapedFileSizeCommitLog()                                        &amp;&amp; isWarmMapedFileEnable()) {                mappedFile.warmMappedFile(getFlushDiskType(),                                          getFlushLeastPagesWhenWarmMapedFile());            }            ...        }    } catch (InterruptedException e) {        ...    } catch (IOException e) {        ...    } finally {        ...    }    return true;}</code></pre><p>warmMappedFile 每间隔 <code>OS_PAGE_SIZE</code> 向 mappedByteBuffer 写入一个 0，此时对应页恰好产生一个缺页中断，操作系统为对应页分配物理内存。同时，如果刷盘策略为同步刷盘，需要对每页进行刷盘。最后，通过 JNA 调用 mlock 方法锁定 mappedByteBuffer 对应的物理内存，阻止操作系统将相关的内存页调度到交换空间（swap space），以此提升后续在访问 MappedFile 时的读写性能。warmMappedFile 核心代码精简如下。</p><pre><code class="java">public void warmMappedFile(FlushDiskType type, int pages) {    ByteBuffer byteBuffer = this.mappedByteBuffer.slice();    int flush = 0;    for (int i = 0, j = 0; i &lt; this.fileSize; i += MappedFile.OS_PAGE_SIZE, j++) {        byteBuffer.put(i, (byte) 0);        if (type == FlushDiskType.SYNC_FLUSH) {            if ((i / OS_PAGE_SIZE) - (flush / OS_PAGE_SIZE) &gt;= pages) {                flush = i;                mappedByteBuffer.force();            }        }        // prevent gc        if (j % 1000 == 0) {            ...        }    }    // force flush when prepare load finished    if (type == FlushDiskType.SYNC_FLUSH) {        mappedByteBuffer.force();    }    this.mlock();}</code></pre><h2 id="CommitLog与ConsumeQueue与Index索引文件与CheckPoint文件"><a href="#CommitLog与ConsumeQueue与Index索引文件与CheckPoint文件" class="headerlink" title="CommitLog与ConsumeQueue与Index索引文件与CheckPoint文件"></a>CommitLog与ConsumeQueue与Index索引文件与CheckPoint文件</h2><p><img src="/img/2019-07-21-RocketMQ&#32;CommitLog.jpg" srcset="/img/loading.gif" alt="cmlg"></p><p><img src="/img/2019-07-21-RocketMQ&#32;CommitLog&#32;Item.jpg" srcset="/img/loading.gif" alt=""></p><p><img src="/img/commit_log_logic_view.webp" srcset="/img/loading.gif" alt="lv"></p><p>可以看到commit log中 <strong>同一主题的消息是不连续的存储在文件中的</strong> 为了避免从commitlog中遍历查找订阅主题下的消息  rocketmq设计了消息消费队列文件(ConsumeQueue) 该文件可以看作CommitLog关于消息消费的索引文件 其构建机制为:当消息到达commitlog后 由专门的线程产生消息转发任务 从而构建消息消费队列文件以及索引文件</p><p>consumequeue的第一级 目录为消息主题 第二级目录为主题的消息队列</p><p>另一方面 为了加速消息条目的检索速度与节省磁盘空间 每一个ConsumeQueue条目不会存储消息的全量信息 其存储格式如下图所示</p><p><img src="/img/consume_queue_entry.webp" srcset="/img/loading.gif" alt="cqe"></p><p>单个ConsumeQueue文件默认包含30w个条目</p><p>但是有个小问题–如果consumequeue的创建数量是根据broker设置的每个主题的队列数 且consumequeue的写入 是顺序写入 也就是写完一个consumequeue的文件 再写下一个 而consumer消费时 每个consumequeue只能被同时被一个consumer消费 而这一主题下的其他consumequue文件此时还没有任何数据 那分配到这些consumequeue的consumer这段时间岂不是只能闲着了?</p><p>后续–</p><p>自己试了一下 应该不是顺序写入 主题的consumequeue下的每个文件锁存储的应该都是该主题下的消息的不同部分 但是具体是采用什么算法的目前还不知道</p><p>刚才又研究了一下 具体写入哪个ConsumeQueue似乎是由<code>DispatchRequest</code>中的<code>queueId</code>参数决定的 而这个参数其是由<code>CommitLog::checkMessageAndReturnSize</code>方法设置 而这个方法中设置该字段的值 就是通过读取ByteBuffer中的对应字段 照这么个说法的话 往哪个cconsumequeue中写入是由producer决定的？ 应该是通过设置<code>MessageQueueSelector</code>实现 其可以设置<code>SelectMessageQueueByHash</code>或者<code>SelectMessageQueueByRandom</code>来实现 默认按照<code>MQFaultStrategy</code>的算法来选择一个消息队列进行发送 其通过<code>TopicInfo</code>维护的一个自增字段对总的ConsumeQueue数取余数得到</p><ul><li>Index索引文件</li></ul><p>ConsumeQueue是专门为消息订阅构建的索引文件 提高根据主题与消息队列检索消息的速度 而index索引文件则是通过hash索引机制为消息建立索引</p><p><img src="/img/rmq_index_file.png" srcset="/img/loading.gif" alt="rmqidf"><br>虽然这两张图没啥区别把。。。。<br><img src="/img/2019-07-21-RocketMQ&#32;Index.jpg" srcset="/img/loading.gif" alt="cmlg"><br>更多内容不再展开 懒得看</p><h3 id="Checkpoint文件"><a href="#Checkpoint文件" class="headerlink" title="Checkpoint文件"></a>Checkpoint文件</h3><p>该文件的作用是记录前三类文件的刷盘时间点 文件固定长度为4K 其中只是用该文件前面的24字节</p><p>存储格式:</p><p>| 8字节commitlog刷盘时间点| 消费队列文件刷盘时间点 | 索引文件刷盘时间点 |</p><p>该文件会在broker关闭时/以及以上三类文件进行刷盘操作时 进行刷盘持久化</p><h3 id="实时更新消息消费队列和索引文件"><a href="#实时更新消息消费队列和索引文件" class="headerlink" title="实时更新消息消费队列和索引文件"></a>实时更新消息消费队列和索引文件</h3><p>RocketMQ通过开启一个线程<code>ReputMessageService</code>来<strong>异步的</strong>转发<code>CommitLog</code>文件更新事件 相应的任务处理器根据转发消息及时更新<code>ConsumeQueue</code>，<code>IndexFile</code>文件 Broker服务器在启动<code>ReputMessageService</code>时 初始化其<code>reputFromOffset</code>参数</p><h3 id="消息队列与索引文件恢复的一致性问题"><a href="#消息队列与索引文件恢复的一致性问题" class="headerlink" title="消息队列与索引文件恢复的一致性问题"></a>消息队列与索引文件恢复的一致性问题</h3><p>就像上面写的对于消费队列和索引文件的更新相较于对CommitLog的更新而言是异步 这就涉及到这样一个问题: 如果消息成功存储到CommitLog文件中 转发任务未成功执行 此时 消息服务器Broker由于某个原因宕机 那么将导致CommitLog ConsumeQueue IndexFile文件不一致</p><p>下面说明RocketMQ中如何处理这种问题</p><p>参看<code>DefaultMessageStore::load</code>方法</p><pre><code class="java">    public boolean load() {        boolean result = true;        try {            // 判断上一次退出是否正常 isTempFileExist检查是否存在abort文件 如果存在 说明上一次异常退出            boolean lastExitOK = !this.isTempFileExist();            log.info(&quot;last shutdown {}&quot;, lastExitOK ? &quot;normally&quot; : &quot;abnormally&quot;);            if (null != scheduleMessageService) {                // 加载延迟队列 与定时消息有关                result = result &amp;&amp; this.scheduleMessageService.load();            }            // load Commit Log            result = result &amp;&amp; this.commitLog.load();            // load Consume Queue            result = result &amp;&amp; this.loadConsumeQueue();            if (result) {                // 加载存储检测点 检测点主要记录commitlog文件 consumequeue文件 index索引文件的刷盘点                this.storeCheckpoint =                    new StoreCheckpoint(StorePathConfigHelper.getStoreCheckpoint(this.messageStoreConfig.getStorePathRootDir()));                // 加载索引文件 如果上次异常退出 且索引文件的刷盘时间小于该索引文件最大的消息时间戳 则将该文件立刻销毁                this.indexService.load(lastExitOK);                // 根据是否正常退出 执行不同的恢复策略                this.recover(lastExitOK);                log.info(&quot;load over, and the max phy offset = {}&quot;, this.getMaxPhyOffset());            }        } catch (Exception e) {            log.error(&quot;load exception&quot;, e);            result = false;        }        if (!result) {            this.allocateMappedFileService.shutdown();        }        return result;    }</code></pre><p><code>DefaultMessageStore::recover</code>的两种策略:</p><ul><li><p><code>DefaultMessageStore::recoverNormally</code></p></li><li><p><code>DefaultMessageStore::recoverAbnormally</code></p><p>  从上次异常停止中 恢复时 从最后一个文件往前走 找到第一个消息存储正常的文件 另外 如果commitlog目录中没有消息文件 而在消息消费队列目录下存在文件 则需要销毁 大致流程如下:</p><ol><li>通过<code>isMappedFileMatchedRecover(mappedFile)</code>方法 判断mappedfile是否满足要求 首先判断文件的的魔数 若魔数符合 则判断文件第一条消息的存储时间是否为0 为0则说明该存储文件中没有存储任何消息 返回false</li><li>对比文件第一条消息的时间戳与检测点 <strong>文件第一条消息的时间戳小于文件检测点 说明该文件部分消息是可靠的 则从该文件恢复</strong></li><li>遍历已找到的MappedFile中的消息 检查消息的合法性 并将之重新转发到消息消费队列与索引文件(这也意味着潜在的消息重复问题)</li><li>如果未找到MappedFile 则设置Commitlog的flushedWhere committedWhere指针都为0 并销毁消息消费文件</li></ol></li></ul><h3 id="刷盘策略"><a href="#刷盘策略" class="headerlink" title="刷盘策略"></a>刷盘策略</h3><p>RocketMQ的读写是基于JAVA NIO的内存映射机制<code>MappedByteBuffer</code>的 消息存储时首先将消息追加到内存 再根据配置的刷盘策略在不同时间进行刷写磁盘 如果是同步刷盘 则在消息追加到内存后 同步调用<code>MappedByteBuffer::force</code>方法 如果是异步刷盘则消息追加到内存后立刻返回给消息发送端 RocketMQ使用一个单独的线程按照某一个设定的频率执行刷盘操作(索引文件的刷盘除外)</p><pre><code class="java">    public void handleDiskFlush(AppendMessageResult result, PutMessageResult putMessageResult, MessageExt messageExt) {        // Synchronization flush        // ...someCode        }        // Asynchronous flush        else {            if (!this.defaultMessageStore.getMessageStoreConfig().isTransientStorePoolEnable()) {                flushCommitLogService.wakeup();            } else {                commitLogService.wakeup();            }        }    }</code></pre><p>可以看到同步刷盘下<code>CommitLog::handleDiskFlush</code>方法内部同步调用相关方法 而异步刷盘则仅仅根据是否启用<code>TransientStorePool</code>来唤醒对应的服务线程</p><ul><li><p>同步刷盘</p></li><li><p>异步刷盘</p><p>  在<code>transinetStorePoolEnable</code>为true的情况下</p><ol><li>首先将消息直接追加到堆外的DirectByteBuffer</li><li><code>CommitRealTimeService</code>线程默认每200ms将ByteBuffer新追加的内容的数据提交到<code>MappedByteBuffer</code></li><li><code>MappedByteBuffer</code>在内存中追加提交的内容 <code>wrotePosition</code>向后移动 然后返回</li><li>commit操作成功后 将commitedPosition指针向后移动本次提交的内容长度</li><li><code>FlushRealTimeService</code>线程默认每<strong>500ms</strong>将MappedBytebuffer中新追加的内存通过调用<code>MappedByteBuffer::force</code>方法将数据刷到磁盘</li></ol></li></ul><h3 id="过期文件删除"><a href="#过期文件删除" class="headerlink" title="过期文件删除"></a>过期文件删除</h3><p>RocketMQ不会永久存储消息文件在服务器上 并引入了一种机制来删除已过期的文件 RocketMQ<strong>顺序</strong>写<code>CommitLog</code>,<code>ConsumeQueue</code>文件 最后所有的写操作全部落在最后一个<code>Commitlog</code>或者<code>ConsumeQueue</code>文件上 之前的文件在下一个文件创建后将不会再被更新 RocketMQ清除过期文件的方法是:如果非当前写文件在一定时间间隔内没有再次被更新 则被认为是过期文件 RocketMQ也不会再关注该文件上的消息是否全部被消费 默认过期时间是72h 可通过在broker配置文件中设置<code>fileReservedTime</code>来改变过期时间</p><p>rocketmq 会在如下情况对消息文件进行删除</p><ol><li>指定删除文件的时间点 在该时间点对过期消息文件进行删除</li><li>磁盘空间是否充足 否则触发过期文件删除操作</li><li>命令删除</li></ol><p>其通过<code>File::getFreeSpace</code>和<code>File::getTotalSpace</code>计算当前磁盘占用量 一旦占用超过指定值 默认0.85 将会触发立即删除过期文件 如果占用过高默认0.90将拒绝新消息写入</p><h2 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h2><ul><li><a href="http://objcoding.com/2019/07/27/rocketmq-consumer-subscription/" target="_blank" rel="noopener">rocketmq 为什么要保证订阅关系一致性</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>源码</category>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MessageQueue</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Redis命令执行周期</title>
    <link href="/2019/08/10/Redis%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/index/"/>
    <url>/2019/08/10/Redis%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/index/</url>
    
    <content type="html"><![CDATA[<h1 id="Redis命令执行周期"><a href="#Redis命令执行周期" class="headerlink" title="Redis命令执行周期"></a>Redis命令执行周期</h1><p>Redis是典型的事件驱动程序 事件处理显得更为重要 Redis将事件分为两大类:</p><ol><li><em>文件事件</em> socket的读写事件</li><li><em>时间事件</em> 用于处理一些需要周期性执行的定时任务</li></ol><p><img src="/img/redis_objtype_datastrcuture.png" srcset="/img/loading.gif" alt=""></p><p><strong>注意双向链表已不再使用</strong> 且zset有序列表有时还会用到hash类型 将hash和skiplist结合使用 这样可以将查找特定元素的操作的时间开销缩小到O(1) 而<code>zrange</code> <code>zrank</code>等命令的复杂度仍为O(logN) 也就是结合了两者的优点 略微牺牲了一些空间 达到更好的综合性能</p><p><code>server.h</code>中的zset结构定义如下</p><pre><code class="c">typedef struct zset {    dict *dict;    zskiplist *zsl;} zset;</code></pre><p>Redis中的五种基本数据类型在<code>server.h</code>中的宏定义</p><pre><code class="c">/* The actual Redis Object */#define OBJ_STRING 0    /* String object. */#define OBJ_LIST 1      /* List object. */#define OBJ_SET 2       /* Set object. */#define OBJ_ZSET 3      /* Sorted set object. */#define OBJ_HASH 4      /* Hash object. */</code></pre><p>这5种类型在redis中都用<code>robj</code>来封装</p><pre><code class="c">typedef struct redisObject {    unsigned type:4;// 记录其封装的是哪种类型的数据    unsigned encoding:4;// 编码方式    unsigned lru:LRU_BITS; /* LRU time (relative to global lru_clock) or                            * LFU data (least significant 8 bits frequency                            * and most significant 16 bits access time). */    int refcount;//引用计数    void *ptr;} robj;</code></pre><p><code>ptr</code>字段指向某一实际的数据结构 可以看到创建某一个数据结构的实例时 需要创建robj和其本身 需要分配两次内存 两次内存分配操作效率低, 而且数据分离存储降低了计算机高速缓存的效率 对于sds较短的情况下得不偿失</p><p>而上图中字符串的<code>embstr</code> 则意味着 robj与<code>sds</code>一次分配完毕 连续存储</p><p>客户端结构体:</p><pre><code class="c">typedef struct client {    uint64_t id;            /* Client incremental unique ID. */    connection *conn;    int resp;               /* RESP protocol version. Can be 2 or 3. */    redisDb *db;            /* Pointer to currently SELECTed DB. */    robj *name;             /* As set by CLIENT SETNAME. */    sds querybuf;           /* Buffer we use to accumulate client queries. */    size_t qb_pos;          /* The position we have read in querybuf. */    sds pending_querybuf;   /* If this client is flagged as master, this buffer                               represents the yet not applied portion of the                               replication stream that we are receiving from                               the master. */    size_t querybuf_peak;   /* Recent (100ms or more) peak of querybuf size. */    int argc;               /* Num of arguments of current command. */    robj **argv;            /* Arguments of current command. */    struct redisCommand *cmd, *lastcmd;  /* Last command executed. */    user *user;             /* User associated with this connection. If the                               user is set to NULL the connection can do                               anything (admin). */    int reqtype;            /* Request protocol type: PROTO_REQ_* */    int multibulklen;       /* Number of multi bulk arguments left to read. */    long bulklen;           /* Length of bulk argument in multi bulk request. */    list *reply;            /* List of reply objects to send to the client. */    unsigned long long reply_bytes; /* Tot bytes of objects in reply list. */    size_t sentlen;         /* Amount of bytes already sent in the current                               buffer or object being sent. */    time_t ctime;           /* Client creation time. */    time_t lastinteraction; /* Time of the last interaction, used for timeout */    time_t obuf_soft_limit_reached_time;    uint64_t flags;         /* Client flags: CLIENT_* macros. */    int authenticated;      /* Needed when the default user requires auth. */    int replstate;          /* Replication state if this is a slave. */    int repl_put_online_on_ack; /* Install slave write handler on first ACK. */    int repldbfd;           /* Replication DB file descriptor. */    off_t repldboff;        /* Replication DB file offset. */    off_t repldbsize;       /* Replication DB file size. */    sds replpreamble;       /* Replication DB preamble. */    long long read_reploff; /* Read replication offset if this is a master. */    long long reploff;      /* Applied replication offset if this is a master. */    long long repl_ack_off; /* Replication ack offset, if this is a slave. */    long long repl_ack_time;/* Replication ack time, if this is a slave. */    long long psync_initial_offset; /* FULLRESYNC reply offset other slaves                                       copying this slave output buffer                                       should use. */    char replid[CONFIG_RUN_ID_SIZE+1]; /* Master replication ID (if master). */    int slave_listening_port; /* As configured with: SLAVECONF listening-port */    char slave_ip[NET_IP_STR_LEN]; /* Optionally given by REPLCONF ip-address */    int slave_capa;         /* Slave capabilities: SLAVE_CAPA_* bitwise OR. */    multiState mstate;      /* MULTI/EXEC state */    int btype;              /* Type of blocking op if CLIENT_BLOCKED. */    blockingState bpop;     /* blocking state */    long long woff;         /* Last write global replication offset. */    list *watched_keys;     /* Keys WATCHED for MULTI/EXEC CAS */    dict *pubsub_channels;  /* channels a client is interested in (SUBSCRIBE) */    list *pubsub_patterns;  /* patterns a client is interested in (SUBSCRIBE) */    sds peerid;             /* Cached peer ID. */    listNode *client_list_node; /* list node in client list */    /* If this client is in tracking mode and this field is non zero,     * invalidation messages for keys fetched by this client will be send to     * the specified client ID. */    uint64_t client_tracking_redirection;    /* Response buffer */    int bufpos;    char buf[PROTO_REPLY_CHUNK_BYTES];} client;</code></pre><p><code>redisDb</code>结构体表示</p><pre><code class="c">/* Redis database representation. There are multiple databases identified * by integers from 0 (the default database) up to the max configured * database. The database number is the &#39;id&#39; field in the structure. */typedef struct redisDb {    dict *dict;                 /* The keyspace for this DB */    dict *expires;              /* Timeout of keys with a timeout set */    dict *blocking_keys;        /* Keys with clients waiting for data (BLPOP)*/    dict *ready_keys;           /* Blocked keys that received a PUSH */    dict *watched_keys;         /* WATCHED keys for MULTI/EXEC CAS */    int id;                     /* Database ID */    long long avg_ttl;          /* Average TTL, just for stats */    unsigned long expires_cursor; /* Cursor of the active expire cycle. */    list *defrag_later;         /* List of key names to attempt to defrag one by one, gradually. */} redisDb;</code></pre><p>Redis支持通过命令<code>MULTI</code>开启事务 <code>EXEC</code>来执行事务 REDIS采用乐观锁机制 开启事务的同时可以使用<code>WATCH [key]</code>的命令监控关心的数据键 而<code>watch_keys</code>字段就是存储的被<code>watch</code>命令监控的所有数据键 其中<code>key-value</code>分别为数据键与客户端对象 当redis服务器接收到写命令时 会从该字典查找该数据键 找到说明正有客户端在监控此数据键 于是标记客户端对象为dirty 待REDIS收到该客户端的额EXEC命令时 如果客户端带有dirty标记 则会拒接执行事务</p><p>服务端结构体<code>redisServer</code>也定义在<code>server.h</code>中 要素过多不再展示</p><h2 id="事件处理"><a href="#事件处理" class="headerlink" title="事件处理"></a>事件处理</h2><p>各类事件的结构体定义在<code>ae.h</code>中 并将其封装在<code>aeEventLoop</code>中</p><pre><code class="c">/* File event structure */typedef struct aeFileEvent {    int mask; /* one of AE_(READABLE|WRITABLE|BARRIER) */    aeFileProc *rfileProc;// 读事件的处理函数指针    aeFileProc *wfileProc;// 写事件的处理函数指针    void *clientData;} aeFileEvent;/* Time event structure */typedef struct aeTimeEvent {    long long id; /* time event identifier. */    long when_sec; /* seconds */    long when_ms; /* milliseconds */    aeTimeProc *timeProc;    aeEventFinalizerProc *finalizerProc;    void *clientData;    struct aeTimeEvent *prev;    struct aeTimeEvent *next;} aeTimeEvent;/* A fired event */typedef struct aeFiredEvent {    int fd;    int mask;} aeFiredEvent;/* State of an event based program */typedef struct aeEventLoop {    int maxfd;   /* highest file descriptor currently registered */    int setsize; /* max number of file descriptors tracked */    long long timeEventNextId;    time_t lastTime;     /* Used to detect system clock skew */    aeFileEvent *events; /* Registered events */    aeFiredEvent *fired; /* Fired events 数组结构*/    aeTimeEvent *timeEventHead; // 链表结构    int stop; // 标记事件循环是否结束    void *apidata; /* This is used for polling API specific data 对4种I/O多路复用模型的封装*/    aeBeforeSleepProc *beforesleep;// 进程阻塞前调用    aeBeforeSleepProc *aftersleep; // 进程被唤醒后调用    int flags;} aeEventLoop;</code></pre><p>事件驱动程序通常存在<code>while/for</code>循环 循环等在事件发生并处理 Redis中的事件循环如下</p><pre><code class="c">void aeMain(aeEventLoop *eventLoop) {    eventLoop-&gt;stop = 0;    while (!eventLoop-&gt;stop) {        if (eventLoop-&gt;beforesleep != NULL)            eventLoop-&gt;beforesleep(eventLoop);        aeProcessEvents(eventLoop, AE_ALL_EVENTS|AE_CALL_AFTER_SLEEP);    }}</code></pre><p><code>AE_ALL_EVENTS</code>表示 该函数需要处理文件和时间事件<br><code>AE_CALL_AFTER_SLEEP</code> 表示阻塞等待文件事件后需要执行<code>afterSleep</code>函数</p><h3 id="关于epoll"><a href="#关于epoll" class="headerlink" title="关于epoll"></a>关于epoll</h3><p>Linux内核为处理大量并发网络连接而提供的解决方案 能够显著提高系统CPU利用率 其主要有3个API</p><ul><li><code>epoll_create</code> 该函数创建一个epoll专用的文件描述符</li><li><code>epoll_wait</code> 该函数阻塞进程 直到监控的若干个网络连接有事件发生</li><li><code>epoll_ctl</code> 注册修改或删除需要监控的事件</li></ul><p>具体参数及用途可见<code>ehall.h</code></p><p>redis并没有直接使用epoll提供的api而是对<code>evport, epoll, kqueue, ae_select</code> 四种I/O多路复用模型进行了统一封装 具体的代码实现在<code>ae_[model].c</code>中 编译时会检查OS支持的I/O多路复用模型 根据一定规则进行选择</p><p>Redis服务器启动时需要创建socket并监听 等待客户端连接 客户端与服务器建立socket连接之后 服务器会等待客户端的命令请求 服务器处理完客户端的命令请求之后会将回复暂存在client结构体的buf缓冲区 待客户端文件描述符的可写事件发生时才会真正往客户端发送命令回复 这些都需要创建对应的文件事件</p><p>如在<code>server.c</code>的<code>initServer</code>方法中调用该方位 并设置客户端连接的处理函数为<code>acceptTcpHandler</code></p><pre><code class="c">aeCreateFileEvent(server.el, server.ipfd[j], AE_READABLE,            acceptTcpHandler,NULL);</code></pre><p>事件循环的处理函数</p><pre><code class="c">/* Process every pending time event, then every pending file event * (that may be registered by time event callbacks just processed). * Without special flags the function sleeps until some file event * fires, or when the next time event occurs (if any). * * If flags is 0, the function does nothing and returns. * if flags has AE_ALL_EVENTS set, all the kind of events are processed. * if flags has AE_FILE_EVENTS set, file events are processed. * if flags has AE_TIME_EVENTS set, time events are processed. * if flags has AE_DONT_WAIT set the function returns ASAP until all * the events that&#39;s possible to process without to wait are processed. * if flags has AE_CALL_AFTER_SLEEP set, the aftersleep callback is called. * * The function returns the number of events processed. */int aeProcessEvents(aeEventLoop *eventLoop, int flags){    int processed = 0, numevents;    /* Nothing to do? return ASAP */    if (!(flags &amp; AE_TIME_EVENTS) &amp;&amp; !(flags &amp; AE_FILE_EVENTS)) return 0;    /* Note that we want call select() even if there are no     * file events to process as long as we want to process time     * events, in order to sleep until the next time event is ready     * to fire. */    if (eventLoop-&gt;maxfd != -1 ||        ((flags &amp; AE_TIME_EVENTS) &amp;&amp; !(flags &amp; AE_DONT_WAIT))) {        int j;        aeTimeEvent *shortest = NULL;        struct timeval tv, *tvp;        if (flags &amp; AE_TIME_EVENTS &amp;&amp; !(flags &amp; AE_DONT_WAIT))            shortest = aeSearchNearestTimer(eventLoop);        if (shortest) {            long now_sec, now_ms;            aeGetTime(&amp;now_sec, &amp;now_ms);            tvp = &amp;tv;            /* How many milliseconds we need to wait for the next             * time event to fire? */            long long ms =                (shortest-&gt;when_sec - now_sec)*1000 +                shortest-&gt;when_ms - now_ms;            if (ms &gt; 0) {                tvp-&gt;tv_sec = ms/1000;                tvp-&gt;tv_usec = (ms % 1000)*1000;            } else {                tvp-&gt;tv_sec = 0;                tvp-&gt;tv_usec = 0;            }        } else {            /* If we have to check for events but need to return             * ASAP because of AE_DONT_WAIT we need to set the timeout             * to zero */            if (flags &amp; AE_DONT_WAIT) {                tv.tv_sec = tv.tv_usec = 0;                tvp = &amp;tv;            } else {                /* Otherwise we can block */                tvp = NULL; /* wait forever */            }        }        if (eventLoop-&gt;flags &amp; AE_DONT_WAIT) {            tv.tv_sec = tv.tv_usec = 0;            tvp = &amp;tv;        }        /* Call the multiplexing API, will return only on timeout or when         * some event fires. */        numevents = aeApiPoll(eventLoop, tvp);        /* After sleep callback. */        if (eventLoop-&gt;aftersleep != NULL &amp;&amp; flags &amp; AE_CALL_AFTER_SLEEP)            eventLoop-&gt;aftersleep(eventLoop);        for (j = 0; j &lt; numevents; j++) {            aeFileEvent *fe = &amp;eventLoop-&gt;events[eventLoop-&gt;fired[j].fd];            int mask = eventLoop-&gt;fired[j].mask;            int fd = eventLoop-&gt;fired[j].fd;            int fired = 0; /* Number of events fired for current fd. */            /* Normally we execute the readable event first, and the writable             * event laster. This is useful as sometimes we may be able             * to serve the reply of a query immediately after processing the             * query.             *             * However if AE_BARRIER is set in the mask, our application is             * asking us to do the reverse: never fire the writable event             * after the readable. In such a case, we invert the calls.             * This is useful when, for instance, we want to do things             * in the beforeSleep() hook, like fsynching a file to disk,             * before replying to a client. */            int invert = fe-&gt;mask &amp; AE_BARRIER;            /* Note the &quot;fe-&gt;mask &amp; mask &amp; ...&quot; code: maybe an already             * processed event removed an element that fired and we still             * didn&#39;t processed, so we check if the event is still valid.             *             * Fire the readable event if the call sequence is not             * inverted. */            if (!invert &amp;&amp; fe-&gt;mask &amp; mask &amp; AE_READABLE) {                fe-&gt;rfileProc(eventLoop,fd,fe-&gt;clientData,mask);                fired++;            }            /* Fire the writable event. */            if (fe-&gt;mask &amp; mask &amp; AE_WRITABLE) {                if (!fired || fe-&gt;wfileProc != fe-&gt;rfileProc) {                    fe-&gt;wfileProc(eventLoop,fd,fe-&gt;clientData,mask);                    fired++;                }            }            /* If we have to invert the call, fire the readable event now             * after the writable one. */            if (invert &amp;&amp; fe-&gt;mask &amp; mask &amp; AE_READABLE) {                if (!fired || fe-&gt;wfileProc != fe-&gt;rfileProc) {                    fe-&gt;rfileProc(eventLoop,fd,fe-&gt;clientData,mask);                    fired++;                }            }            processed++;        }    }    /* Check time events */    if (flags &amp; AE_TIME_EVENTS)        processed += processTimeEvents(eventLoop);    return processed; /* return the number of processed file/time events */}</code></pre><p>简单的来讲 该函数会先遍历<code>eventLoop</code>的时间事件链表 找出最近下个时间事件触发的时间 并在该时间与当前时间的差值作为 <code>aeApiPoll</code>的最长阻塞时间</p><p><code>aeApiApoll</code>阻塞等待文件事件发生 其调用<code>epoll_wait</code> 而传入给<code>aeApiPoll</code>的tvp参数将作为<code>epoll_wait</code>的超时参数以ms为单位 <code>epoll_wait</code>在每次轮询时会检查是否已经达到该时间如果达到则返回之</p><p>然后<code>aeApiPoll</code>将获得事件(<code>aeApiState</code>的<code>events</code>数组)置到<code>eventLoop</code>的<code>fired</code>数组中 并进行行管的属性设置 如事件类型的设置等等</p><p>随后返回到<code>aeProcessEvent</code>函数中 由该函数对已触发的文件事件进行进一步的处理</p><p>最后<code>aeProcessEvent</code>处理时间事件</p><h2 id="时间事件"><a href="#时间事件" class="headerlink" title="时间事件"></a>时间事件</h2><p>Redis中只有一个时间事件在<code>server.c</code>的<code>initServer</code>函数中调用</p><pre><code class="c">aeCreateTimeEvent(server.el, 1, serverCron, NULL, NULL) </code></pre><p>用户可以设置<code>serverCron</code>函数的执行频率 其对应<code>server.hz</code>字段 最小为1 最大为500 默认为10 该函数会根据执行频率进行一些相关的操作 此外还会无条件执行诸如清除过期客户端 过期键等操作</p>]]></content>
    
    
    <categories>
      
      <category>源码</category>
      
      <category>C</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Redis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Redis源码解析笔记</title>
    <link href="/2019/08/07/Redis%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%AC%94%E8%AE%B0/index/"/>
    <url>/2019/08/07/Redis%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%AC%94%E8%AE%B0/index/</url>
    
    <content type="html"><![CDATA[<h1 id="Redis源码解析笔记"><a href="#Redis源码解析笔记" class="headerlink" title="Redis源码解析笔记"></a>Redis源码解析笔记</h1><p>学习redis源码顺便学下C</p><h2 id="数据类型与结构"><a href="#数据类型与结构" class="headerlink" title="数据类型与结构"></a>数据类型与结构</h2><h3 id="简单动态字符串-SDS-Simple-Dynamic-String"><a href="#简单动态字符串-SDS-Simple-Dynamic-String" class="headerlink" title="简单动态字符串(SDS, Simple Dynamic String)"></a>简单动态字符串(SDS, Simple Dynamic String)</h3><p>结构体基本定义</p><pre><code class="c">struct __attribute__ ((__packed__)) sdshdr64 {    uint64_t len; /* used */    uint64_t alloc; /* excluding the header and null terminator */    unsigned char flags; /* 3 lsb of type, 5 unused bits */    char buf[];};</code></pre><p><code>__attribute__</code>用来在函数或数据声明中设置其属性(函数属性 变量属性 类型属性) 这些参数用来对编译器的优化进行调整和设置 括号内为其参数 <code>__attribute__ ((__packed__))</code>宏用来取消C编译器对结构体进行对齐优化 通常情况下 结构体按照所有变量大小的最小公倍数做数字对齐 用<code>packed</code>修饰后 则变为按1字节对齐 在这里这样设置可以减少每个sds结构体的内存占用</p><p>同样出于提高内存利用率的考虑 <code>sds.h</code>中定义了5种sds结构体 最小的为其使用于创建长度小于32字节的字符串 可以看到其直接省略到了<code>len</code>和<code>alloc</code>字段 字符串的长度直接用flags字段的后5位表示</p><pre><code class="c">struct __attribute__ ((__packed__)) sdshdr5 {    unsigned char flags; /* 3 lsb of type, and 5 msb of string length */    char buf[];};</code></pre><p>通过<code>char buf[]</code>来实现扩容方便且二进制安全的字符串(考虑C语言中以<code>\0</code>作为字符串终止的标志由此带来的一系列问题) <code>len</code>记录已用长度 <code>alloc</code>记录总分配长度  <code>flags</code>的低3位记录类型 高5位保留</p><h4 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h4><ul><li><p>创建字符串</p><p>  <code>sds sdsnewlen(const void *init, size_t initlen)</code>其直接返回sds结构体的<code>buf</code>指针 <code>buf=sh+hdrlen</code> sh是为结构体变量起始地址 hdrlen是对应sds类型的header长度 header长度指除buf以外的结构体字段总长 注意sdshdr5 会被转化为sdshdr8<br>  而<code>sh = s_malloc(hdrlen+initlen+1);</code>是为了算上末尾的<code>\0</code></p></li><li><p>释放字符串</p><ul><li>提供了两种方式<code>void sdsfree(sds s)</code>直接释放内存</li><li>sdsclear<pre><code class="c">void sdsclear(sds s) {sdssetlen(s, 0);s[0] = &#39;\0&#39;;}</code></pre>重置统计值达到清空目的 该方法仅仅讲sds的<code>len</code>字段归0</li></ul></li><li><p>拼接字符串</p></li></ul><p><img src="/img/redis_sds_cat.png" srcset="/img/loading.gif" alt="sdscat"></p><p>sdsMakeRoomFor最后对类型的判断 如果类型没有发生变化就对原sds调用realloc 否则直接调用malloc重新分配内存</p><p>关于<code>realloc</code>函数</p><p><code>realloc(void *__ptr, size_t __size)</code>：更改已经配置的内存空间，即更改由malloc()函数分配的内存空间的大小。</p><p>如果将分配的内存减少，realloc仅仅是改变索引的信息。</p><p>如果是将分配的内存扩大，则有以下情况：<br>1）如果当前内存段后面有需要的内存空间，则直接扩展这段内存空间，realloc()将返回原指针。<br>2）如果当前内存段后面的空闲字节不够，那么就使用堆中的第一个能够满足这一要求的内存块，将目前的数据复制到新的位置，并将原来的数据块释放掉，返回新的内存块位置。<br>3）如果申请失败，将返回NULL，此时，原来的指针仍然有效。</p><h3 id="有序集合–跳跃表"><a href="#有序集合–跳跃表" class="headerlink" title="有序集合–跳跃表"></a>有序集合–跳跃表</h3><p><img src="/img/ds_skiplist.jpg" srcset="/img/loading.gif" alt="sl"></p><p>跳跃表性质:</p><p>(1) 由很多层结构组成</p><p>(2) 每一层都是一个有序的链表</p><p>(3) 最底层(Level 1)的链表包含所有元素</p><p>(4) 如果一个元素出现在 Level i 的链表中，则它在 Level i 之下的链表也都会出现。</p><p>(5) 每个节点包含两个指针，一个指向同一链表中的下一个元素，一个指向下面一层的元素。</p><p>(6) 每层结构包含指向本层下一个节点的指针 指向本层下个节点中间所跨越的节点个数为本层的<code>跨度(span)</code></p><p>相比于平衡树 跳跃表实现更加简单 但是牺牲了一定的空间来达到快速查找的目的 其查询插入删除操作复杂度均为<strong>O(logN)</strong></p><p>redis中的跳跃表节点定义:</p><pre><code class="c">typedef struct zskiplistNode {    sds ele;//存储字符类型的数据    double score;// 存储排序的分值    struct zskiplistNode *backward;// 指向最底层的本节点的前一个节点 用于从后向前遍历时    struct zskiplistLevel {        struct zskiplistNode *forward;//指向本层的下一个节点        unsigned long span;    } level[];} zskiplistNode;</code></pre><p>跳跃表以及有序集合结构定义:</p><pre><code class="c">typedef struct zskiplist {    struct zskiplistNode *header, *tail;    unsigned long length;    int level;} zskiplist;typedef struct zset {    dict *dict;    zskiplist *zsl;} zset;</code></pre><h4 id="基本操作-1"><a href="#基本操作-1" class="headerlink" title="基本操作"></a>基本操作</h4><ul><li>插入</li></ul><p>参见t_set.c的<code>zskiplistNode *zslInsert(zskiplist *zsl, double score, sds ele)</code>函数</p><pre><code class="c">zskiplistNode *zslInsert(zskiplist *zsl, double score, sds ele) {    zskiplistNode *update[ZSKIPLIST_MAXLEVEL], *x;    unsigned int rank[ZSKIPLIST_MAXLEVEL];    int i, level;    serverAssert(!isnan(score));    x = zsl-&gt;header;    for (i = zsl-&gt;level-1; i &gt;= 0; i--) {        /* store rank that is crossed to reach the insert position */        rank[i] = i == (zsl-&gt;level-1) ? 0 : rank[i+1];        while (x-&gt;level[i].forward &amp;&amp;                (x-&gt;level[i].forward-&gt;score &lt; score ||                    (x-&gt;level[i].forward-&gt;score == score &amp;&amp;                    sdscmp(x-&gt;level[i].forward-&gt;ele,ele) &lt; 0)))        {            rank[i] += x-&gt;level[i].span;            x = x-&gt;level[i].forward;        }        update[i] = x;    }    /* we assume the element is not already inside, since we allow duplicated     * scores, reinserting the same element should never happen since the     * caller of zslInsert() should test in the hash table if the element is     * already inside or not. */    level = zslRandomLevel();    if (level &gt; zsl-&gt;level) {        for (i = zsl-&gt;level; i &lt; level; i++) {            rank[i] = 0;            update[i] = zsl-&gt;header;            update[i]-&gt;level[i].span = zsl-&gt;length;        }        zsl-&gt;level = level;    }    x = zslCreateNode(level,score,ele);    for (i = 0; i &lt; level; i++) {        x-&gt;level[i].forward = update[i]-&gt;level[i].forward;        update[i]-&gt;level[i].forward = x;        /* update span covered by update[i] as x is inserted here */        x-&gt;level[i].span = update[i]-&gt;level[i].span - (rank[0] - rank[i]);        update[i]-&gt;level[i].span = (rank[0] - rank[i]) + 1;    }    /* increment span for untouched levels */    for (i = level; i &lt; zsl-&gt;level; i++) {        update[i]-&gt;level[i].span++;    }    x-&gt;backward = (update[0] == zsl-&gt;header) ? NULL : update[0];    if (x-&gt;level[0].forward)        x-&gt;level[0].forward-&gt;backward = x;    else基本操作        zsl-&gt;tail = x;    zsl-&gt;length++;    return x;}</code></pre><p>大概是3步</p><ol><li>找到每一层该节点应插入的位置 用rank和update数组记录每层的步长和节点</li><li>插入节点的高度随机决定 调整跳跃表的高度</li><li>插入节点</li></ol><ul><li>删除</li></ul><p>以后再说。。。</p><h4 id="跳跃表在Redis中的应用"><a href="#跳跃表在Redis中的应用" class="headerlink" title="跳跃表在Redis中的应用"></a>跳跃表在Redis中的应用</h4><p>有序集合在Redis中除跳跃表外 还有<strong>压缩列表</strong>的实现方式 创建有序集合时默认采用<strong>压缩列表</strong>的实现 插入元素时 如果压缩列表条目数大于<code>zset-max-ziplist-entries</code>或要插入的字符串长度大于<code>zset-max-ziplist-value</code><br>就会将之转化为跳跃表 这两个参数可以在配置文件中进行配置</p><h3 id="有序集合–压缩列表"><a href="#有序集合–压缩列表" class="headerlink" title="有序集合–压缩列表"></a>有序集合–压缩列表</h3><p>压缩列表本质上就是一个<strong>字节数组</strong></p><p><img src="/img/redis_ziplist_struct.jpg" srcset="/img/loading.gif" alt="zls"></p><p>其中<code>zllen</code>字段记录压缩列表的元素个数 占2字节 因此压缩列表存储的元素最多不能超过65535个</p><p>而<code>entry</code>的结构如下 <strong>| previous_entry_len | encoding | content|</strong> 其中previous_entry_len字段在前一个entry不超过254个字节时占1bytes 超过则占 5bytes 其中第一字节固定为0xFE 后4字节为实际长度</p><ul><li><p>关于string.h中的<code>memmove</code>和<code>memcpy</code></p><p>  二者的作用都是拷贝一定长度的内存的内容 前者允许src和dst的内存区域存在重叠  后者不允许</p></li></ul><h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>压缩列表的一个重要的问题在于其插入和删除元素时可能会由于entry的<code>previous_entry_len</code>字段的变动引发级联(连锁)更新 这种更新效率极其低下</p><h3 id="字典-散列表"><a href="#字典-散列表" class="headerlink" title="字典(散列表)"></a>字典(散列表)</h3><p>扩容的时机 在向字典中添加元素时会调用<code>_dictKeyIndex</code>进而调用<code>_dictExpandIfNeeded</code>进行判断</p><p>hash: 把任意长度的输入通过散列算法转化为固定类型固定长度到散列值 hash函数可以把不同键转换成唯一的整数类型 通常具有如下特征</p><ol><li>相同的输入经hash计算后得出相同的输出</li><li>不同的输入经hash计算后一般得出不同输出的值 但也有可能得出相同输出值</li></ol><p>Redis客户端采取<strong>times33</strong>散列函数 核心算法:<code>hash(i) = hash(i-1) * 33 + str[i]</code>来计算字符串的hash值 计算速度快输出值分布较好 redis服务端采取<strong>siphash</strong>算法其对于有规律计算出到键也具有较好的随机分布性但实现较为复杂</p><ul><li>redis中的hash表结构体定义</li></ul><pre><code class="c">/* This is our hash table structure. Every dictionary has two of this as we * implement incremental rehashing, for the old to the new table. */typedef struct dictht {    dictEntry **table; //指针数组用于存储键值对    unsigned long size;//table数组的大小    unsigned long sizemask;//掩码=size-1    unsigned long used;//table数组已存元素个数 包括next单链表的数据} dictht;</code></pre><p>redis中的hash表中的size也始终保持为2的幂次方 以方便将对hash的取余运算转化为位运算 其初始值为4</p><ul><li>redis中的hash表节点定义</li></ul><pre><code class="c">typedef struct dictEntry {    void *key;    union {        void *val;        uint64_t u64;        int64_t s64;        double d;    } v;    struct dictEntry *next; //hash冲突时指向冲突的元素 形成单链表 采用头插法} dictEntry;</code></pre><ul><li>redis中的字典定义</li></ul><pre><code class="c">typedef struct dict {    dictType *type;//该字典对应的特定的操作函数    void *privdata;//该字典依赖的数据    dictht ht[2];//hash表    long rehashidx; /* rehashing not in progress if rehashidx == -1  存储的值表示hash表ht0的rehash操作进行到了哪个索引值*/    unsigned long iterators; /* number of iterators currently running */} dict;</code></pre><ul><li>字典的操作函数定义</li></ul><pre><code class="c">typedef struct dictType {    uint64_t (*hashFunction)(const void *key);    void *(*keyDup)(void *privdata, const void *key);    void *(*valDup)(void *privdata, const void *obj);    int (*keyCompare)(void *privdata, const void *key1, const void *key2);    void (*keyDestructor)(void *privdata, void *key);    void (*valDestructor)(void *privdata, void *obj);} dictType;</code></pre><p><img src="/img/redis_dict.jpg" srcset="/img/loading.gif" alt="dict"></p><ul><li>添加元素</li></ul><p>db.c中可见</p><pre><code class="c">void setKey(redisDb *db, robj *key, robj *val) {    if (lookupKeyWrite(db,key) == NULL) {        dbAdd(db,key,val);    } else {        dbOverwrite(db,key,val);    }    incrRefCount(val);    removeExpire(db,key);    signalModifiedKey(db,key);}</code></pre><ol><li>调用dbFind函数查询是否存在不存在则调用<code>dbAdd</code>函数 否则调用<code>dbOverwrite</code>函数添加元素</li><li><code>dbOverwrite</code>最终调用了dict.h中的<code>dictAdd函数</code></li></ol><pre><code class="c">dictEntry *dictAddRaw(dict *d, void *key, dictEntry **existing){    long index;    dictEntry *entry;    dictht *ht;    if (dictIsRehashing(d)) _dictRehashStep(d);// 。。。    /* Get the index of the new element, or -1 if     * the element already exists. */    if ((index = _dictKeyIndex(d, key, dictHashKey(d,key), existing)) == -1)// 已存在返回NULL        return NULL;    /* Allocate the memory and store the new entry.     * Insert the element in top, with the assumption that in a database     * system it is more likely that recently added entries are accessed     * more frequently. */    ht = dictIsRehashing(d) ? &amp;d-&gt;ht[1] : &amp;d-&gt;ht[0];// 如果正在进行rehash操作则插入到ht[1]中    entry = zmalloc(sizeof(*entry));    entry-&gt;next = ht-&gt;table[index];    ht-&gt;table[index] = entry;    ht-&gt;used++;    /* Set the hash entry fields. */    dictSetKey(d, entry, key);    return entry;}</code></pre><h4 id="渐进式rehash"><a href="#渐进式rehash" class="headerlink" title="渐进式rehash"></a>渐进式rehash</h4><p>rehash在除了在扩容时会触发 缩容时也会 其分以下几步完成</p><ol><li>给hash表ht[1]申请足够的空间 扩容时空间大小为当前容量*2 当使用量不足总空间的10%时 进行缩容 缩容大小为整好包含d-&gt;ht[0].used个节点的2的幂次方整数 并标记<code>rehashidx</code>字段为0</li><li>进行rehash操作调用<code>dictRehash</code>实现 重新计算ht[0]中每个键的hash值与索引值 一次添加到hash表ht[1] 并把老hash表中该键值对删除 把字典中字段rehashidx修改为hash表ht[0]操作节点的索引值</li><li>清空ht[0] 对调ht[0]和ht[1]的值 标记<code>rehashidx</code>为-1</li></ol><h4 id="字典的遍历"><a href="#字典的遍历" class="headerlink" title="字典的遍历"></a>字典的遍历</h4><p>两种方式</p><ul><li>全遍历 <code>keys</code> 一次遍历完整个数据库</li><li>间断遍历 <code>hscan</code> 每次命只取部分数据 分多次遍历</li></ul><pre><code class="c">typedef struct dictIterator {    dict *d;//迭代字典    long index;//当前迭代到hash表中哪个索引值    int table, safe;//table表示正在迭代的hash表 ht[0]与ht[1] safe表示当前创建的迭代器是否是安全迭代器    dictEntry *entry, *nextEntry;//当前节点 下一个节点    /* unsafe iterator fingerprint for misuse detection. */    long long fingerprint;//字典指纹 当字典发生改变时该值随之改变} dictIterator;</code></pre><p>迭代器分为两类</p><ol><li>普通迭代器 只遍历数据</li><li>安全迭代器 支持遍历的同时更新数据</li></ol><h5 id="全遍历"><a href="#全遍历" class="headerlink" title="全遍历"></a>全遍历</h5><ul><li><p>普通迭代器迭代过程</p><ol><li><code>dictGetIterator</code>函数初始化一个普通迭代器</li><li>循环调用<code>dictNext</code>函数依次遍历字典中hash表的节点 首次遍历时会通过<code>dictFingerprint</code>拿到当前字典的指纹值</li><li>调用<code>dictNext</code>函数遍历完字典hash表中全部节点数据后 释放迭代器时会继续调用<code>dictFingerprint</code>函数计算字典的指纹值 并于首次拿到的指纹值比较 不等则输出assertion failed 退出执行</li></ol></li><li><p>安全迭代器</p><p>  通过限制rehash的进行来保证数据的准确性 迭代过程中可以进行增删改查等操作 如keys 命令就采用这种方式</p><ol><li>调用<code>dictGetSafeIterator</code>函数初始化一个安全迭代器 此时会把<code>iter-&gt;safe</code>置1以标识之</li><li>循环调用<code>dictNext</code>函数依次遍历字典中hash表的节点 首次遍历时会把iterators字段进行+1操作 确保渐进式rehash在迭代过程中会被中断</li><li>迭代完毕后释放iterator并对字典中的<code>iterators</code>字段-1 确保后续rehash正常进行</li></ol></li></ul><pre><code class="c">static void _dictRehashStep(dict *d) {    if (d-&gt;iterators == 0) dictRehash(d,1);}</code></pre><h5 id="间断遍历"><a href="#间断遍历" class="headerlink" title="间断遍历"></a>间断遍历</h5><ul><li><p>核心算法:</p><p>  reverse binary iteration 游标对sizemask的反码进行或运算 然后按bit反转 将反转后的结果自增 再反转 得出结果 这种算法可能会导致重复遍历 但不会遗漏 其基于这样一个事实:redis扩容及缩容都整好为整倍数的增长或者减少 根据这个特征很容易就能推导出同一个节点在扩容/缩容后在新的hash表中的分布位置从而避免重复遍历或者遗漏遍历 关于该算法的说明可参见<a href="https://github.com/antirez/redis/pull/579#issuecomment-16871583" target="_blank" rel="noopener">github提交记录</a></p></li></ul><p>以下是commitor在对于该算法的说明的部分摘要</p><blockquote><p>Whenever a hash table is grown from size 2^N to size 2^(N+1), we know that elements in slot i will be distributed across slots i and i + N, because growing adds a significant bit to every element’s hash. When an iterator has just started and emitted slot 0, we would like to not emit slot N again in the new table because we know it was already emitted via slot 0 in the old table. We wish to do the same for larger growth factors. For instance: going from N to 4N, slot i will be distributed among i, i + N, i + 2N, and i + 3N. More generally, going from N to 2^M * N, slot i will be distributed among i + j*N where j &lt;- [0, 2^M-1]. If we look at the binary representation of these slots, it is clear that they have the same M most significant bits. Therefore, if we have an iteration scheme where these bits are ignored, we can avoid re-visiting these slots.</p></blockquote><p>db.c中的<code>void scanGenericCommand(client *c, robj *o, unsigned long cursor)</code>部分代码</p><pre><code class="c">do {            cursor = dictScan(ht, cursor, scanCallback, NULL, privdata);        } while (cursor &amp;&amp;              maxiterations-- &amp;&amp;              listLength(keys) &lt; (unsigned long)count);</code></pre><p>dict.c中的<code>dictScan</code>部分相关代码:</p><pre><code class="c">unsigned long dictScan(dict *d,                       unsigned long v,                       dictScanFunction *fn,                       dictScanBucketFunction* bucketfn,                       void *privdata){    ...      t0 = &amp;(d-&gt;ht[0]);        m0 = t0-&gt;sizemask;        /* Emit entries at cursor */        if (bucketfn) bucketfn(privdata, &amp;t0-&gt;table[v &amp; m0]);        de = t0-&gt;table[v &amp; m0];        while (de) {            next = de-&gt;next;            fn(privdata, de);            de = next;        }        /* Set unmasked bits so incrementing the reversed cursor         * operates on the masked bits */        v |= ~m0;        /* Increment the reverse cursor */        v = rev(v);        v++;        v = rev(v);    ...}</code></pre><h3 id="整数集合-intset"><a href="#整数集合-intset" class="headerlink" title="整数集合(intset)"></a>整数集合(intset)</h3><p>一个高效有序 存储整型数据的数据结构</p><pre><code>127.0.0.1:6379&gt; sadd test1Set 1 2 -1 -6(integer) 4127.0.0.1:6379&gt; object encoding test1Set&quot;intset&quot;</code></pre><p>其会在以下两种情况下发生转化:</p><ol><li>元素个数超过一定数量(默认512)后转化为hashtable</li><li>增加非整型变量时转化为hashtable</li></ol><p>intset结构定义:</p><pre><code class="c">typedef struct intset {    uint32_t encoding;    uint32_t length;    int8_t contents[];} intset;</code></pre><p>其查找采用二分搜索</p><h3 id="quicklist"><a href="#quicklist" class="headerlink" title="quicklist"></a>quicklist</h3><p>在引入quicklist之前 redis采用ziplist和adlist(双端链表)作为list的底层实现 当元素比较少且元素长度较短时 redis采用ziplist 否则采用adlist 因为当元素长度比较小时 ziplist可以有效节省存储空间 但ziplist存储空间是连续的 当元素个数比较多时 修改元素必须重新分配存储空间 无疑会影响redis的执行效率</p><p>quikclist由list和ziplist结合而成</p><p>quicklist是一个双向链表 而其每一个节点都是一个ziplist 当ziplist节点个数过多 quicklist退化为双向链表 一个极端的情况下每个ziplist节点只包含一个entry; 当ziplist元素过少时 quicklist可退化为ziplsit 一种极端的情况是quicklist中只有一个ziplist节点</p><p><img src="/img/redis_quicklist_1.png" srcset="/img/loading.gif" alt="QL"></p><pre><code class="c">typedef struct quicklist {    quicklistNode *head;    quicklistNode *tail;    unsigned long count;        /* total count of all entries in all ziplists */    unsigned long len;          /* number of quicklistNodes */    int fill : 16;              /* fill factor for individual nodes */    unsigned int compress : 16; /* depth of end nodes not to compress;0=off */} quicklist;</code></pre><p>其中的<code>fill</code>字段为正数时用来表明每个ziplist最多包含的数据项数 为负数时表示ziplist最大占用内存-1为4kB -5为64kb</p><pre><code class="c">/* quicklistNode is a 32 byte struct describing a ziplist for a quicklist. * We use bit fields keep the quicklistNode at 32 bytes. * count: 16 bits, max 65536 (max zl bytes is 65k, so max count actually &lt; 32k). * encoding: 2 bits, RAW=1, LZF=2. * container: 2 bits, NONE=1, ZIPLIST=2. * recompress: 1 bit, bool, true if node is temporarry decompressed for usage. * attempted_compress: 1 bit, boolean, used for verifying during testing. * extra: 10 bits, free for future use; pads out the remainder of 32 bits */typedef struct quicklistNode {    struct quicklistNode *prev;    struct quicklistNode *next;    unsigned char *zl;    unsigned int sz;             /* ziplist size in bytes */    unsigned int count : 16;     /* count of items in ziplist */    unsigned int encoding : 2;   /* RAW==1 or LZF==2 */    unsigned int container : 2;  /* NONE==1 or ZIPLIST==2 */    unsigned int recompress : 1; /* was this node previous compressed? */    unsigned int attempted_compress : 1; /* node can&#39;t compress; too small */    unsigned int extra : 10; /* more bits to steal for future usage */} quicklistNode;</code></pre><p>其中的<code>zl</code>字段指向该节点对应的ziplist结构</p><p>quicklist每个节点的实际数据存储结构为ziplist 该结构主要优点就是节省存储空间 redis还允许对其进行进一步压缩 redis采用<strong>LZF</strong>算法 压缩过后的数据分为多个片段 每个片段有两个部分 <strong>一个部分是解释字段 另一个部分是存放具体的数据字段</strong> 解释字段栈1-3字节 数据字段可能不存在 </p><p>该压缩算法的基本思想比较简单就是 <strong>数据与前面重复的 记录重复位置以及重复长度 否则直接记录原始数据内容</strong></p><p>具体的压缩格式略过</p><p>具体代码实现略过 看着过于复杂</p>]]></content>
    
    
    <categories>
      
      <category>源码</category>
      
      <category>C</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Redis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>内存管理</title>
    <link href="/2019/04/08/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/index/"/>
    <url>/2019/04/08/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/index/</url>
    
    <content type="html"><![CDATA[<h1 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h1><h2 id="虚拟内存"><a href="#虚拟内存" class="headerlink" title="虚拟内存"></a>虚拟内存</h2><h3 id="虚拟内存的作用"><a href="#虚拟内存的作用" class="headerlink" title="虚拟内存的作用"></a>虚拟内存的作用</h3><ol><li>将主存作为一个存储在磁盘上的地址空间的高速缓存 主存中只保留活动区域 并根据需要在磁盘和主存之间来回传送数据(内存页的换入换出)</li><li>为每个进程提供了一个一致的地址空间 从而简化了内存管理</li><li>保护了每个进程的地址空间不被其它进程破坏</li></ol><p>任意时刻 虚拟页面的集合分为3个不相交的子集</p><ol><li>未分配的 VM系统未分配的页</li><li>缓存的 当前已缓存在物理内存中的已分配页</li><li>未缓存的 未缓存在物理内存中的已分配页</li></ol><p><img src="/img/vm-cache.png" srcset="/img/loading.gif" alt=""></p><h3 id="地址翻译"><a href="#地址翻译" class="headerlink" title="地址翻译"></a>地址翻译</h3><p>CPU的一个控制寄存器–页表基址寄存器(Page Table Base Register) 指向当前页表 n位的虚拟地址包括两个部分 一个P位的虚拟页面偏移(Virtual Page Offset VPO)和一个(n-p)位的虚拟页号(Virtual Page Number) MMU利用VPN 来选择适当的页表项(Virtual Page Entry VPE) 并将条目中的物理页号和虚拟地址中的VPO串联起来 就得到相应的物理地址</p><p><img src="/img/page_table_translation.jpg" srcset="/img/loading.gif" alt=""></p><ul><li>当页面命中时 CPU硬件的执行步骤如下</li></ul><ol><li>处理器生成一个虚拟地址 并將之传送给MMU</li><li>MMU生成PTE地址 并从高速缓存/主存请求得到它</li><li>高速缓存/主存向MMU返回PTE</li><li>MMU根据PTE构造物理地址</li><li>高速缓存/主存返回所请求的数据给处理器</li></ol><ul><li>当发生缺页时 执行步骤</li></ul><p>1-3 同上<br>4. PTE的有效位为0 MMU触发异常 传递CPU中的控制到操作系统内核中的异常处理程序<br>5. 缺页处理程序确定出物理内存中的牺牲页 如果该页面已经被修改了则將之换出到磁盘<br>6. 缺页处理程序页面调入新的页面 并更新PTE<br>7. 缺页程序返回到原来的进程 再次执行导致缺页的指令 CPU将引起缺页的虚拟地址重新发送给MMU 因为虚拟页面现在缓存在物理内存中 所以将会命中</p><h2 id="内存分页"><a href="#内存分页" class="headerlink" title="内存分页"></a>内存分页</h2><p><strong>思想</strong>：通过映射，将连续的线性地址（虚拟地址）与任意的物理内存地址相关联，逻辑上连续的线性地址其对应的物理地址可以不连续。</p><h3 id="32位下运行机制"><a href="#32位下运行机制" class="headerlink" title="32位下运行机制"></a>32位下运行机制</h3><p>先从CR3中获取页目录物理地址，然后用虚拟地址的高10位乘4作为页目录表中的偏移量去寻址目录项pde，从pde中读出页表物理地址，然后用中间10位乘4的积作为页表中的偏移量去寻址pte，从该pte中读出页框物理地址，用虚拟地址的低12位作为页框中的偏移量，最终完成映射。</p><p><strong>分页机制的作用</strong>：</p><ul><li>将线性地址转换成物理地址</li><li>用大小相等的页代替大小不等的段</li></ul><p>在分页机制打开前，要将页表加载到控制寄存器cr3中，这是启用分页机制的先决条件，因此，打开分页机制前加载到cr3寄存器中的仍是页表的物理地址，页表中的页表项的地址也是物理地址。</p><p>在32位保护模式 4K页大小，一级页表的情形下，虚拟线性地址的前20位用来标识内存块索引数，后12位用来在页内索引。（2^20=1M, 2^12=4K)</p><p>一级页表的弊端：</p><ol><li>一级页表全满需要占用4M空间(32位机 地址长32bit 4字节 1M个页表项 * 4字节 = 4M)</li><li>一级页表所有页表项必须提前建好，OS占虚拟地址空间的高1G，用户占低3G</li><li>每个进程都有自己的页表，进程较多的话，占用空间太大</li></ol><p>在32位保护模式 4K页大小，二级页表的情形下，高10位用来定位页目录中的页表，中间10位用来定位物理页，低12位用来页内索引</p><p><strong>启动分页机制</strong>:</p><ol><li>准备好页目录及页表</li><li>将页表（页目录）地址写入控制寄存器cr3</li><li>寄存器cr0的PG位置1</li></ol><p><strong>页表项及页目录结构</strong>：</p><p>高12～31位存储地址，0～11存储相关属性</p><h2 id="翻译后备缓冲器TLB-又称快表-Translation-Lookaside-Buffer"><a href="#翻译后备缓冲器TLB-又称快表-Translation-Lookaside-Buffer" class="headerlink" title="翻译后备缓冲器TLB(又称快表 Translation Lookaside Buffer)"></a>翻译后备缓冲器TLB(又称快表 Translation Lookaside Buffer)</h2><p>用来缓存虚拟地址页框和物理地址的映射关系，用以避免过于频繁的映射计算和内存读取，其对开发人员不可见，但可以通过重新加载cr3寄存器使整个TLB失效，或者通过<strong>invlpg（invalidate page）</strong>指令来间接更新TLB，invlpg指令用来在TLB中刷新某个虚拟地址对应的条目，指令格式为<strong>invlpg [m]</strong> （m表示操作数为虚拟地址）</p><p>当使用TLB并命中时 <strong>所有的地址翻译都在芯片上的MMU执行</strong> 因此速度非常快</p><h3 id="多级页表"><a href="#多级页表" class="headerlink" title="多级页表"></a>多级页表</h3><p>假设有一个32位的地址空间 4KB的页 和一个4字节的PTE 那么即使应用所引用的只是虚拟地址空间中的很小一部分 也总需要一个4MB的页表驻留内存中 如果是64位系统那么这一现象将更加严重</p><p>仅有一级页表的话会导致页表占用内存过多 内存利用效率低下的问题 因此 我们可以引入多级页表来缓解这一问题 这种方法可以从两个方面来解决这一问题</p><ol><li>如果一级页表中的一个PTE是空的 那么相应的二级页表就根本不会存在 这代表着一种巨大的节约 因为对于一个普通的程序 4GB虚拟地址空间的大部分都会是未分配的</li><li>只有一级月表才需要总是在内存中 虚拟内存系统可以在需要时创建 页面调入或调出二级页表从而减少了内存的压力</li></ol><p><img src="/img/pt_of_corei7.jpg" srcset="/img/loading.gif" alt=""></p><h2 id="内存段的保护"><a href="#内存段的保护" class="headerlink" title="内存段的保护"></a>内存段的保护</h2><p>当引用一个内存段时，需要往寄存器中加载段选择子，为了避免出现非法引用内存的情况，需要作如下检查：</p><ol><li>根据选择子的值验证段描述符是否越界，即选择子的高13位的值一定要小于等于描述符表（GDT || LDT）中描述符的个数，就像数组下标一样，不能越界</li></ol><p>公式：</p><pre><code class="shell">描述符表基地址+选择子中的索引值 * 8 + 7 &lt;= 描述符表基地址 + 描述符表界限值</code></pre><ol start="2"><li>根据描述符中的<strong>Type</strong>字段，判断段寄存器的用途和段类型是否匹配。</li><li>检查完Type后，检查段是否存在，如果存在则将选择子装载入段寄存器，同时段描述符缓冲寄存器也会更新为对应选择子对应的段描述符的内容，随后将段描述符中的A位置为一，表示已访问过</li></ol><h2 id="特权级"><a href="#特权级" class="headerlink" title="特权级"></a>特权级</h2><p>任务是由处理器执行的，任务在特权级变换时，本质上时处理器的特权级在变化，由一个特权级变为另一个。<em>处理器在不同的特权级下使用不同级别的栈，原因是如果在同一个栈中容纳所有特权级的数据时，这种交叉引用会使栈变得非常混乱。</em></p><p>特权级转移分为两类：一类是由中断门 调用门等实现特权级由低向高转移，另一类是调用返回指令从高向低返回，这是<strong>唯一</strong>一种能让处理器从高向低转移的情况。</p><p>当特权级由低向高转移时，它会自动把当时低特权级的栈地址压入转移后高特权级所在的栈中。</p><p>TSS中的栈指针时固定的，如果想要保留上一次高特权级的栈指针，需要手动更新TSS中相应的栈数据。</p><h3 id="CPL-DPL-RPL"><a href="#CPL-DPL-RPL" class="headerlink" title="CPL DPL RPL"></a>CPL DPL RPL</h3><p>当前特权级CPL保存在CS选择子中的RPL部分（CS代码段寄存器的低两位）</p><p>处理器特权级转移时，会根据这三个值进行特权级检查。</p><p>当访问者访问某段描述符所描述的内存段时，其特权级应高于该描述符DPL所表示的特权级。</p><h4 id="一致性代码段"><a href="#一致性代码段" class="headerlink" title="一致性代码段"></a>一致性代码段</h4><p>如果段描述符中的S位为0，则用type字段中的C位来表示其是否是一致性代码段，C为1则代表是一致性代码段。一致性代码段是指如果自己是转移后的目标代码段，自己的特权级（DPL）一定要大于等于转移前的CPL（在数值上 转移前CPL&gt;=目标段DPL）。</p><p>一致性代码段的一大特点是转移后的特权级不与自己的特权级（DPL）为主，而是与转移前的低特权级一致，听取 依从转移前的低特权级，也就是说，处理器遇到目标段为一致性代码段时，并不会将CPL用该目标段的DPL替换。这种转移本身并没有提升特权级，只是可以跑到更高的代码段去执行指令，对计算机而言，比干不会因为特权级的升高而产生潜在危险。</p><p>注意，代码段有一致性 非一致性之分，数据段总是非一致的，即数据段不允许比本段特权级更低的代码段访问。</p><h4 id="门-调用门与RPL"><a href="#门-调用门与RPL" class="headerlink" title="门 调用门与RPL"></a>门 调用门与RPL</h4><p>处理器只有通过“门结构”才能低特权转移到高特权</p><p>门结构：_记录一段程序起始地址的描述符_，同段描述符类似，都是8字节大小的数据结构，用来描述门中通向的代码。</p><p>门描述符的结构见 _《操作系统真相还原》236页，我懒得再去网上找了。</p><p>仅仅凭CPL和DPL，处理器可以通过门访问并获得任何资源，无法保证计算机的安全。RPL（Request Privilege Level）的出现解决了这个问题（低特权级的程序访问高特权级的资源）。它代表的是真正请求者的特权级，即真正资源需求者的CPL。<br>引入RPL后，（通过调用门）请求某高DPL级别的资源时，需在数值上满足 <em>DPL_GATE &gt;= CPL &gt;= DPL &amp;&amp; RPL &lt;= DPL</em></p><p>以下公式皆为数值上</p><p>不通过门，直接访问:</p><p><em>CPL &gt;= target code segment DPL &amp;&amp; RPL &gt;= target code segment DPL</em></p><p><em>CPL &lt;= target data segment DPL &amp;&amp; RPL &lt;= target data segment DPL</em></p><p>对于非一致性代码段，jmp指令只能平级转移：</p><p><em>DPL_GATE &gt;= CPL = DPL_CODE</em><br><em>RPL &lt;= DPL_GATE</em></p><h2 id="组织虚存的数据结构"><a href="#组织虚存的数据结构" class="headerlink" title="组织虚存的数据结构"></a>组织虚存的数据结构</h2><p>主要讨论位图和链表这两种方式</p><ul><li><p>位图</p><p>内存被划分成小到几个字或大到几千字节的分配单元，每个的单元对应位图中的一位，0表示空闲，1表示占用。<br>这种方式的主要问题是在决定把一个占k个分配单元的进程调入内存时，存储管理器必须搜索位图。在位图中找出由k个连续0的串。查找指定0串是一个耗时操作。</p></li><li><p>链表</p><p>维护一个记录已分配内存段和空闲内存段的链表，链表中的一个节点或者包含一个进程，或者是两个进程间的一个空闲区。链表的每一个节点都包含以下区域：空闲区或进程的指示标志 起始地址 长度 和指向下一节点的指针。</p><p>因为进程表中表示终止进程的节点中，通常含有指向对应于其段链表节点的指针，因此使用双端链表要比单链表更好，因为其更容易找到上一个节点并合并。</p></li></ul><p>存储管理器分配的几种方法：首次适配，最佳适配，最差适配，快速适配法。</p><h2 id="slab分配器"><a href="#slab分配器" class="headerlink" title="slab分配器"></a>slab分配器</h2><p>这部分内容来自<a href="https://www.ibm.com/developerworks/cn/linux/l-linux-slab-allocator/index.html" target="_blank" rel="noopener">ibm developer</a></p><h3 id="内存存分配策略"><a href="#内存存分配策略" class="headerlink" title="内存存分配策略"></a>内存存分配策略</h3><p>每个内存管理器都使用了一种基于堆的分配策略。在这种方法中，大块内存（称为 堆）用来为用户定义的目的提供内存。当用户需要一块内存时，就请求给自己分配一定大小的内存。堆管理器会查看可用内存的情况（使用特定算法）并返回一块内存。搜索过程中使用的一些算法有 <strong>first-fit</strong>（在堆中搜索到的第一个满足请求的内存块 ）和 <strong>best-fit</strong>（使用堆中满足请求的最合适的内存块）。当用户使用完内存后，就将内存返回给堆。</p><p>这种基于堆的分配策略的根本问题是碎片（fragmentation）。当内存块被分配后，它们会以不同的顺序在不同的时间返回。这样会在堆中留下一些洞，需要花一些时间才能有效地管理空闲内存。这种算法通常具有较高的内存使用效率（分配需要的内存），但是却需要花费更多时间来对堆进行管理。</p><p><strong>buddy memory allocation</strong>，是一种更快的内存分配技术，它将内存划分为 2 的幂次方个分区，并使用 best-fit 方法来分配内存请求。当用户释放内存时，就会检查 buddy 块，查看其相邻的内存块是否也已经被释放。如果是的话，将合并内存块以最小化内存碎片。这个算法的时间效率更高，但是由于使用 best-fit 方法的缘故，会产生内存浪费。</p><p>Linux 所使用的 slab 分配器的基础是 Jeff Bonwick 为 SunOS 操作系统首次引入的一种算法。Jeff 的分配器是围绕对象缓存进行的。在内核中，会为有限的对象集（例如文件描述符和其他常见结构）分配大量内存。Jeff 发现对<strong>内核中普通对象进行初始化所需的时间超过了对其进行分配和释放所需的时间。因此他的结论是不应该将内存释放回一个全局的内存池，而是将内存保持为针对特定目而初始化的状态</strong>。例如，如果内存被分配给了一个互斥锁，那么只需在为互斥锁首次分配内存时执行一次互斥锁初始化函数（mutex_init）即可。后续的内存分配不需要执行这个初始化函数，因为从上次释放和调用析构之后，它已经处于所需的状态中了。</p><p>Linux slab 分配器使用了这种思想和其他一些思想来构建一个在空间和时间上都具有高效性的内存分配器。</p><p>下图 给出了 slab 结构的高层组织结构。在最高层是 cache_chain，这是一个 slab 缓存的链接列表。这对于 best-fit 算法非常有用，可以用来查找最适合所需要的分配大小的缓存（遍历列表）。</p><p><img src="/img/Linux_slab.gif" srcset="/img/loading.gif" alt=""></p><p><code>slabs_full</code></p><p>完全分配的 slab</p><p><code>slabs_partial</code></p><p>部分分配的 slab</p><p><code>slabs_empty</code></p><p>空 slab，或者没有对象被分配</p><p>//这么看来Nettty管理内存的策略和这个slab分配器的实现非常的相似</p><p>注意 slabs_empty 列表中的 slab 是进行<strong>回收（reaping）</strong>的主要备选对象。正是通过此过程，slab 所使用的内存被返回给操作系统供其他用户使用。</p><p>slab 列表中的每个 slab 都是一个连续的内存块（一个或多个连续页），它们被划分成一个个对象。<strong>这些对象是从特定缓存中进行分配和释放的基本元素</strong>。注意 slab 是 slab 分配器进行操作的最小分配单位，因此如果需要对 slab 进行扩展，这也就是所扩展的最小值。通常来说，每个 slab 被分配为多个对象。</p><p><strong>由于对象是从 slab 中进行分配和释放的，因此单个 slab 可以在 slab 列表之间进行移动</strong>。例如，当一个 slab 中的所有对象都被使用完时，就从 slabs_partial 列表中移动到 slabs_full 列表中。当一个 slab 完全被分配并且有对象被释放后，就从 slabs_full 列表中移动到 slabs_partial 列表中。当所有对象都被释放之后，就从 slabs_partial 列表移动到 slabs_empty 列表中。</p><h3 id="slab-背后的动机"><a href="#slab-背后的动机" class="headerlink" title="slab 背后的动机"></a>slab 背后的动机</h3><p>与传统的内存管理模式相比， slab 缓存分配器提供了很多优点。首先，内核通常依赖于对小对象的分配，它们会在系统生命周期内进行无数次分配。slab 缓存分配器通过对类似大小的对象进行缓存而提供这种功能，从而避免了常见的碎片问题。slab 分配器还支持通用对象的初始化，从而避免了为同一目而对一个对象重复进行初始化。最后，slab 分配器还可以支持硬件缓存对齐和着色，这允许不同缓存中的对象占用相同的缓存行，从而提高缓存的利用率并获得更好的性能。</p><p>//本文后面还对slab相关的C api进行了介绍</p><h2 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h2><p>linux中使用<code>linux/mm_types.h</code>中的<code>page</code>结构体来描述物理页 其过于庞大 在此处不再展示 其记录了物理页的状态 引用计数 虚拟地址等等</p><h2 id="页高速缓存与页回写"><a href="#页高速缓存与页回写" class="headerlink" title="页高速缓存与页回写"></a>页高速缓存与页回写</h2><ul><li><p>回写:</p><p>  程序执行写操作直接写入到缓存中 后端存储不会立刻直接更新 而是将页写操作只写写到缓存中 后端存储不会立刻直接更新 而是将页高速缓存中被写入的页面标记为脏 并加入到脏页链表中 然后由回写任务周期性第将脏页写回磁盘</p></li><li><p>缓存回收策略:</p><p>  linux采用了双链表近似的实现了一个LRU算法 其维护两个链表-活跃链表和非活跃链表 前者不会被换出 而后者会 且处于前者的页面必须在其被访问时就处于后者(更清晰的表述是 当一个缓存页被分配(allocated)时 它就会被放入inactive_list(<strong>一个FIFO的队列</strong>) 而当该缓存页第一次被引用(referenced)时 它就会被加入到active_list(<strong>一个LRU managed的链表</strong>)) 页面从inactive的尾部加入并从其头部移除 当活跃链表超过了非活跃链表 那么linux将调用<code>refill_inactive</code>函数来将一部分active_list中的尾部页面移入到 inactive_list中 以便能够再次被回收 并使active_list中页面的数量大概维持在总缓存页的三分之二左右<br>  这种双链表方式被称为<code>LRU-2</code> 推而广之到多链表实现就是<code>LRU-K</code> <strong>其优点在于两个链表各自进行淘汰 在页面仅被使用一次的情况下 能够较快的被淘汰</strong><br>  鉴于所读的那本书描述的即便是加上一些补充说明还是过于模糊 这里引用一下文档的说明</p><p>  注意主有干净的页可以被回收</p><p>  The lists resemble a simplified LRU 2Q <a href="https://www.kernel.org/doc/gorman/html/understand/understand031.html#johnson94low" target="_blank" rel="noopener">JS94</a> where two lists called Am and A1 are maintained. With LRU 2Q, pages when first allocated are placed on a FIFO queue called A1. If they are referenced while on that queue, they are placed in a normal LRU managed list called Am. This is roughly analogous to using lru_cache_add() to place pages on a queue called inactive_list (A1) and using mark_page_accessed() to get moved to the active_list (Am). The algorithm describes how the size of the two lists have to be tuned but Linux takes a simpler approach by using refill_inactive() to move pages from the bottom of active_list to inactive_list to keep active_list about two thirds the size of the total page cache. Figure ?? illustrates how the two lists are structured, how pages are added and how pages move between the lists with refill_inactive().</p><p>  <img src="/img/Linux_Page_Cache_LRU_List.png" srcset="/img/loading.gif" alt=""></p><p>  关于此算法的更详细的说明可见<a href="https://www.kernel.org/doc/gorman/html/understand/understand013.html" target="_blank" rel="noopener">knerl.org</a></p></li><li><p>Linux页高速缓存</p><p>  缓存的页来自正规文件 块设备文件和内存映射文件读写 如此一来 页高速缓存就包含了最近被访问过的文件中的数据块 在执行一个I/O操作前(如<code>read</code>等) 内核会检查数据是否已经在页高速缓存中了 如果需要的数据确实在高速缓存中 那么内核可以从内存中迅速的返回需要的页 而不再需要从相对较慢的磁盘上读取数据</p><p>  为了让Linux的页缓存具有普适性(不仅仅通过扩展<code>inode</code>结构来缓存文件i/o) Linux页高速缓存使用<code>address_space</code>结构体来描述页缓存 当一个文件被10个<code>vm_area_struct</code>结构体标识(5个进程每个调用mmap两次) 那么这个文件只能有一个<code>address_space</code>数据结构 也就是说文件可以有多个虚拟地址 但只能在物理内存有一份 下面是定义在<code>linux/fs.h</code>中的<code>address_space</code>结构体 其字段在注释中都有解释不再赘述 重点关注一下<code>host</code>字段 指向本结构锁关联的内核对象比如inode <code>i_mmap</code>是一个和<code>radix</code>树结合的快速检索树 用以快速检查页是否液晶在页高速缓存中</p><pre><code class="c"></code></pre></li></ul><p>/**</p><ul><li>struct address_space - Contents of a cacheable, mappable object.</li><li>@host: Owner, either the inode or the block_device.</li><li>@i_pages: Cached pages.</li><li>@gfp_mask: Memory allocation flags to use for allocating pages.</li><li>@i_mmap_writable: Number of VM_SHARED mappings.</li><li>@nr_thps: Number of THPs in the pagecache (non-shmem only).</li><li>@i_mmap: Tree of private and shared mappings.</li><li>@i_mmap_rwsem: Protects @i_mmap and @i_mmap_writable.</li><li>@nrpages: Number of page entries, protected by the i_pages lock.</li><li>@nrexceptional: Shadow or DAX entries, protected by the i_pages lock.</li><li>@writeback_index: Writeback starts here.</li><li>@a_ops: Methods.</li><li>@flags: Error bits and flags (AS_*).</li><li>@wb_err: The most recent error which has occurred.</li><li>@private_lock: For use by the owner of the address_space.</li><li>@private_list: For use by the owner of the address_space.</li><li>@private_data: For use by the owner of the address_space.</li><li>/<br>struct address_space {<br>  struct inode        <em>host;<br>  struct xarray        i_pages;<br>  gfp_t            gfp_mask;<br>  atomic_t        i_mmap_writable;<br>#ifdef CONFIG_READ_ONLY_THP_FOR_FS<br>  /</em> number of thp, only for non-shmem files */<br>  atomic_t        nr_thps;<br>#endif<br>  struct rb_root_cached    i_mmap;<br>  struct rw_semaphore    i_mmap_rwsem;<br>  unsigned long        nrpages;<br>  unsigned long        nrexceptional;<br>  pgoff_t            writeback_index;<br>  const struct address_space_operations *a_ops;<br>  unsigned long        flags;<br>  errseq_t        wb_err;<br>  spinlock_t        private_lock;<br>  struct list_head    private_list;<br>  void            *private_data;<br>} <strong>attribute</strong>((aligned(sizeof(long)))) __randomize_layout;<br>```</li></ul>]]></content>
    
    
    <categories>
      
      <category>操作系统</category>
      
      <category>内存管理</category>
      
    </categories>
    
    
    <tags>
      
      <tag>内存管理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>进程与线程</title>
    <link href="/2019/03/27/%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B/index/"/>
    <url>/2019/03/27/%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B/index/</url>
    
    <content type="html"><![CDATA[<h1 id="进程与线程"><a href="#进程与线程" class="headerlink" title="进程与线程"></a>进程与线程</h1><h2 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h2><p>正在运行的程序, 一种控制流集合, 集合中至少包括一条执行流, 执行流之间相互独立, 它们共享进程的所有资源</p><p>进程= 线程 + 资源</p><p>进程提供给应用程序的抽象:</p><ul><li>一个独立的逻辑控制流(好像程序独占的使用处理器一样)</li><li>一个私有独立的地址空间(好像程序独立的使用内存一样)</li></ul><h3 id="虚拟地址空间"><a href="#虚拟地址空间" class="headerlink" title="虚拟地址空间"></a>虚拟地址空间</h3><p>典型的linux进程虚拟地址空间布局</p><p><img src="/img/virtual_address_space.png" srcset="/img/loading.gif" alt="process"></p><ul><li>虚拟内存描述符</li></ul><p>linux中使用<code>mm_struct</code>来描述进程的虚拟内存空间 该结构体定义在<code>include/linux/mm_types</code>中 下面展示的是简化版本 可以看到其中定义了包括内存区域链表 VMA形成的红黑树 <code>spinlock_t page_table_lock</code>自旋页表锁 以及代码段 数据段 堆等的首尾地址 全部页面数 上锁页面数 分配的物理页数 正在使用该地址的进程数(由于linux自由国情在此 似乎说成任务 更严谨一些 当该值为0时 对应的描述符将被回收)等等 另外 <code>mmap</code>和<code>mm_rb</code>这两种数据结构都是用来描述地址空间中的全部内存区域的 前者以链表形式存放 后者以红黑树的形式存放 前者用于高效遍历 后者用于搜索指定元素(比如试图得到某个地址被包含在的VMA)</p><ul><li><p>关于页表锁</p><p>  操作和检索页表时 必须使用<code>page_table_lock</code>锁字段 以防止竞争条件!</p></li></ul><pre><code class="c">struct mm_struct {    struct {        struct vm_area_struct *mmap;        /* list of VMAs */        struct rb_root mm_rb;        u64 vmacache_seqnum;                   /* per-thread vmacache *        int map_count;            /* number of VMAs */        spinlock_t page_table_lock; /* Protects page tables and some                         * counters                         */        struct rw_semaphore mmap_sem;        struct list_head mmlist; /* List of maybe swapped mm&#39;s.    These                      * are globally strung together off                      * init_mm.mmlist, and are protected                      * by mmlist_lock                      */        unsigned long hiwater_rss; /* High-watermark of RSS usage */        unsigned long hiwater_vm;  /* High-water virtual memory usage */        unsigned long total_vm;       /* Total pages mapped */        unsigned long locked_vm;   /* Pages that have PG_mlocked set */        atomic64_t    pinned_vm;   /* Refcount permanently increased */        unsigned long data_vm;       /* VM_WRITE &amp; ~VM_SHARED &amp; ~VM_STACK */        unsigned long exec_vm;       /* VM_EXEC &amp; ~VM_WRITE &amp; ~VM_STACK */        unsigned long stack_vm;       /* VM_STACK */        unsigned long def_flags;        spinlock_t arg_lock; /* protect the below fields */        unsigned long start_code, end_code, start_data, end_data;        unsigned long start_brk, brk, start_stack;        unsigned long arg_start, arg_end, env_start, env_end;        unsigned long saved_auxv[AT_VECTOR_SIZE]; /* for /proc/PID/auxv */        } __randomize_layout;        unsigned long cpu_bitmap[];};</code></pre><p>在任务描述符中<code>task_struct</code> <code>mm域</code>存放着仅策划那个使用的内存描述符 <code>fork()</code>函数利用<code>copy_mm()</code>函数复制父进程的内存描述符 如果父进程希望子进程共享地址空间 可以在调用<code>clone()</code>时 设置<code>CLONE_VM</code>表示 通常这样的进程被称为线程</p><h3 id="虚拟内存区域"><a href="#虚拟内存区域" class="headerlink" title="虚拟内存区域"></a>虚拟内存区域</h3><p>由<code>vm_area_struct</code>结构体描述 定义在文件<code>linux/mm_types</code>中 内存区域在Linux内核中也经常称作虚拟内存区域 其描述了指定地址空间内连续区间上的一个独立内存范围 内核将每个内存区域作为一个单独的内存对象进行管理 每个内存区用于偶一致的属性 如访问权限 和支持的操作等</p><pre><code class="c">/* * This struct defines a memory VMM memory area. There is one of these * per VM-area/task.  A VM area is any part of the process virtual memory * space that has a special rule for the page-fault handlers (ie a shared * library, the executable area etc). */struct vm_area_struct {    /* The first cache line has the info for VMA tree walking. */    unsigned long vm_start;        /* Our start address within vm_mm. */    unsigned long vm_end;        /* The first byte after our end address                       within vm_mm. */    /* linked list of VM areas per task, sorted by address */    struct vm_area_struct *vm_next, *vm_prev;    struct rb_node vm_rb;    /*     * Largest free memory gap in bytes to the left of this VMA.     * Either between this VMA and vma-&gt;vm_prev, or between one of the     * VMAs below us in the VMA rbtree and its -&gt;vm_prev. This helps     * get_unmapped_area find a free area of the right size.     */    unsigned long rb_subtree_gap;    /* Second cache line starts here. */    struct mm_struct *vm_mm;    /* The address space we belong to. */    /*     * Access permissions of this VMA.     * See vmf_insert_mixed_prot() for discussion.     */    pgprot_t vm_page_prot;    unsigned long vm_flags;        /* Flags, see mm.h. */    /*     * For areas with an address space and backing store,     * linkage into the address_space-&gt;i_mmap interval tree.     */    struct {        struct rb_node rb;        unsigned long rb_subtree_last;    } shared;    /*     * A file&#39;s MAP_PRIVATE vma can be in both i_mmap tree and anon_vma     * list, after a COW of one of the file pages.    A MAP_SHARED vma     * can only be in the i_mmap tree.  An anonymous MAP_PRIVATE, stack     * or brk vma (with NULL file) can only be in an anon_vma list.     */    struct list_head anon_vma_chain; /* Serialized by mmap_sem &amp;                      * page_table_lock */    struct anon_vma *anon_vma;    /* Serialized by page_table_lock */    /* Function pointers to deal with this struct. */    const struct vm_operations_struct *vm_ops;    /* Information about our backing store: */    unsigned long vm_pgoff;        /* Offset (within vm_file) in PAGE_SIZE                       units */    struct file * vm_file;        /* File we map to (can be NULL). */    void * vm_private_data;        /* was vm_pte (shared mem) */    ....    struct vm_userfaultfd_ctx vm_userfaultfd_ctx;} __randomize_layout;</code></pre><p>注意其中的<code>flags</code> 字段 其记录了这段<code>VMA</code>的特定 如如可读 可写 可执行 区域增长方向 页面是否共享 是否可用于共享内存 或用于映射设备I/O空间等等 可参见<code>linux/mm.h</code>头文件</p><p>对于实际使用中 进程的内存空间的使用情况 可以通过查看<code>/proc/&lt;pid&gt;/maps</code>文件来查看 也可以用<code>pmap &lt;pid&gt;</code>命令查看</p><h2 id="调度"><a href="#调度" class="headerlink" title="调度"></a>调度</h2><p><img src="/img/process_queue.jpg" srcset="/img/loading.gif" alt="pq"></p><p>当进程创建完毕后 他们被放入就绪队列 队列通常以链表的形式被组织起来 就绪队列的header拥有一个指向列表上的第一PCB块的指针</p><p>同时当一个正在运行中的线程被中断(中断会使CPU终止当前任务并去执行内核例程) 或等待某个特殊事件的发生时–比如该进程发起了I/O请求 由于I/O设备速度较慢 这些进程将会挂起等待I/O完成 并被置入等待队列</p><p><img src="/img/process_schedule_2.png" srcset="/img/loading.gif" alt=""></p><p>上图显示了三种被置入等待队列的情况</p><ol><li>当前进程发起了I/O请求 然后被置入I/O等待队列</li><li>当前进程创建了子进程 并被置入等待队列中等待子进程执行完毕</li><li>当前进程时间片用尽 被强制换下处理器 置入等待队列</li></ol><ul><li>进程的类型</li></ul><ol><li>I/O密集型 在等待I/O上花费了绝大多数时间</li><li>计算机密集型 花费了绝大部分时间在计算上</li></ol><ul><li>何时调度</li></ul><ol><li>创建一个进程之后 需要决定是运行父进程还是子进程 由于两者都处于就绪态 故这是一个正常的调度决策 可以任意决定</li><li>一个进程退出时 必须做出调度决策 一个进程不在运行 就必须从就绪进程中选择某个另外的  如果没有就绪的进程通常会运行一个系统提供的空闲进程</li><li>当一个进程阻塞在I/O或信号量上或由于其他原因阻塞时</li><li>在一个I/O中断发生时 就必须做出调度决策 如果中断来自I/O设备 而该设备现在完成运作 某些被阻塞的等待该I/O进程就可以称为运行的就绪进程了 是让新就绪的进程运行还是继续运行当前程序 由<strong>调度程序</strong>决定</li></ol><ul><li><p>上下文切换</p><p>  上下文是由程序正确运行所需的状态组成的 包括存放在内存中的程序数据和代码 通用目的寄存器 栈 程序寄存器 环境变量以及程序所打开的文件描述符的集合</p></li></ul><p>将当前CPU 核心分配给其他进程需要保存现场进程的状态至PCB 并从下一个进程的PCB中恢复其状态 上下文切换是一项纯粹的开销 上下文切换的速度取决于内存的速度 需要保存/恢复的寄存器数量and the existence instructions?(不太清除这是啥意思) 通常这一过程会在若干微秒内完成</p><ul><li><p>Dispatcher</p><p>  Cpu调度函数的另一个部分是分派器(Dispacher不确定怎么称呼 就姑且给它这么翻译了) 其负责将Cpu核心的控制权交给由调度器选定的另一个线程 这涉及到如下工作</p><ol><li><p>切换上下文至另一个进程</p></li><li><p>切换至用户态</p></li><li><p>跳转到用户程序中合适的执行位置来恢复程序的执行</p><p>由于每次上下文切换时都会调用 故调度器要尽可能快的完成工作 其停止当前进程并切换到另一个进程运行的时间称之为分派时延(原文是 Dispatch latency) </p><p>在linux中我们可以通过cat /proc/{proc_num}/status 来查看指定进程的切换情况</p></li></ol></li></ul><h3 id="调度水平的衡量"><a href="#调度水平的衡量" class="headerlink" title="调度水平的衡量"></a>调度水平的衡量</h3><ol><li>cpu利用率</li><li>吞吐量 单位时间内完成的进程数</li><li>turnaround time 从特定进程的视角来看 一个重要的指标是该进程执行总共花费了多长时间 其是进程在就绪队列中的等待时间 CPU执行时间和I/O执行时间之和</li><li>等待时间 进程在就绪队列中的等待时间</li><li>响应时间 交互系统而言</li></ol><h3 id="交互式系统中的调度"><a href="#交互式系统中的调度" class="headerlink" title="交互式系统中的调度"></a>交互式系统中的调度</h3><p>队列</p><ul><li>轮转调度(Round-Robin Scheduling)</li></ul><p>每个进程都被分配一个时间段 称为<strong>时间片(quantum)</strong> 即允许该进程在该时间段中运行 如果时间片结束时该进程还在运行 或 该进程提前结束或阻塞 则将剥夺CPU使用权并分配给另一个进程</p><p>该算法实现较为简单 只需要维护一张可运行的进程(线程)列表 当一个进程用完它的时间片后 就被移到队列的末尾</p><pre><code>- 时间片    从一个进程切换到另一个需要一定的时间进程管理事务处理----保存和装入寄存器值及内存映像 更新各种表格和列表 清除和重新调入内存高速缓存等    时间片设置的太长会导致过多的进程切换 降低CPU效率 设置太长又可能引起对短交互请求的响应时间变长</code></pre><ul><li>优先级调度</li></ul><p>每个进程被赋予一个优先级 优先级高的进程先运行</p><p>为了防止高优先级永无止尽的运行下去使其他的进程长期处理’饥饿’状态 我们可以设置最长一   个允许运行的最大时间片 持续占用CPU时间超过所允许的时间则进行切换 也可以使低优先级的进程随着等待时间的增加而增长</p><p>优先级也可以由系统动态的决定 对于I/O密集型进程 其需要CPU时 应立即分配之 以便启动下一个I/O请求 这样就可以在另一个进程计算的同时执行I/O操作 这种I/O密集型进程长期等待CPU只会造成其无谓的占用内存 对于使I/O密集型进程获得较好服务的一种简单的算法是将其优先级设置为1/f,f为该进程在一次时间片中所占的部分</p><p>也可以将进程安优先级分类 各类之间优先级调度 各类之内使用轮转调度在高优先级的进程组中不为空时不理会低优先级进程 但这样的设计可能会使低优先级进程产生饥饿现象(……)</p><h3 id="调度算法"><a href="#调度算法" class="headerlink" title="调度算法"></a>调度算法</h3><ul><li><p>先来先服务(First Come First Served)</p><p>  这种算法下 先请求CPU的进程会先被分配到CPU 该算法可以很简单的以FIFO队列实现 然而 这种算法下 进程的平均等待时间过长</p></li><li><p>短作业优先法(Short-Job-First)</p><p>  该算法根据每个进程占用CPU时间片的长度进行安排 将占用时间片最少的进程优先处理 如果两个进程的占用CPU时间片相同则根据FCFS处理</p><p>  该方法的进程平均等待时间是最短的 但是它不能CPU调度所完好的实现 因为无法知道下一个进程究竟会需要多长的时间片 只能采用如指数平均法进行预测</p><p>  <img src="/img/exponential_average.png" srcset="/img/loading.gif" alt=""></p></li><li><p>时间片轮转调度(RR)</p></li><li><p>优先级调度</p></li><li><p>多层队列调度(Multilevel Queue Scheduling)</p><p>  优先级调度算法仅仅由队列实现将使寻找最大优先级的工作时间复杂度达到O(n) 因此我们可以使用多级队列 并于RR算法相结合 可以有效的降低时间复杂度</p><p>  <img src="/img/ml_q.png" srcset="/img/loading.gif" alt=""></p></li><li><p>多层反馈队列调度(Multilevel Feedback Queue Scheduling)</p><p>  对多层队列调度的改进</p><p>  据CPU burst的特点区分进程。如果进程使用过多CPU时间则转移到更低队列，在低priority队列中等待时间过长的进程可被转移到高priority队列（aging的一种形式）。</p><ul><li><p>可由以下参数定义:</p><ol><li>队列数量</li></ol></li></ul><ol start="2"><li>每个队列的调度算法</li><li>用于确定何时升级到更高priority队列的方法</li><li>用于确定何时降级到更低priority队列的方法</li><li>用于确定进程需要服务时应进入哪个队列的方法</li></ol></li></ul><h3 id="Linux中的进程调度实现"><a href="#Linux中的进程调度实现" class="headerlink" title="Linux中的进程调度实现"></a>Linux中的进程调度实现</h3><p>Linux调度器是以模块方式提供的 这样做的目的是允许不同类型的进程可以有针对性的选择调度算法 其允许多种不同的可动态添加的调度算法并存 调度属于自己范畴的进程 每个调度器都有一个优先级 基础的调度器代码定义在<code>kernel/shed.c</code>中 其按照优先级数序遍历调度类 拥有一个可执行进程的最高优先级调度器类胜出</p><p>就像之前讨论的 进程调度器有两个通用概念:<strong>进程优先级和时间片</strong></p><ul><li><p>关于优先级</p><p>  linux中有两种不同的优先级范围</p><p>  第一种采用nice值 越大的nice值意味着越低的优先级</p><p>  第二种是实时优先级 越高的实时优先级数值意味着进程优先级越高 任何实时进程的优先级都高于普通的进程</p></li><li><p>关于时间片</p><p>  一个用以表示被抢占前该进程能持续运行时间的数值 任何长时间片都将导致系统交互表现欠佳</p></li></ul><p>linux中进程调度的入口点是函数<code>schedule</code> 其定义在<code>kernel/sched.c</code>(在2.7之前的版本)中 是内核其他部分用于进程调度的入口 schedule通常都需要和一个具体的调度类相关联 也就是说它会找到一个最高优先级的调度类 后者需要有自己的可运行队列 最终获得下一个该运行的进程</p><h4 id="CFS-Completely-Fair-Scheduler"><a href="#CFS-Completely-Fair-Scheduler" class="headerlink" title="CFS(Completely Fair Scheduler)"></a>CFS(Completely Fair Scheduler)</h4><p>基于这样的理念: <strong>进程的调度效果应如系统具备一个理想中的完美多任务处理器 在这种系统中每个进程都能获得1/n的处理器时间 n是可运行进程的数量</strong></p><p>CSF首先在所有可运行进程总数的基础上 计算出一个进程应该运行多久 然后每个进程都按其权重(越低的nice 越高的权重)在全部可运行进程中所占比例的”时间片”来运行 但是当可运行任务数量趋于无限时 它们各自所获得的处理器使用比和时间片都将趋于0 并由此带来不能忍受的过高的切换消耗 CFS为此引入每个进程获得的时间片底线 这个底线称为最小粒度 默认情况下这个值是1ms</p><p>当CFS需要选择下一个运行进程时 它会挑一个具有最小<code>vruntime</code>(存放进程的虚拟运行时间 CSF使用该变量来记录一个程序运行了多久以及计算其还应运行多久)</p><p>CSF使用<strong>红黑树</strong> 来组织可运行的进程队列 并利用其迅速找到最小<code>vruntime</code>的进程</p><ul><li><p>休眠</p><p>  休眠的进程处于一个特殊的不可执状态 如果没有这种状态 调度程序可能会选出一个暂时无法(不想)往下执行的进程 且休眠就必须以轮询的方式实现</p><p>  通常休眠都是为了等待一些事件 可能是一段时间从文件I/O读更多数据 或是某个硬件事件 一个进程还有可能在尝试获取一个已被占用的内核信号量时被迫休眠 还有文件I/O等等</p><p>  对于将要进入休眠的进程 内核將之标记为休眠状态 然后从可执行红黑树中移除 放入<strong>等待队列</strong>  然后schedule调用其他可运行进程 唤醒的过程整好相反</p><ul><li><p>唤醒</p><p>  唤醒操作通过<code>wake_up()</code>进行 它调用函数<code>try_to_wake_up()</code> 唤醒指定的等待队列上的所有进程 将之设置为<code>TASK_RUNNING</code>状态 调用<code>enqueue_task()</code>将此进程放入红黑树中 通常哪段代码促使条件的达成 它就要负责对等待队列调用<code>wake_ip()</code></p></li></ul></li><li><p>放弃处理器时间 </p><p>  linux通过<code>sched_yield()</code>系统调用  提供了一种让进程显示的将处理器时间让给其他等待执行进程的机制 其通过将进程从活动队列中移到过期队列实现的 从而不仅使该进程被抢占 还能确保其在一定时间内不会再被执行了</p></li></ul><h2 id="IPC"><a href="#IPC" class="headerlink" title="IPC"></a>IPC</h2><p>通常下IPC的实现有如下几种实现方式:</p><ol><li>共享内存(文件)</li><li>管道</li><li>信号量</li><li>消息队列</li></ol><h3 id="并发"><a href="#并发" class="headerlink" title="并发"></a>并发</h3><p>造成并发访问的情况包括:</p><ol><li>中断</li><li>软中断和tasklet</li><li>内核抢占</li><li>睡眠及用户空间的同步</li><li>对称多任务</li></ol><h3 id="临界区"><a href="#临界区" class="headerlink" title="临界区"></a>临界区</h3><p>我们把对共享内存进行访问的程序片段称为临界区(Critical Section) 如果我们适当的安排 使得两个进程不可能同时处于临界区中 那么就能够避免竞态条件</p><p>一个好的临界区设计应满足以下几点</p><ol><li>任何两个进程不能同时处于临界区</li><li>不应对CPU的速度和数量做任何假设</li><li>临界区外的进程不能阻塞其他进程</li><li>不得使进程无限期的等待进入临界区</li></ol><p><img src="/img/critical_area.jpg" srcset="/img/loading.gif" alt=""></p><ul><li>方案</li></ul><ol><li><p>屏蔽中断</p><p> 每个进程刚进入临界区后立刻屏蔽所有中断 在离开之前再打开中断 但是由于众所周知的原因—CPU只有发生时钟中断或者其他中断时才会进行进程切换 这样虽然可以保证本进程退出临界区前不会有其他进程进入 但是 却把屏蔽中断的权力交给了进程 这可能导致整个系统终止 <strong>且如果是多处理器系统 那么屏蔽中断仅仅对disable指令所指定的CPU有效其它CPU仍将照旧运行</strong></p></li><li><p>锁变量</p><p> 设想有一个共享锁变量 其初始值为0 当一个进程想进入临界区时 首先测试这把锁 如果该锁为0 则该进程将其设置为1并进入临界区 若为1 则等待直到0</p><p> 但是这种实现方式 在并发条件下 仍有可能会由两到多个进程进入临界区 比如考虑 进程A读该变量发现为0 进入临界区 在將之置为1之前进程2也读了该变量 那么进程2也会进入临界区</p></li><li><p>严格轮询法</p><p> <img src="/img/emmm.jpeg" srcset="/img/loading.gif" alt=""></p><p> 整形变量turn用于记录轮到哪个进程进入临界区</p><p> 该方案严格的要求两个进程轮流进入临界区 但是在两个进程运行速度差距较大的情况—考虑进程0 执行完临界区代码后将turn设置为1 而进程1 还在完成上一次的非临界区代码 那么 进程0如果想要再次执行临界区代码 就只能等待进程1完成 不满足临界区设计的第3点</p></li><li><p>Peterson算法</p><p> <img src="/img/peterson.png" srcset="/img/loading.gif" alt=""></p><p> 考虑两个进程同时进入enter_region的情况 它们都将自己的pid存入turn中 其中先进入的进程设置的pid将被后进入的覆盖掉 假设进程1后进入 那么两个进程都执行到while时 进程0会循环0次并直接进入临界区 而进程1则不能 只能等待进程0在调用leave_region后 才能进入临界区</p></li><li><p>TSL(test and set lock) 指令</p><p> 其将一个内存字lock读到寄存器RX中 然后在该内存地址上存一个非0值 读字和写字操作保证是不可分的 即该指令结束之前其他处理器均不允许访问该内存字 执行TSL指令的CPU将锁住内存总线 以禁止其他CPU在本指令结束之前访问内存</p><p> 一个可替代该指令的指令是XCHG 该指令原子的交换了两个位置的内容</p></li></ol><ul><li><p><strong>关于锁住存储总线和屏蔽中断的区别</strong></p><p>  屏蔽中断 然后在读内存字之后跟着写操作并不能阻止总线上的第2个处理器在读操作和写操作之间访问该内存字 事实上 处理器1上屏蔽中断对处理器2不会有任何影响 让处理器2远离内存的唯一方法直到处理器1完成的唯一方法就是锁住总线</p></li></ul><h3 id="信号量"><a href="#信号量" class="headerlink" title="信号量"></a>信号量</h3><p>使用一个整型变量来累计唤醒次数 供以后使用 一个信号量的取值 可以是0或正值</p><p>其有两个基本操作 down和up</p><p>对于down 检查其值是否大于0 大于则减1 小于则睡眠 检查数值 修改变量值以及可能发生的睡眠操作均为一个单一 不可分割的原子操作</p><p>up对信号量的值增1 如果有一个或多个进程在该信号量上睡眠 无法完成一个先前的down 则系统选择一个唤醒并允许该进程完成其down操作 该操作同样保持原子性</p><p><img src="/img/samaphore_cons_prod.jpg" srcset="/img/loading.gif" alt=""></p><h3 id="互斥量"><a href="#互斥量" class="headerlink" title="互斥量"></a>互斥量</h3><p>可以视为信号量的简化版本 其没有后者的计数能力 只有<strong>加锁和解锁</strong>两个状态</p><p><img src="/img/mutex.jpg" srcset="/img/loading.gif" alt=""></p><h3 id="管程-Monitor"><a href="#管程-Monitor" class="headerlink" title="管程(Monitor)"></a>管程(Monitor)</h3><p><strong>一个管程是由过程 变量 及数据结构组成的一个集合</strong> 它们组成一个特殊的模块或软件包 进程可以在任何需要的时候调用管程中的过程 但它们<strong>不能在管程之外的过程中直接访问管程内的数据结构</strong>(JAVA的synchronized关键字配合对象锁就非常满足这个定义 个人感觉)</p><p>管程中任意时刻只能有一个活跃进程 这一特性使其能有效的完成互斥</p><p>其是编程语言的组成部分 因此编译器可以采用与其他过程调用不同的方法来处理对管程的调用</p><p>管程有两个相关的操作wait和signal</p><ul><li>wait</li></ul><p>一个管程发现其无法继续运行时 它会在某个条件变量上调用wait操作 该操作导致自身阻塞 并将使另一个之前等待在管程之外的进程调入管程</p><ul><li>signal</li></ul><p>可以用来唤醒正在睡眠中的管程</p><h3 id="屏障-Barrier"><a href="#屏障-Barrier" class="headerlink" title="屏障(Barrier)"></a>屏障(Barrier)</h3><p>通过在每个阶段的结尾设置屏障 可以使只有所有的进程由就绪准备下一个阶段 否则任何进程都不能进入一个阶段 到一个进程到达屏障时 它就被屏障阻拦 直到所有的进程都到达该屏障为止</p><h2 id="多任务"><a href="#多任务" class="headerlink" title="多任务"></a>多任务</h2><p>windows 中的进程和线程是严格区分的 即每一个进程都有对应的pcb 每一个线程都有对应的tcb</p><p>而Linux中 所有的执行实体都称为 <strong>任务(Task)</strong> 每一个任务都类似于一个单线程的进程 具有内存空间 执行实体 文件资源等 不过Linux下不同的任务之间可以选择共享内存空间 因而实际意义上 共享了同一个内存空间的多个任务构成了一个进程 也就是说每一个线程都有自己<code>task_struct</code> 这些任务也就成了这个进程里的线程 </p><p><img src="/img/Linux_task_struct.png" srcset="/img/loading.gif" alt=""></p><p>举例说明: 假设一个进程有四个线程 对于对线程有专门的支持到的系统而言 通常会有一个指向包含四个不同的线程的进程描述符 该描述符负责描述向地址空间 打开文件等这样的共享资源 而线程本身再区描述它独占的资源 而linux仅仅创建四个进程并分配四个普通的<code>task_struct</code> 建立这四个进程时指明它们的共享资源</p><p>即便是如C语言中的pthread_create这样的函数其本质上也是如此 在用这个函数创建一个新的线程时 不过是将线程同样映射到进程空间上即可</p><p>以下来自<a href="https://stackoverflow.com/questions/4854940/how-does-pthread-work" target="_blank" rel="noopener">StackOverFlow</a>中的说明</p><blockquote><p>On Linux, both fork() and ptrheads use the same syscall clone(), which creates a new process. The difference between them is simply the parameters they send to<br>clone(), when creating a new thread, it simply makes both processes use the same memory mappings.</p><p>Remember, in Linux (and other modern Unixes), memory mappings, stacks, processor state, PIDs, and others are orthogonal features of a process; so you can create<br>a new process with just a new stack and process state (sharing everything else), and call it a thread.</p></blockquote><p>linux中没有tcb pcb的专用数据结构 而是统一的使用了<strong>task_struct</strong>结构体 在linux 0.11中该结构体定义如下 Linux中进程间具有明显的继承关系 所有的进程都是pid为1的init进程的子进程 内核在系统启动的最后阶段启动init进程 该进程读取系统最初的初始化脚本并执行相关程序 最终完成系统启动的整个过程 每个进程都有一个父进程和0到多个子进程</p><pre><code class="c">struct task_struct{/* these are hardcoded - don&#39;t touch */    long state;            /* -1 unrunnable, 0 runnable, &gt;0 stopped */    long counter;    long priority;    long signal;    struct sigaction sigaction[32];    long blocked;            /* bitmap of masked signals *//* various fields */    int exit_code;    unsigned long start_code, end_code, end_data, brk, start_stack;    long pid, father, pgrp, session, leader;    unsigned short uid, euid, suid;    unsigned short gid, egid, sgid;    long alarm;    long utime, stime, cutime, cstime, start_time;    unsigned short used_math;/* file system info */    int tty;            /* -1 if no tty, so it must be signed */    unsigned short umask;    struct m_inode *pwd;    struct m_inode *root;    struct m_inode *executable;    unsigned long close_on_exec;    struct file *filp[NR_OPEN];/* ldt for this task 0 - zero 1 - cs 2 - ds&amp;ss */    struct desc_struct ldt[3];/* tss for this task */    struct tss_struct tss;}</code></pre><table><thead><tr><th>线程</th><th>进程</th></tr></thead><tbody><tr><td>资源调度的最小单位</td><td>资源分配的最小单位</td></tr><tr><td>复用进程数据结构</td><td>task_struct</td></tr><tr><td>使用进程的地址空间</td><td>用有独立的地址空间</td></tr></tbody></table><h2 id="调度-1"><a href="#调度-1" class="headerlink" title="调度"></a>调度</h2><p>现代操作系统采用时分的形式调度运行的线程 操作系统会分出一个个时间片 线程会分配到若干时间片 当时间片用完即发生线程调度 并等待下次分配</p><h2 id="内核线程与用户线程"><a href="#内核线程与用户线程" class="headerlink" title="内核线程与用户线程"></a>内核线程与用户线程</h2><ul><li>线程是什么</li></ul><p>线程一段函数的载体 本质上就是一段独立的执行流 有自己寄存器映像 上下文环境 内存资源和栈 对于任务调度器而言 执行流(线程)是调度的基本单元</p><ul><li>任务调度器</li></ul><p>OS用于进行任务的轮转调度的由处理器运行的一个软件模块 是OS的一部分 调度器在内部维护一个任务表(进程表/线程表/调度表) 然后按照一定的算法从任务表中选择一个任务 再将该任务放到处理器上运行 其是多任务操作系统的核心</p><p>线程的实现有两种方式 由操作系统原生支持 用户通过系统调用使用线程 要么操作系统不支持线程 由进程自己实现 因此线程要么在0特权级的内核中实现 要么在3特权级上的用户空间实现</p><p>两种情况下的实现方式</p><ol><li>用户空间中实现线程</li></ol><p>用户空间中实现线程的好处是可移植性强</p><p>在不支持线程的Os上 OS调度器只以整个进程的方式调度 将处理器的使用权交给这个线程 然后由进程中的调度器自己去协调分配处理器时间 即需要在进程内实现线程表</p><pre><code>* 优点    1. 线程调度算法是由用户程序自己实现的 可以根据应用情况为某些线程加权调度    2. 将线程的寄存器装载到cpu时 可以在用户空间完成 即不用陷入到内核态 这样就免去了进入内核时的入栈及出栈操作 减少了陷入内核态的代价* 缺点    1. 进程中的某个线程若出现了阻塞 而操作系统不知道进程中存在线程(它会认为该进程是传统的单线程进程) 因此会将整个进程挂起 即进程中到全部线程都无法运行    2. 线程在用户空间实现 和在内核空间相比只是在内部操作系统内核提供的线程可以让进程占用更多的处理器资源(一个内核调度来讲 一个进程内部有更多的线程 相对于其在调度器中拥有更多的独立的执行流) 从而真正实现程序的&#39;提速&#39; 且当某线程阻塞后 由于线程由内核空间实现 所以只会阻塞本线程 进程内的其他线程不受影响</code></pre><h2 id="用户态与内核态"><a href="#用户态与内核态" class="headerlink" title="用户态与内核态"></a>用户态与内核态</h2><p>处理器与OS提供的一种机制来限制应用程序可以自由访问的地址空间范围</p><p>处理器通常是通过某个控制寄存器中的一个模式位(mode bit)来提供这种功能的 该寄存器描述了当前进程享有的特权 设置了模式位时 进程运行在内核态中 否则则是用户态 在用户态下 进程不能执行任何特权指令 内核态下则可以执行指令集中的任何指令并可以访问系统中的任何位置</p><p>进程可通过<strong>中断 故障或陷入系统调用</strong>等手段进入内核态</p><p>Linux中可以通过/proc来访问内核的数据结构 其将许多内核数据结构输出为一个用户程序可以读的文本文件层次结构</p><h2 id="进程控制"><a href="#进程控制" class="headerlink" title="进程控制"></a>进程控制</h2><p>C语言中的相关的库函数都定义在&lt;unistd.h&gt;文件中</p><ul><li>fork系统调用</li></ul><p>由于采取了写时复制 fork的实际开销就是复制父进程的页表以及给子进程创建唯一的进程描述符</p><p>其在父进程中会返回子进程pid 子进程中返回0</p><pre><code class="c">extern __pid_t fork (void) __THROWNL;</code></pre><p>子进程与父进程</p><ul><li>并发执行 子进程和父进程是并发运行的独立进程</li><li>相同但是独立的地址空间</li><li>共享文件</li></ul><p>输入:</p><pre><code class="c">#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;#include &lt;stdio.h&gt;void main() {    pid_t pid;    int x=1;    pid = fork();    if (pid == 0) {        printf(&quot;print from child: %d\n&quot;, ++x);        exit(0);    }    printf(&quot;Print from parent: %d\n&quot;, --x);    exit(0);}</code></pre><p>输出如下:</p><pre><code>Print from parent: 0print from child: 2</code></pre><ul><li>回收子进程</li></ul><p>当一个进程由于某种原因终止时 内核不是立即把它从系统中清除 而是将其保持在一种已终止的状态中 直到其被它的父进程回收 当父进程回收已终止的子进程时 内核将子进程的退出状态传递给父进程 然后抛弃已终止的进程 从此该进程才会不存在 一个终止但未被回收的进程称为<strong>僵死进程(Zombie)</strong></p><p>一个进程可以通过waitpid函数来等待其子进程终止或停止</p><ul><li>使进程休眠</li></ul><pre><code class="c">extern unsigned int sleep (unsigned int __seconds);</code></pre><ul><li>使调用函数休眠 直到收到某个信号</li></ul><pre><code class="c">extern int pause (void)</code></pre><ul><li>加载并运行程序</li></ul><pre><code class="c">execve(const char* name, const char* const* argv)</code></pre><ul><li>exit()</li></ul><p>终止进程 该调用释放掉进程占用的资源 并在最后调用<code>schedule</code>使CPU调度下一个进程 该调用永不返回 通常C编译器会在main的返回点放置调用<code>exit</code>的代码</p><ul><li>关于execve与fork的区别</li></ul><p>首先要理解程序与进程的区别 程序是一堆代码和数据 可以作为目标文件存在于磁盘上 或者作为段存在于地址空间中 进程是执行中程序的一个具体的实例 程序总是运行在某个进程的上下文中</p><p>fork函数在新的子进程中运行相同的程序 新的子进程是父进程的一个复制品 而execve函数在当前进程的上下文加载并运行一个新的程序 其会覆盖当前进程的地址空间 但是并没有创建一个新的进程 新的程序仍然有相同的pid 并且继承了调用execve函数时已经打开的所有文件描述符</p><p>execve 调用一次且从不返回 除非没找到要加载运行的目标文件</p><ul><li><p>关于malloc与mmap</p><ul><li><p>malloc</p><p>  调用malloc()时，是在PCB表(进程表)结构中的堆重点内容中申请空间，若申请空间失败，即超过给定的堆最大空间时，将会调用brk()系统调用，将堆空间向未使用的区域扩展，brk()之后新增的堆空间不会自动清除，需使用相应的系统调用来清除</p></li><li><p>mmap</p><p>  调用mmap()系统调用使得进程之间通过映射同一个普通文件实现共享内存 普通文件被映射到进程地址空间后，进程可以像访问普通内存一样对文件进行访问，不必再调用read()，write（）等操作</p></li></ul></li></ul><h2 id="关于mmap的进一步说明"><a href="#关于mmap的进一步说明" class="headerlink" title="关于mmap的进一步说明"></a>关于mmap的进一步说明</h2><p>大部分摘自<a href="https://nieyong.github.io/wiki_cpu/mmap%E8%AF%A6%E8%A7%A3.html" target="_blank" rel="noopener">这位大佬的博客</a></p><p>mmap–内存映射，简而言之就是将内核空间的一段内存区域映射到用户空间。映射成功后，用户对这段内存区域的修改可以直接反映到内核空间，相反，内核空间对这段区域的修改也直接反映用户空间。那么对于内核空间与用户空间两者之间需要大量数据传输等操作的话效率是非常高的。当然，也可以将内核空间的一段内存区域同时映射到多个进程，这样还可以实现进程间的共享内存通信。</p><p>系统调用mmap()就是用来实现上面说的内存映射。最常见的操作就是文件（在Linux下设备也被看做文件）的操作，可以将某文件映射至内存(进程空间)，如此可以把对文件的操作转为对内存的操作，以此避免更多的lseek()与read()、write()操作，这点对于大文件或者频繁访问的文件而言尤其受益。</p><p>mmap的函数原型生命在<code>linux/mm.h</code>中 该函数性设由file参数指定的文件 具体映射的是从偏移offset处开始 长度为len字节的范围内的数据 如果file参数是NULL且offset参数也是0 那么这种情况就被称为<strong>匿名映射(anonymouse mapping)</strong> 如果指定了文件和偏移量 那么这种映射就被称为 <strong>文件映射(file-backed mapping)</strong>  除此之外还可以指定从虚拟内存空间的某个地址开始搜索(利用最开头虚拟地址空间部分所说的红黑树进行搜索) 以及所申请的内存段的属性(如映射是否可以共享 可读可写可执行 在I/O操作上是否阻塞等等) 而如果要删除地址区间则可以用<code>mummap</code>函数</p><pre><code class="c">extern unsigned long do_mmap(struct file *file, unsigned long addr,    unsigned long len, unsigned long prot, unsigned long flags,    vm_flags_t vm_flags, unsigned long pgoff, unsigned long *populate,    struct list_head *uf);</code></pre><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>mmap将一个文件或者其它对象映射进内存。文件被映射到多个页上，如果文件的大小不是所有页的大小之和，最后一个页不被使用的空间将会清零。munmap执行相反的操作，删除特定地址区域的对象映射。</p><p>当使用mmap映射文件到进程后，就可以直接操作这段虚拟地址进行文件的读写等操作，不必再调用read，write等系统调用。但需注意，直接对该段内存写时不会写入超过当前文件大小的内容。</p><p>采用共享内存通信的一个显而易见的好处是效率高，因为进程可以直接读写内存，而不需要任何数据的拷贝。对于像管道和消息队列等通信方式，则需要在内核(因为很多I/O操作都是在内核模式下完成的)和用户空间进行四次的数据拷贝，而共享内存则只拷贝两次数据：一次从输入文件到共享内存区，另一次从共享内存区到输出文件。实际上，进程之间在共享内存时，并不总是读写少量数据后就解除映射，有新的通信时，再重新建立共享内存区域。而是保持共享区域，直到通信完毕为止，这样，数据内容一直保存在共享内存中，并没有写回文件。共享内存中的内容往往是在解除映射时才写回文件的。因此，采用共享内存的通信方式效率是非常高的。</p><p>通常使用mmap()的三种情况： <strong>提高I/O效率、匿名内存映射、共享内存进程通信 。</strong></p><p>用户空间mmap()函数<code>void *mmap(void *start, size_t length, int prot, int flags,int fd, off_t offset)</code>，下面就其参数解释如下：</p><ul><li>start：用户进程中要映射的用户空间的起始地址，通常为NULL（由内核来指定）</li><li>length：要映射的内存区域的大小</li><li>prot：期望的内存保护标志</li><li>flags：指定映射对象的类型</li><li>fd：文件描述符（由open函数返回）</li><li>offset：设置在内核空间中已经分配好的的内存区域中的偏移，例如文件的偏移量，大小为PAGE_SIZE的整数倍</li><li>返回值：mmap()返回被映射区的指针，该指针就是需要映射的内核空间在用户空间的虚拟地址</li></ul><p><img src="/img/linux_sys_call_mmap.png" srcset="/img/loading.gif" alt="mmap"></p><p>UNIX访问文件的传统方法是用open打开它们, 如果有多个进程访问同一个文件, 则每一个进程在自己的地址空间都包含有该文件的副本，这不必要地浪费了存储空间。 两个进程同时读一个文件的同一页时。 系统要将该页从磁盘读到高速缓冲区中, 每个进程再执行一个存储器内的复制操作将数据从高速缓冲区读到自己的地址空间。</p><p>现在考虑另一种处理方法共享存储映射: 进程A和进程B都将该页映射到自己的地址空间, 当进程A第一次访问该页中的数据时, 它生成一个缺页中断. 内核此时读入这一页到内存并更新页表使之指向它.以后, 当进程B访问同一页面而出现缺页中断时, 该页已经在内存, 内核只需要将进程B的页表登记项指向次页即可.</p><ul><li>孤儿进程</li></ul><p>父进程在子进程之前退出 必须有机制保证子进程能找到一个新的父亲 否则成为孤儿的进程就会在 退出时永远处于僵死状态 白白耗费内存</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>mmap系统调用的最终目的是将设备或文件映射到用户进程的虚拟地址空间，实现用户进程对文件的直接读写，这个任务可以分为以下三步:</p><ul><li><p>在用户虚拟地址空间中寻找空闲的满足要求的一段连续的虚拟地址空间,为映射做准备(由内核mmap系统调用完成)<br>假如vm_area_struct描述的是一个文件映射的虚存空间，成员vm_file便指向被映射的文件的file结构，vm_pgoff是该虚存空间起始地址在vm_file文件里面的文件偏移，单位为物理页面。mmap系统调用所完成的工作就是准备这样一段虚存空间,并建立vm_area_struct结构体,将其传给具体的设备驱动程序.</p></li><li><p>建立虚拟地址空间和文件或设备的物理地址之间的映射(设备驱动完成)<br>建立文件映射的第二步就是建立虚拟地址和具体的物理地址之间的映射，这是通过修改进程页表来实现的。mmap方法是file_opeartions结构的成员:int (*mmap)(struct file *,struct vm_area_struct *);</p><ul><li><p>linux有2个方法建立页表:</p><p>  使用remap_pfn_range一次建立所有页表。int remap_pfn_range(struct vm_area_struct <em>vma, unsigned long virt_addr, unsigned long pfn, unsigned long size, pgprot_t prot)。<br>  使用nopage VMA方法每次建立一个页表项。 struct page *(</em>nopage)(struct vm_area_struct *vma, unsigned long address, int *type);<br>  使用方面的限制：remap_pfn_range不能映射常规内存，只存取保留页和在物理内存顶之上的物理地址。因为保留页和在物理内存顶之上的物理地址内存管理系统的各个子模块管理不到。640 KB 和 1MB 是保留页可能映射，设备I/O内存也可以映射。如果想把kmalloc()申请的内存映射到用户空间，则可以通过mem_map_reserve()把相应的内存设置为保留后就可以。</p></li></ul></li><li><p>当实际访问新映射的页面时的操作(由缺页中断完成)<br>page cache及swap cache中页面的区分：一个被访问文件的物理页面都驻留在page cache或swap cache中，一个页面的所有信息由struct page来描述。struct page中有一个域为指针mapping ，它指向一个struct address_space类型结构。page cache或swap cache中的所有页面就是根据address_space结构以及一个偏移量来区分的。<br>文件与 address_space结构的对应：一个具体的文件在打开后，内核会在内存中为之建立一个struct inode结构，其中的i_mapping域指向一个address_space结构。这样，一个文件就对应一个address_space结构，一个 address_space与一个偏移量能够确定一个page cache 或swap cache中的一个页面。因此，当要寻址某个数据时，很容易根据给定的文件及数据在文件内的偏移量而找到相应的页面。<br>进程调用mmap()时，只是在进程空间内新增了一块相应大小的缓冲区，并设置了相应的访问标识，但并没有建立进程空间到物理页面的映射。因此，第一次访问该空间时，会引发一个缺页异常。<br>对于共享内存映射情况，缺页异常处理程序首先在swap cache中寻找目标页（符合address_space以及偏移量的物理页），如果找到，则直接返回地址；如果没有找到，则判断该页是否在交换区 (swap area)，如果在，则执行一个换入操作；如果上述两种情况都不满足，处理程序将分配新的物理页面，并把它插入到page cache中。进程最终将更新进程页表。 注：对于映射普通文件情况（非共享映射），缺页异常处理程序首先会在page cache中根据address_space以及数据偏移量寻找相应的页面。如果没有找到，则说明文件数据还没有读入内存，处理程序会从磁盘读入相应的页面，并返回相应地址，同时，进程页表也会更新.<br>所有进程在映射同一个共享内存区域时，情况都一样，在建立线性地址与物理地址之间的映射之后，不论进程各自的返回地址如何，实际访问的必然是同一个共享内存区域对应的物理页面。</p></li></ul><h3 id="关于PageCache和SwapCache"><a href="#关于PageCache和SwapCache" class="headerlink" title="关于PageCache和SwapCache"></a>关于PageCache和SwapCache</h3><p>可以参见<a href="http://140.120.7.21/LinuxRef/mmLinux/VmOutline/pagecache.html" target="_blank" rel="noopener">这篇文章</a></p><p>each page of an executable image or mmap()ed file is associated with a per-inode cache, allowing the disk file to be used as backing storage for the page. Finally, anonymous pages (those without a disk file to serve as backing storage - pages of malloc()’d memory, for example) are assigned an entry in the system swapfile, and those pages are maintained in the swap cache.</p><p>Note that anonymous pages don’t get added to the swap cache - and don’t have swap space reserved - until the first time they are evicted from a process’s memory map, whereas pages mapped from files begin life in the page cache. Thus, the character of the swap cache is different than that of the page cache, and it makes sense to make the distinction.</p><p>概括性的讲</p><p>swap cache主要是存放那些无根（就是说没有文件系统中的某个文件和其<br>对应）的page，例如你用malloc分配出来的。<br>它对应的file device就是swapfile。<br>它和page cache的区别在于，当文件从file system上读取出来的时候，<br>它的内容就会同时读入page cache中。但是当你用malloc分配内存的<br>时候，并不马上放到swap cache中，而是在进程中不再使用该内存的<br>时候它才被读入swap cache中。 –<a href="https://blog.csdn.net/wz125/article/details/1680903" target="_blank" rel="noopener">blog</a></p><h2 id="信号"><a href="#信号" class="headerlink" title="信号"></a>信号</h2><p>关于kill 与 kill -9</p><pre><code>Both Kill and Kill -9 are used to kill a process . But the difference is seen in how the process which  received the Kill or Kill -9 behaves.Kill will generate a SIGTERM signal asking a process to kill itself gracefully i.e , free memory or take care of other child processes. Killing a process using kill will not have any side effects like unrelased memory because it was gracefully killed.Kill -9 works similarly but it doesn&#39;t wait for the program to gracefully die. Kill -9 generates a SIGKILL  signal which won&#39;t check the state of the process and kills the process immediately.The major difference is SIGTERM(generated by Kill) can  be ignored if the process is still to reach safe state(clear memory or similar activity) and the process may not be killed.Process cannot ignore the SIGKILL (generated by Kill -9) and will be killed immediately irrespective of the state they are in(this may some time cause some issues but the process is killed for sure).</code></pre><p>以上内容来自来自<a href="https://www.quora.com/What-is-the-difference-between-Kill-and-Kill-9-command-in-Unix" target="_blank" rel="noopener">quora</a></p>]]></content>
    
    
    <categories>
      
      <category>操作系统</category>
      
    </categories>
    
    
    <tags>
      
      <tag>任务</tag>
      
      <tag>OS</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
